{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Data Science to Write Comedy\n",
    "Note- If code is rerun, findings may be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By: \n",
    "#### Brandon Greenspan: [LinkedIn](https://https://www.linkedin.com/in/brandonlgreenspan) [Github](https://git.generalassemb.ly/bgreenspan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents:\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "- [Executive Summary](#Executive-Summary)\n",
    "- [Data Collection](#Data-Collection)\n",
    "- [Cleaning & EDA](##Cleaning-&-EDA)\n",
    "- [Model Preparation](#Model-Preparation)\n",
    "    - [Baseline Model](#Model-0:-Baseline)\n",
    "    - [Train Test Split](#Train-Test-Split)\n",
    "- [Modeling](#Modeling)\n",
    "    - [Logistic Regression](#Model-1:-Logistic-Regression)\n",
    "    - [Multinomial Naive Bayes](#Model-2:-Multinomial-Naive-Bayes)\n",
    "    - [Gaussian Naive Bayes](#Model-3:-Gaussian-Naive-Bayes)\n",
    "    - [KNN](#Model-4:-KNN)\n",
    "    - [Support Vector Machine Model](Model-5:-SVM-Model)\n",
    "- [Model Selection](#Model-Selection)\n",
    "- [Model Evaluation](#Model-Evaluation)\n",
    "- [Conclusion](#Conclusion)\n",
    "    - [Recommendations](#Recommendations)\n",
    "    - [Future Steps](#Future-Steps)\n",
    "- [References](#References)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I am a comedy writer (True Story).  I've run out of my own original ideas, so in order to get a good idea to pitch to Mike Schur, I want to find common word choice/themes that fans of both The Good Place and Parks & Recreation are using.  In order to accomplish this, I will need to build a classification model.  While the success metric I'm aiming for in the model selection is accuracy, ultimately, my goal is to find the words that the model misclassifies in order to best identify the overlap of both shows that might indicate what elements a successful Mike Schur show would contain.  Ideally, I want the __accuracy rate of a selected model to be between 90% & 95%__, which means that my model should be a better predictor than the baseline model, but not too accurate because I actually need enough misclassification to come up with show ideas.  If the model theoretically did not misclassify any variables, then I would have an blank page, which is not an improvement upon how many comedic ideas I can conjure.\n",
    "\n",
    "Before I get to meet Michael Schur, I have to present my process for showrunners at NBC, which is not a technical audience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I comedy writer, I'm only as good as my last script.  Unfortunately, I haven't written any good scripts, and I have accepted the fact that if I can't succeed with comedic talent, I can succeed with data.  Michael Schur had a lot of success with sitcoms like The Office, Parks & Rec, Brooklyn 99, The Good Place, and many more.  I have the ambitious goal to pitch a show idea to Michael Schur, while generating my show idea from data.\n",
    "\n",
    "In order to generate a show that Michael Schur would want to develop, I will scrape the Subreddits for Parks & Rec and The Good Place to get an idea of what truly resonated with fans.  Data cleaning is fairly minimal since we are only working with Booleans and Strings, so we won't need to impute any string data.  To get a better understanding of our data, we will see how long our posts are as well as the most common words used for each Subreddit.  Out of these common words, we will create a custom stoplist for modeling.\n",
    "\n",
    "After calculating our baseline model, we will run a Train, Test, Split for the purposes of training and testing any future models, and evaluating the accuracy scores of each model.  The classification models we build are Logistic Regression, Multinomial Naive Bayes, Gaussian Naive Bayes, KNN, & Support Vector Machine.  Since we will be using a gridsearch method for all of these models in order to finetune our hyperparameters, this does take some time to run.\n",
    "\n",
    "Once we select a model, we will investigate our confusion matrix and assess the model's accuracy, sensitivity, specificity, precision, and true negative rates to get a sense as to the strength of the model beyond the initial accuracy score that led to its selection.  Then, we'll evaluate the strongest coefficients that the model is built on to see which words/phrases are the strongest predictors of a certain show.  Lastly, we'll dig into the words and phrases that most often lead to misclassification.\n",
    "\n",
    "After looking at our misclassified words, we'll finally try to generate a sitcom idea.  Will the idea be enough to impress Mike Schur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.base import TransformerMixin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import regex as re\n",
    "\n",
    "ignore: Warning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection will be done through the Pushshift API in order to scrape the subreddits for Parks & Rec and The Good Place.  The function will help automate the process in order for me to create two separate datafames- one for each show and then concatenate them into one database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mahdi helped us build this code through his intro lesson\n",
    "def query_pushshift(subreddit, kind = 'submission', day_window = 365, n = 14):\n",
    "    SUBFIELDS = ['title', 'selftext', 'subreddit', 'created_utc',\n",
    "                 'author', 'num_comments', 'score', 'is_self', 'is_original_content',\n",
    "                'over_18','is_crosspostable']\n",
    "    \n",
    "    # establish base url and stem\n",
    "    BASE_URL = f\"https://api.pushshift.io/reddit/search/{kind}\" # also known as the \"API endpoint\" \n",
    "    stem = f\"{BASE_URL}?subreddit={subreddit}&size=500\" # always pulling max of 500\n",
    "    \n",
    "    # instantiate empty list for temp storage\n",
    "    posts = []\n",
    "    \n",
    "    # implement for loop with `time.sleep(2)`\n",
    "    for i in range(1, n + 1):\n",
    "        URL = \"{}&after={}d\".format(stem, day_window * i)\n",
    "        print(\"Querying from: \" + URL)\n",
    "        response = requests.get(URL)\n",
    "        assert response.status_code == 200\n",
    "        mine = response.json()['data']\n",
    "        df = pd.DataFrame.from_dict(mine)\n",
    "        posts.append(df)\n",
    "        time.sleep(2) # required time as listed by the robots page\n",
    "    \n",
    "    # pd.concat storage list\n",
    "    full = pd.concat(posts, sort=False)\n",
    "    \n",
    "    # if submission\n",
    "    if kind == \"submission\":\n",
    "        # select desired columns\n",
    "        full = full[SUBFIELDS]\n",
    "        # drop duplicates\n",
    "        full.drop_duplicates(inplace = True)\n",
    "        # select `is_self` == True\n",
    "        full = full.loc[full['is_self'] == True]\n",
    "\n",
    "    # create `timestamp` column\n",
    "    full['timestamp'] = full[\"created_utc\"].map(dt.date.fromtimestamp)\n",
    "    \n",
    "    print(\"Query Complete!\")    \n",
    "    return full "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_place_df = pd.read_csv('./goodplace.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1019, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_place_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parks_and_rec_df = pd.read_csv('./parksandrecs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1081, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parks_and_rec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying different values for day_window, n, and size, I wanted to ensure that I had enough data (over 500 rows for each show) and that my classes were relatively balanced.  There weren't nearly as many submissions as I expected for each page, so I decided to scrape 14 years worth of data (the age of Reddit).  This is also interesting to note because the shows did not overlap in timeframe, and considering humor changes with time, it will be interesting to see what words/phrases misclassify due to changes in taste over time.  My classes look balanced (1081 v. 1019, with Parks & Rec being my majority class), so I'm ready to proceed to cleaning.  I have also exported the data into csvs in case my second test run would like to be duplicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([parks_and_rec_df, good_place_df], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenated both subreddits into one database.  Checking my head and tail to ensure it was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>over_18</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Small detain I noticed (not sure if anyone did...</td>\n",
       "      <td>On like my 3rd watch through. Will Arnett must...</td>\n",
       "      <td>PandR</td>\n",
       "      <td>1556457656</td>\n",
       "      <td>michaelskarn007</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-04-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many Lowe’s could Rob Lowe rob, if Rob Low...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PandR</td>\n",
       "      <td>1556466443</td>\n",
       "      <td>spiritofgonzo1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-04-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andy is peter quill</td>\n",
       "      <td>Wouldn’t it be funny if at the end of end game...</td>\n",
       "      <td>PandR</td>\n",
       "      <td>1556481813</td>\n",
       "      <td>Pawandynee</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-04-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Small detain I noticed (not sure if anyone did...   \n",
       "1  How many Lowe’s could Rob Lowe rob, if Rob Low...   \n",
       "2                                Andy is peter quill   \n",
       "\n",
       "                                            selftext subreddit  created_utc  \\\n",
       "0  On like my 3rd watch through. Will Arnett must...     PandR   1556457656   \n",
       "1                                                NaN     PandR   1556466443   \n",
       "2  Wouldn’t it be funny if at the end of end game...     PandR   1556481813   \n",
       "\n",
       "            author  num_comments  score  is_self is_original_content  over_18  \\\n",
       "0  michaelskarn007             3      0     True               False    False   \n",
       "1   spiritofgonzo1             0     27     True               False    False   \n",
       "2       Pawandynee             2      0     True               False    False   \n",
       "\n",
       "  is_crosspostable   timestamp  \n",
       "0             True  2019-04-28  \n",
       "1             True  2019-04-28  \n",
       "2             True  2019-04-28  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>over_18</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>[SPOILER] Janet Theory</td>\n",
       "      <td>Please do not read this if you haven't watched...</td>\n",
       "      <td>TheGoodPlace</td>\n",
       "      <td>1491858194</td>\n",
       "      <td>BrianFoxShow</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>Is Janet Allegorical? [spoiler]</td>\n",
       "      <td>Since my Janet is god theory didn't resonate -...</td>\n",
       "      <td>TheGoodPlace</td>\n",
       "      <td>1491921533</td>\n",
       "      <td>kdubstep</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>Having just finished the (amazing) first seaso...</td>\n",
       "      <td>That pizza is DEFINITELY gluten-free.</td>\n",
       "      <td>TheGoodPlace</td>\n",
       "      <td>1492246895</td>\n",
       "      <td>weblowinherseys</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-04-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "2097                             [SPOILER] Janet Theory   \n",
       "2098                    Is Janet Allegorical? [spoiler]   \n",
       "2099  Having just finished the (amazing) first seaso...   \n",
       "\n",
       "                                               selftext     subreddit  \\\n",
       "2097  Please do not read this if you haven't watched...  TheGoodPlace   \n",
       "2098  Since my Janet is god theory didn't resonate -...  TheGoodPlace   \n",
       "2099              That pizza is DEFINITELY gluten-free.  TheGoodPlace   \n",
       "\n",
       "      created_utc           author  num_comments  score  is_self  \\\n",
       "2097   1491858194     BrianFoxShow            10     42     True   \n",
       "2098   1491921533         kdubstep             5      3     True   \n",
       "2099   1492246895  weblowinherseys             4     40     True   \n",
       "\n",
       "     is_original_content  over_18 is_crosspostable   timestamp  \n",
       "2097                 NaN    False              NaN  2017-04-10  \n",
       "2098                 NaN    False              NaN  2017-04-11  \n",
       "2099                 NaN    False              NaN  2017-04-15  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checked head and tail to ensure the data properly went to one dataframe, which it did.  Now checking for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                     0\n",
       "selftext                164\n",
       "subreddit                 0\n",
       "created_utc               0\n",
       "author                    0\n",
       "num_comments              0\n",
       "score                     0\n",
       "is_self                   0\n",
       "is_original_content    1490\n",
       "over_18                   0\n",
       "is_crosspostable       1155\n",
       "timestamp                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                  object\n",
       "selftext               object\n",
       "subreddit              object\n",
       "created_utc             int64\n",
       "author                 object\n",
       "num_comments            int64\n",
       "score                   int64\n",
       "is_self                  bool\n",
       "is_original_content    object\n",
       "over_18                  bool\n",
       "is_crosspostable       object\n",
       "timestamp              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of nulls in original content, and crosspostable.  These are Boolean categories that can be later be turned into binomials, so I'm going to dig a little deeper into the values into that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    610\n",
       "Name: is_original_content, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_original_content'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to keep original content column, since there are no True values.  If every row would hold the same value, then it is unimportant to help differentiate for our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['is_original_content'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     896\n",
       "False     49\n",
       "Name: is_crosspostable, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_crosspostable'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to impute my null values as False.  Which will later be turned into a binomial.  I can assume a null value is equivalent to \"not true,\" so I find this imputation to be justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('False', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['over_18'] = df['over_18'].astype(str).replace({\n",
    "    'False' : '0',\n",
    "    'True' : '1'\n",
    "}).astype(int) \n",
    "'''type is Boolean, while I could have used mapping, I decided to convert booleans to strings, \n",
    "assign my values, then change the type to integer'''\n",
    "df['is_self'] = df['is_self'].astype(str).replace({\n",
    "    'False' : '0',\n",
    "    'True' : '1'\n",
    "}).astype(int)\n",
    "df['is_crosspostable'] = df['is_crosspostable'].astype(str).replace({\n",
    "    'False' : '0',\n",
    "    'True' : '1'\n",
    "}).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All null values have be handled and boolean categories have been converted to binomial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>over_18</th>\n",
       "      <th>is_crosspostable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.100000e+03</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.475290e+09</td>\n",
       "      <td>10.367143</td>\n",
       "      <td>25.894762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.426667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.880685e+07</td>\n",
       "      <td>34.101018</td>\n",
       "      <td>77.697823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084233</td>\n",
       "      <td>0.494711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.295516e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.434767e+09</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.495146e+09</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.525667e+09</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.561222e+09</td>\n",
       "      <td>887.000000</td>\n",
       "      <td>1310.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_utc  num_comments        score  is_self      over_18  \\\n",
       "count  2.100000e+03   2100.000000  2100.000000   2100.0  2100.000000   \n",
       "mean   1.475290e+09     10.367143    25.894762      1.0     0.007143   \n",
       "std    6.880685e+07     34.101018    77.697823      0.0     0.084233   \n",
       "min    1.295516e+09      0.000000     0.000000      1.0     0.000000   \n",
       "25%    1.434767e+09      2.000000     2.000000      1.0     0.000000   \n",
       "50%    1.495146e+09      5.000000     8.000000      1.0     0.000000   \n",
       "75%    1.525667e+09     11.000000    20.000000      1.0     0.000000   \n",
       "max    1.561222e+09    887.000000  1310.000000      1.0     1.000000   \n",
       "\n",
       "       is_crosspostable  \n",
       "count       2100.000000  \n",
       "mean           0.426667  \n",
       "std            0.494711  \n",
       "min            0.000000  \n",
       "25%            0.000000  \n",
       "50%            0.000000  \n",
       "75%            1.000000  \n",
       "max            1.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All values of is_self are 1, so I will drop that.  over_18 is a very imbalanced class, with only .7% having a true value, so I will drop that column too, especially considering that the shows themselves are not 18 and over shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['is_self', 'over_18'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PandR           1081\n",
       "TheGoodPlace    1019\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PandR (Parks & Rec) is my majority class and therefore my future baseline model.  Going to turn this into a binomial with PandR set to 1 and TheGoodPlace set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>PandR_subr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Small detain I noticed (not sure if anyone did...</td>\n",
       "      <td>On like my 3rd watch through. Will Arnett must...</td>\n",
       "      <td>1556457656</td>\n",
       "      <td>michaelskarn007</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many Lowe’s could Rob Lowe rob, if Rob Low...</td>\n",
       "      <td>False</td>\n",
       "      <td>1556466443</td>\n",
       "      <td>spiritofgonzo1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andy is peter quill</td>\n",
       "      <td>Wouldn’t it be funny if at the end of end game...</td>\n",
       "      <td>1556481813</td>\n",
       "      <td>Pawandynee</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Champion song?</td>\n",
       "      <td>I know I'm not imagining it, but I can't find ...</td>\n",
       "      <td>1556500237</td>\n",
       "      <td>awesometoenails</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sorry if this has been done here before but I ...</td>\n",
       "      <td>Um Leslie? I typed your symptoms into the thin...</td>\n",
       "      <td>1556650836</td>\n",
       "      <td>Cheerio419</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Small detain I noticed (not sure if anyone did...   \n",
       "1  How many Lowe’s could Rob Lowe rob, if Rob Low...   \n",
       "2                                Andy is peter quill   \n",
       "3                                     Champion song?   \n",
       "4  Sorry if this has been done here before but I ...   \n",
       "\n",
       "                                            selftext  created_utc  \\\n",
       "0  On like my 3rd watch through. Will Arnett must...   1556457656   \n",
       "1                                              False   1556466443   \n",
       "2  Wouldn’t it be funny if at the end of end game...   1556481813   \n",
       "3  I know I'm not imagining it, but I can't find ...   1556500237   \n",
       "4  Um Leslie? I typed your symptoms into the thin...   1556650836   \n",
       "\n",
       "            author  num_comments  score  is_crosspostable   timestamp  \\\n",
       "0  michaelskarn007             3      0                 1  2019-04-28   \n",
       "1   spiritofgonzo1             0     27                 1  2019-04-28   \n",
       "2       Pawandynee             2      0                 1  2019-04-28   \n",
       "3  awesometoenails             5      1                 1  2019-04-28   \n",
       "4       Cheerio419            18     14                 1  2019-04-30   \n",
       "\n",
       "   PandR_subr  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PandR_subr'] = df['subreddit'].astype(str).replace({\n",
    "    'TheGoodPlace' : '0',\n",
    "    'PandR' : '1'\n",
    "}).astype(int)\n",
    "df.drop(columns = ['subreddit']).head()\n",
    "# my binomial column is replacing the original subreddit column since that has now been turned into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['all_text'] = df['title'] + \" \" + df['selftext'] + \" \" + df['author']\n",
    "df.drop(columns = ['title', 'selftext', 'author'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merged all text into one column since I will be using CountVectorizer in EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_strings = df['all_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will run all of the text through CountVectorizer to create a matrix to get a better sense of word(s) count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(lowercase = True, # turn everything to lower case\n",
    "                       ngram_range = (1,2)) # for EDA, I will look at 1 word and 2 word phrases\n",
    "\n",
    "# data run through cvec must be transformed\n",
    "X_text = cvec.fit_transform(list_of_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 made</th>\n",
       "      <th>00 min</th>\n",
       "      <th>000</th>\n",
       "      <th>000 000</th>\n",
       "      <th>000 605</th>\n",
       "      <th>000 candles</th>\n",
       "      <th>000 days</th>\n",
       "      <th>000 for</th>\n",
       "      <th>000 how</th>\n",
       "      <th>...</th>\n",
       "      <th>zynerd</th>\n",
       "      <th>zzesty</th>\n",
       "      <th>宀宀분당건마</th>\n",
       "      <th>宀宀분당건마 양재오피</th>\n",
       "      <th>분당키스방</th>\n",
       "      <th>분당키스방 제이제이45다컴</th>\n",
       "      <th>양재오피</th>\n",
       "      <th>양재오피 분당키스방</th>\n",
       "      <th>제이제이45다컴</th>\n",
       "      <th>제이제이45다컴 removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 97362 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  00 made  00 min  000  000 000  000 605  000 candles  000 days  000 for  \\\n",
       "0   0        0       0    0        0        0            0         0        0   \n",
       "1   0        0       0    0        0        0            0         0        0   \n",
       "2   0        0       0    0        0        0            0         0        0   \n",
       "3   0        0       0    0        0        0            0         0        0   \n",
       "4   0        0       0    0        0        0            0         0        0   \n",
       "\n",
       "   000 how  ...  zynerd  zzesty  宀宀분당건마  宀宀분당건마 양재오피  분당키스방  분당키스방 제이제이45다컴  \\\n",
       "0        0  ...       0       0       0            0      0               0   \n",
       "1        0  ...       0       0       0            0      0               0   \n",
       "2        0  ...       0       0       0            0      0               0   \n",
       "3        0  ...       0       0       0            0      0               0   \n",
       "4        0  ...       0       0       0            0      0               0   \n",
       "\n",
       "   양재오피  양재오피 분당키스방  제이제이45다컴  제이제이45다컴 removed  \n",
       "0     0           0         0                 0  \n",
       "1     0           0         0                 0  \n",
       "2     0           0         0                 0  \n",
       "3     0           0         0                 0  \n",
       "4     0           0         0                 0  \n",
       "\n",
       "[5 rows x 97362 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_df = pd.DataFrame(X_text.toarray(),\n",
    "                      columns = cvec.get_feature_names())\n",
    "\n",
    "X_text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes the text I've run through CountVectorizer and turned it into a matrix, representing the word/2-word count by document.  I notice that the last few columns are not in English.  I'm going to remove the columns that aren't in English.  This is not to say that the comments aren't valid, but interpretability of any model would be hard to explain if I don't speak the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_df.drop(columns = ['宀宀분당건마', '宀宀분당건마 양재오피', '분당키스방', '분당키스방 제이제이45다컴', '양재오피', '양재오피 분당키스방', '제이제이45다컴', '제이제이45다컴 removed'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While my next steps will seem like a duplication of efforts, I first want to further visualize what words appear frequently in both Parks and Rec & The Good Place.  I then would like to find the overlap of top words used for both shows in order to create a custom stoplist for modeling purposes.  First, let's see how long the post length is for our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths_of_posts = [len(each) for each in df['all_text']]\n",
    "plt.hist(lengths_of_posts, bins = 50)\n",
    "plt.title('Histogram of Post Lengths')\n",
    "plt.xlabel('Post Length')\n",
    "plt.ylabel('Number of Posts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our posts are under 1000 words.  Since a post can't have fewer than 0 words, there is an obvious right skew.  We likely see our peak close to 0 because of posts that would contain of mostly images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_df['PandR_subr'] = df['PandR_subr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,let's create separate dataframes for Parks & Rec and The Good Place to evaluate the most common words in each show's Subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>2890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>1073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pr_word_count\n",
       "the           2890\n",
       "and           1972\n",
       "to            1633\n",
       "of            1217\n",
       "it            1073"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_words_df = pd.DataFrame(X_text_df.loc[X_text_df['PandR_subr'] != 0].sum().sort_values(ascending = False))\n",
    "\n",
    "pr_words_df.drop(index = ['PandR_subr'], inplace = True)\n",
    "\n",
    "pr_words_df.columns = ['pr_word_count']\n",
    "\n",
    "pr_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pr_words_df.head(20).plot(kind = 'barh', legend = None)\n",
    "plt.title('Most Common Words for Parks & Rec')\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Words');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common words mostly consist of words that would be filtered out by sklearn's list of English stop words.  The noticeable exception is leslie, the main character of the show, who appears in the top 20 most frequent words for the Parks & Rec subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gp_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>5774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>3036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>2441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>2064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>1865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gp_word_count\n",
       "the            5774\n",
       "to             3036\n",
       "and            2441\n",
       "of             2064\n",
       "that           1865"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repeating same process for The Good Place\n",
    "gp_words_df = pd.DataFrame(X_text_df.loc[X_text_df['PandR_subr'] == 0].sum().sort_values(ascending = False))\n",
    "\n",
    "gp_words_df.columns = ['gp_word_count']\n",
    "\n",
    "gp_words_df.drop(index = ['PandR_subr'], inplace = True)\n",
    "\n",
    "gp_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gp_words_df.head(20).plot(kind = 'barh', legend = None)\n",
    "plt.title('Most Common Words for The Good Place')\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Words');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Parks & Rec, The most common words for The Good Place's Subreddit would also qualify as \"stop words.\"  Also similarly, Michael (one of the main characters for The Good Place) is also listed as a common word, along with the show's name.  My next steps is to evaluate the word count overall, so let's join the dataframes and create another visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr_word_count</th>\n",
       "      <th>gp_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>2890</td>\n",
       "      <td>5774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1972</td>\n",
       "      <td>2441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1633</td>\n",
       "      <td>3036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1217</td>\n",
       "      <td>2064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>1073</td>\n",
       "      <td>1596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hugh</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hugged the</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hugged</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huge wrench</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to abandon</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97354 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pr_word_count  gp_word_count\n",
       "the                   2890           5774\n",
       "and                   1972           2441\n",
       "to                    1633           3036\n",
       "of                    1217           2064\n",
       "it                    1073           1596\n",
       "...                    ...            ...\n",
       "hugh                     0              1\n",
       "hugged the               0              1\n",
       "hugged                   0              1\n",
       "huge wrench              0              1\n",
       "to abandon               0              1\n",
       "\n",
       "[97354 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_df = pr_words_df.join(gp_words_df)\n",
    "\n",
    "word_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr_word_count</th>\n",
       "      <th>gp_word_count</th>\n",
       "      <th>total_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>2890</td>\n",
       "      <td>5774</td>\n",
       "      <td>8664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1972</td>\n",
       "      <td>2441</td>\n",
       "      <td>4413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1633</td>\n",
       "      <td>3036</td>\n",
       "      <td>4669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1217</td>\n",
       "      <td>2064</td>\n",
       "      <td>3281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>1073</td>\n",
       "      <td>1596</td>\n",
       "      <td>2669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hugh</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hugged the</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hugged</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huge wrench</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to abandon</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97354 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pr_word_count  gp_word_count  total_count\n",
       "the                   2890           5774         8664\n",
       "and                   1972           2441         4413\n",
       "to                    1633           3036         4669\n",
       "of                    1217           2064         3281\n",
       "it                    1073           1596         2669\n",
       "...                    ...            ...          ...\n",
       "hugh                     0              1            1\n",
       "hugged the               0              1            1\n",
       "hugged                   0              1            1\n",
       "huge wrench              0              1            1\n",
       "to abandon               0              1            1\n",
       "\n",
       "[97354 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_df['total_count'] = word_count_df['pr_word_count'] + word_count_df['gp_word_count']\n",
    "# adds a new column, adding up the word counts for each show\n",
    "word_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_count_df.sort_values(by = ['total_count'], ascending = False).head(20).plot(kind = 'bar')\n",
    "plt.title('Most Common Words Overall')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Word Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the overall word count for our entire dataframe.  While these are unsuprisingly stop words, it's interesting to note that all of these top 20 terms appear with greater frequency in The Good Place.  When looking back over the common words by show, I do notice that the scale is different for each graph, which is more conveniently displayed via this graph above.  Since we've already found the common words, I want to be proactive and set up a stop words list for any future modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this',\n",
       "       'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'leslie',\n",
       "       'he', 'so', 'be', 'my', 'have', 'like', 'ron', 'they', 'parks', 'or',\n",
       "       'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as',\n",
       "       'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not',\n",
       "       'would', 'one', 'know', 'has', 'deleted', 'parks and', 'andy',\n",
       "       'the show', 'anyone', 'his', 'up', 'out', 'we', 'an', 'com', 'time',\n",
       "       'ben', 'think', 'april', 'where', 'rec', 'how', 'get', 'watch',\n",
       "       'and rec', 'really', 've', 'do', 'http', 'does', 'watching', 'who',\n",
       "       'amp', 'tom', 'any', 'him', 'first', 'some', 'love', 'character',\n",
       "       'find', 'chris', 'don', 'series', 'did', 'by', 'for the', 'pawnee',\n",
       "       'been', 'false', 'ann', 'now', 'after'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_words = X_text_df.loc[X_text_df['PandR_subr'] != 0].sum().sort_values(ascending = False).head(101).index\n",
    "# Generates a top 100 list for most common Parks & Rec words\n",
    "pr_words = pr_words.drop('PandR_subr') \n",
    "# Since it was adding up the count for the dummy column, I had to drop it, so the subreddit name doesn't factor into the model\n",
    "\n",
    "pr_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Office is a top term!  This is most likely referring to the other hit show, The Office, which Parks & Rec often gets compared to, which is likely happening frequently on Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['the', 'to', 'and', 'of', 'that', 'is', 'in', 'it', 'place', 'good',\n",
       "       'this', 'they', 'for', 'but', 'be', 'was', 'good place', 'on',\n",
       "       'michael', 'in the', 'he', 'with', 'bad', 'so', 'eleanor', 'have',\n",
       "       'she', 'the good', 'season', 'we', 'as', 'if', 'are', 'you', 'not',\n",
       "       'what', 'just', 'all', 'show', 'of the', 'her', 'about', 'like',\n",
       "       'there', 'or', 'would', 'bad place', 'chidi', 'people', 'can', 'at',\n",
       "       'how', 'think', 'from', 'out', 'the bad', 'my', 'one', 'to the', 'his',\n",
       "       'janet', 'know', 'when', 'them', 'will', 'episode', 'up', 'because',\n",
       "       'do', 'has', 'by', 'an', 'the show', 'get', 'me', 'their', 'who',\n",
       "       'jason', 'to be', 'tahani', 'other', 'really', 'time', 'why', 'only',\n",
       "       'first', 'more', 'been', 'could', 'even', 'being', 'no', 'were',\n",
       "       'spoilers', 'some', 'see', 'also', 'don', 'into', 'had'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_words = X_text_df.loc[X_text_df['PandR_subr'] == 0].sum().sort_values(ascending = False).head(100).index\n",
    "# Replicate the process for The Good Place\n",
    "gp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'and',\n",
       " 'to',\n",
       " 'of',\n",
       " 'it',\n",
       " 'in',\n",
       " 'is',\n",
       " 'that',\n",
       " 'for',\n",
       " 'this',\n",
       " 'but',\n",
       " 'was',\n",
       " 'on',\n",
       " 'just',\n",
       " 'season',\n",
       " 'you',\n",
       " 'show',\n",
       " 'with',\n",
       " 'he',\n",
       " 'so',\n",
       " 'be',\n",
       " 'my',\n",
       " 'have',\n",
       " 'like',\n",
       " 'they',\n",
       " 'or',\n",
       " 'can',\n",
       " 'what',\n",
       " 'me',\n",
       " 'episode',\n",
       " 'of the',\n",
       " 'at',\n",
       " 'are',\n",
       " 'she',\n",
       " 'as',\n",
       " 'all',\n",
       " 'in the',\n",
       " 'about',\n",
       " 'there',\n",
       " 'from',\n",
       " 'if',\n",
       " 'her',\n",
       " 'when',\n",
       " 'not',\n",
       " 'would',\n",
       " 'one',\n",
       " 'know',\n",
       " 'has',\n",
       " 'the show',\n",
       " 'his',\n",
       " 'up',\n",
       " 'out',\n",
       " 'we',\n",
       " 'an',\n",
       " 'time',\n",
       " 'think',\n",
       " 'how',\n",
       " 'get',\n",
       " 'really',\n",
       " 'do',\n",
       " 'who',\n",
       " 'first',\n",
       " 'some',\n",
       " 'don',\n",
       " 'by',\n",
       " 'been']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = list(gp_words) + list(pr_words)\n",
    "# creates a master list of the top 100 terms for both shows\n",
    "stops = pd.DataFrame(all_words)\n",
    "\n",
    "custom_stopwords = stops.loc[stops.duplicated() == True]\n",
    "# finds the words that appear in the top 100 for both shows\n",
    "stopwords = list(custom_stopwords[0])\n",
    "# turns those words into a stop word list to utilize in model testing\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all of our models, we are trying to use X (text from subreddit submissions) in order to  to predict y (whether a submission belongs in Parks & Rec or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['all_text'] #predictive variable\n",
    "y = df['PandR_subr'] #target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0: Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5147619047619048"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = df['PandR_subr'].mean()\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model would predict the majority class (Parks & Rec) every single time.  Since 51% of our y values are listed as Parks & Rec, our model would only be accurate 51% of the time.  While I want some misclassification, misclassifying almost 50% of the data is too much for me to go through in order to generate ideas for a show.  It's almost the same effect as having a model that has 100% accuracy in that I'll just continue to have writers block.  We need to build a better classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state = 63,\n",
    "                                                    stratify = y # keeps the ratio the same for our train and test\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([ #instantiate transformers and model\n",
    "    ('vectorizer', CountVectorizer()), \n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('lr', LogisticRegression(random_state = 63))\n",
    "])\n",
    "\n",
    "# set hyperparameters\n",
    "\n",
    "pipe_params = {'vectorizer__ngram_range'  : [(1,1),(1,2), (1,3)],\n",
    "              'vectorizer__stop_words': [None, 'english', stopwords], #including cusotm stopwords in addition to english and none\n",
    "              'tfidf__use_idf'  : [True, False], #toggle between use of TFIDF\n",
    "              'lr__solver': ['liblinear', 'lbfgs'],\n",
    "              'lr__penalty': ['l1', 'l2', 'none'] # running this with the lasso and ridge techniques by applying a penalty, also running with no penalty\n",
    "              }\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                 param_grid = pipe_params,\n",
    "                 cv = 5, # cross validation of 5\n",
    "                 scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I want to try to get the best result for our Logistic Regression model, I set up a gridsearch to run 540 possible models in order to return the best fit.  For hyperparameters, we looked at 1-grams, 2-grams, and 3-grams.  We tried with my custom stopwords, english stopwords, and no stopwords.  We tried it both with and and without the use of tfidf, 2 different solvers, and and also tried applying a lasso penalty, ridge penalty, or no penalty.  Each model went through a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "lr_fit = gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fit.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9352380952380952"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9223809523809523"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(lr_fit.best_estimator_, X, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__penalty': 'none',\n",
       " 'lr__solver': 'lbfgs',\n",
       " 'tfidf__use_idf': True,\n",
       " 'vectorizer__ngram_range': (1, 1),\n",
       " 'vectorizer__stop_words': 'english'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fit.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best Logistic Regression model involved no penalty, the \"lbfgs\" solver, use of TFIDF, an ngram range of (1,1), and sklearn's English stop words.  This model was overfit since it had a perfect training score, but a .94 testing score.  Our lower cross-validation score compared to test score shows that our model might not perform as well against unseen data as it did against our test data.  This is a significant improvement from our baseline model of .51, but we can still try to achieve better accuracy.\n",
    "\n",
    "I would like to try a Naive Bayes model on this data to see if we can improve accuracy.  Since I want to try models with and without TFIDF, I will have to build a Multinomial Naive Bayes model for my tests without TFIDF and then build a Gaussian Naive Bayes model for running TFIDF.  First, let's try our Multinomial Naive Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([ # instantiate\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# set hyperparameters\n",
    "\n",
    "pipe_params = {'vectorizer__ngram_range'  : [(1,1),(1,2), (1,3)],\n",
    "              'vectorizer__stop_words': [None, 'english', stopwords],\n",
    "              'mnb__alpha': [.12, .125, .13] #tested out other alpha values and got it narrowed to this\n",
    "              }\n",
    "\n",
    "gsnb = GridSearchCV(pipe,\n",
    "                 param_grid = pipe_params,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy',\n",
    "                 verbose = 2 # let's get progress updates in real time this time\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best result for the Multinomial Naive Bayes model, I set up a gridsearch to run 135 possible models in order to return the best fit.  For hyperparameters, we looked at 1-grams, 2-grams, and 3-grams.  We tried with my custom stopwords, english stopwords, and no stopwords.  Lastly, we tried adjusting our alpha valaues and reran this many times before settling on the values we chose.  Each model went through a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 135 out of 135 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "mnb_fit = gsnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mnb__alpha': 0.12,\n",
       " 'vectorizer__ngram_range': (1, 2),\n",
       " 'vectorizer__stop_words': 'english'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_fit.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9352380952380952"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_fit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9261904761904762"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mnb_fit.best_estimator_, X, y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best fit involed an alpha of .12, ngram range of (1,2), and the sklearn English stop words.  This model was overfit because it had a perfect training score and a 0.93 testing score.  The 0.94 testing score is still significantly more accutate as our baseline model.  Our lower cross-validation score shows that our model may not perform as well against unseen data as it did against our test data since the testing score was higher than the cross-validation score.  Next, I want to see how TFIDF would perform with a Naive Bayes model, so I need to try a Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin): #from Mahdi\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dense transformer is required in order for the Gaussian Naive Bayes model to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([ #instantiate transformers and model\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('_', DenseTransformer()),\n",
    "    ('gnb', GaussianNB())\n",
    "])\n",
    "\n",
    "# set hyperparameters\n",
    "\n",
    "pipe_params = {'vectorizer__ngram_range'  : [(1,1),(1,2), (1,3)],\n",
    "              'vectorizer__stop_words': [None, 'english', stopwords],\n",
    "              'tfidf__use_idf'  : [True, False]\n",
    "              }\n",
    "\n",
    "gsgnb = GridSearchCV(pipe,\n",
    "                 param_grid = pipe_params,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy',\n",
    "                 verbose = 2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I want to try to get the best result for our Gaussian Naive Bayes model, I set up a gridsearch to run 90 possible models in order to return the best fit.  For hyperparameters, we looked at 1-grams, 2-grams, and 3-grams.  We tried with my custom stopwords, english stopwords, and no stopwords.  We tried it both with and and without the use of tfidf.  Each model went through a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   4.5s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.8s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.8s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.5s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.7s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.7s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.5s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   4.0s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.7s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.3s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.1s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.0s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=  16.0s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   7.5s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   7.4s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   7.3s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   7.1s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   4.5s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   4.4s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   4.4s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   4.3s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   4.2s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.4s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.7s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.2s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.1s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.0s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.1s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.1s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.1s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.0s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.9s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.2s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.2s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.3s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.2s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.1s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.7s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.6s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.6s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.6s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   7.6s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   7.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   8.1s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   8.0s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   7.1s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   4.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   4.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   4.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   4.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   4.2s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.3s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.3s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.3s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.1s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:  4.8min finished\n"
     ]
    }
   ],
   "source": [
    "gnb_fit = gsgnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__use_idf': False,\n",
       " 'vectorizer__ngram_range': (1, 3),\n",
       " 'vectorizer__stop_words': 'english'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_fit.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8876190476190476"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_fit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8714285714285713"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(gnb_fit.best_estimator_, X, y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best fit didn't include the use of TFIDF, which is surprising, since the Gaussian model is usually better fit for float values generated by TFIDF.  This model also found the ngram range of (1,3) and sklearn's English stopwords most effective.\n",
    "\n",
    "Since the best model here didn't involve the use of TFIDF, it's no surprise that the Gaussian Naive Bayes model was less accurate than our Multinomial Naive Bayes.\n",
    "\n",
    "This model was overfit because it had a perfect training score with a lesser testing score of 0.89.  This testing score is still higher than our baseline model.  Our cross-validation score shows that our model may not perform as well against unseen data as it did against our test data since the testing score was higher than the cross-validation score.  While the Multinomial Naive Bayes model is certainly our more effective Naive Bayes model, I feel as is all classification problems should attempt to fit a KNN model, so I will set that up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('knn', KNeighborsClassifier())])\n",
    "\n",
    "# set hyperparameters\n",
    "\n",
    "pipe_params = {'vectorizer__ngram_range'  : [(1,1),(1,2), (1,3)],\n",
    "              'vectorizer__stop_words': [None, 'english', stopwords],\n",
    "              'tfidf__use_idf'  : [True, False],\n",
    "              'knn__n_neighbors': [3,5,9],\n",
    "              'knn__weights' : ['uniform', 'distance'], # distance gives more predictive power to closer neighbors\n",
    "               'knn__p': [1,2,'p'] # allows us to test Minkowski, Euclidean, and Manhattan measurements\n",
    "               \n",
    "              }\n",
    "\n",
    "gsknn = GridSearchCV(pipe,\n",
    "                 param_grid = pipe_params,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy',\n",
    "                 verbose = 2   \n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I want to try to get the best result for our KNN model, I set up a gridsearch to run 2120 possible models (this took a little time) in order to return the best fit.  For hyperparameters, we looked at 1-grams, 2-grams, and 3-grams.  We tried with my custom stopwords, english stopwords, and no stopwords.  We tried it both with and and without the use of tfidf.  We also tried running it by looking at the closest 3,5, and 9 neighbors, tried it by weighting and not-weighting neighbor distance, all with the Euclidean, Minkowski, and Mahattan metrics.\n",
    "\n",
    "Each model went through a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed: 16.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n"
     ]
    }
   ],
   "source": [
    "knn_fit = gsknn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knn__n_neighbors': 9,\n",
       " 'knn__p': 2,\n",
       " 'knn__weights': 'distance',\n",
       " 'tfidf__use_idf': True,\n",
       " 'vectorizer__ngram_range': (1, 3),\n",
       " 'vectorizer__stop_words': ['the',\n",
       "  'and',\n",
       "  'to',\n",
       "  'of',\n",
       "  'it',\n",
       "  'in',\n",
       "  'is',\n",
       "  'that',\n",
       "  'for',\n",
       "  'this',\n",
       "  'but',\n",
       "  'was',\n",
       "  'on',\n",
       "  'just',\n",
       "  'season',\n",
       "  'you',\n",
       "  'show',\n",
       "  'with',\n",
       "  'he',\n",
       "  'so',\n",
       "  'be',\n",
       "  'my',\n",
       "  'have',\n",
       "  'like',\n",
       "  'they',\n",
       "  'or',\n",
       "  'can',\n",
       "  'what',\n",
       "  'me',\n",
       "  'episode',\n",
       "  'of the',\n",
       "  'at',\n",
       "  'are',\n",
       "  'she',\n",
       "  'as',\n",
       "  'all',\n",
       "  'in the',\n",
       "  'about',\n",
       "  'there',\n",
       "  'from',\n",
       "  'if',\n",
       "  'her',\n",
       "  'when',\n",
       "  'not',\n",
       "  'would',\n",
       "  'one',\n",
       "  'know',\n",
       "  'has',\n",
       "  'the show',\n",
       "  'his',\n",
       "  'up',\n",
       "  'out',\n",
       "  'we',\n",
       "  'an',\n",
       "  'time',\n",
       "  'think',\n",
       "  'how',\n",
       "  'get',\n",
       "  'really',\n",
       "  'do',\n",
       "  'who',\n",
       "  'first',\n",
       "  'some',\n",
       "  'don',\n",
       "  'by',\n",
       "  'been']}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_fit.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8952380952380953"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_fit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8752380952380954"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(knn_fit.best_estimator_, X, y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best fit involed predictions off the 5 nearest neighbors while using the Euclidean metric, assigning weight to the neighbors based on distance.  This included use of TFIDF with the ngram range of (1,3) and sklearn's English stop words.\n",
    "\n",
    "This model was overfit because it had a perfect training score, which was higher than our 0.89 testing score.  This is still more accurate than our baseline model.  Our cross-validation score shows that our model may not perform as well against unseen data as it did against our test data since the testing score was higher than the cross-validation score.  Lastly, I want to attempt a Support Vector Machine model in order to see if we can better classify in a multidimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([ #instantiate\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svc', SVC())])\n",
    "\n",
    "# set hyperparameters\n",
    "\n",
    "pipe_params = {'vectorizer__ngram_range'  : [(1,1),(1,2), (1,3)],\n",
    "              'vectorizer__stop_words': [None, 'english', stopwords],\n",
    "              'tfidf__use_idf'  : [True, False],\n",
    "               'svc__kernel': ['linear', 'poly', 'rbf'],\n",
    "               'svc__random_state': [63]\n",
    "               \n",
    "              }\n",
    "\n",
    "gssvc = GridSearchCV(pipe,\n",
    "                 param_grid = pipe_params,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy',\n",
    "                 verbose = 2   \n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I want to try to get the best result for our SVM model, I set up a gridsearch to run 90 possible models in order to return the best fit.  For hyperparameters, we looked at 1-grams, 2-grams, and 3-grams.  We tried with my custom stopwords, english stopwords, and no stopwords.  We tried it both with and and without the use of tfidf.  Lastly we tried all 3 different kernels that this method allows.\n",
    "\n",
    "Each model went through a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.3s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.2s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.0s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.0s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.0s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.6s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   6.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.2s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   4.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   4.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.2s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.0s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.0s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.0s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.6s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.3s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   4.3s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   4.3s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   4.6s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   4.0s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   6.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.2s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.6s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   4.2s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   2.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   2.6s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   3.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   2.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   2.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   4.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.6s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.6s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   8.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   8.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   9.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   7.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.6s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.0s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.6s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   4.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   5.6s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   4.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.0s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   8.0s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.0s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   4.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   3.0s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   4.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.6s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   2.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   2.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   2.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   4.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   4.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   6.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.5s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   3.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   3.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.5s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   4.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   2.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   2.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   4.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   2.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   2.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.5s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   2.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   1.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   4.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   4.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   4.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   4.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   4.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   3.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   3.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   3.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   3.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   4.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   4.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'know', 'has', 'the show', 'his', 'up', 'out', 'we', 'an', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 270 out of 270 | elapsed: 12.3min finished\n"
     ]
    }
   ],
   "source": [
    "sv_fit = gssvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993650793650793"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_fit.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9352380952380952"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_fit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9157142857142857"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(sv_fit.best_estimator_, X, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svc__kernel': 'linear',\n",
       " 'svc__random_state': 63,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vectorizer__ngram_range': (1, 2),\n",
       " 'vectorizer__stop_words': 'english'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_fit.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best fit involed using the linear kernel with TFIDF, a ngram range of (1,2), and sklearn's English stopwords.\n",
    "\n",
    "This model was overfit because it had a .999 training score, which was higher than our 0.940 testing score.  This is still more accurate than our baseline model.  Our cross-validation score shows that our model may not perform as well against unseen data as it did against our test data since the testing score was higher than the cross-validation score. \n",
    "\n",
    "Let's compare these models to see which one we should look to further evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy score for our baseline model is: 0.5148.\n",
      "Training accuracy score for our Logistic Regression model is: 1.0.\n",
      "Testing accuracy score for our Logistic Regression model is: 0.9352.\n",
      "Training accuracy score for our Multinomial Naive Bayes model is: 1.0.\n",
      "Testing accuracy score for our Multinomial Naive Bayes model is: 0.9352.\n",
      "Training accuracy score for our Gaussian Naive Bayes model is: 1.0.\n",
      "Testing accuracy score for our Gaussian Naive Bayes model is: 0.8876.\n",
      "Training accuracy score for our KNN model is: 1.0.\n",
      "Testing accuracy score for our KNN  model is: 0.8952.\n",
      "Training accuracy score for our Support Vector model is: 0.9994.\n",
      "Testing accuracy score for our Support Vector model is: 0.9352.\n"
     ]
    }
   ],
   "source": [
    "print(f'Testing accuracy score for our baseline model is: {round(baseline, 4)}.')\n",
    "\n",
    "\n",
    "print(f'Training accuracy score for our Logistic Regression model is: {round(lr_fit.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our Logistic Regression model is: {round(lr_fit.score(X_test, y_test),4)}.')\n",
    "\n",
    "\n",
    "print(f'Training accuracy score for our Multinomial Naive Bayes model is: {round(mnb_fit.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our Multinomial Naive Bayes model is: {round(mnb_fit.score(X_test, y_test),4)}.')\n",
    "\n",
    "print(f'Training accuracy score for our Gaussian Naive Bayes model is: {round(gnb_fit.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our Gaussian Naive Bayes model is: {round(gnb_fit.score(X_test, y_test),4)}.')\n",
    "\n",
    "print(f'Training accuracy score for our KNN model is: {round(knn_fit.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our KNN  model is: {round(knn_fit.score(X_test, y_test),4)}.')\n",
    "\n",
    "print(f'Training accuracy score for our Support Vector model is: {round(sv_fit.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our Support Vector model is: {round(sv_fit.score(X_test, y_test),4)}.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models were overfit since they all had a better training score than testing score.  For a predictive model, I would select the Multinomial Naive Bayes model because it had the highest testing score and cross validation score, which would imply that it would perform the best if exposed to unseen data.  For interpretability purposes, I am selecting the Logistic Regression model.  The Logistic Regression model performed similarly enough to the Multinomial Naive Bayes model, so I can justify this decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I want to create a confusion matrix, so I can find the accuracy, specificity, sensitivity, and precision rates for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = lr_fit.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, lr_preds).ravel() # From Danielle Medellin's confusion matrix setup\n",
    "cm = confusion_matrix(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[234,  21],\n",
       "       [ 13, 257]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The Good Place</th>\n",
       "      <th>Parks &amp; Rec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The Good Place</th>\n",
       "      <td>234</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parks &amp; Rec</th>\n",
       "      <td>13</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                The Good Place  Parks & Rec\n",
       "The Good Place             234           21\n",
       "Parks & Rec                 13          257"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_df = pd.DataFrame(cm)\n",
    "cm_df.columns = ['The Good Place', 'Parks & Rec']\n",
    "\n",
    "cm_df.index = ['The Good Place', \"Parks & Rec\"]\n",
    "#This adds the appropriate row and column names for the confusion matrix\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm_df, annot = True, fmt = 'g', cmap = 'Blues')\n",
    "plt.title('Confusion Matrix', size = 15)\n",
    "plt.ylabel('Predictions', size = 15)\n",
    "plt.xlabel('Actual Class', size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Rate: 0.9352380952380952\n",
      "Sensitivity Rate: 0.9518518518518518\n",
      "Specificity Rate: 0.9176470588235294\n",
      "Precision Rate: 0.9244604316546763\n",
      "True Negative Rate: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp + tn)/(tp+fn+fp+tn)\n",
    "print(f'Accuracy Rate: {accuracy}')\n",
    "\n",
    "sensitivity = tp/(tp+fn)\n",
    "print(f'Sensitivity Rate: {sensitivity}')\n",
    "            \n",
    "specificity = tn/(tn+fp)\n",
    "print(f'Specificity Rate: {specificity}')\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "print(f'Precision Rate: {precision}')\n",
    "\n",
    "true_neg = tn/(tn+fn)\n",
    "print(f'True Negative Rate: {true_neg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy rate states that 94% of all predictions the model made were correct.\n",
    "\n",
    "Our sensitivity rate states that 95% of the time that a row was in the Parks & Rec subreddit, our model was able to predict the row correctly.\n",
    "\n",
    "Our specificity rate states that 92% of the time that a row was not in the Parks & Rec subreddit, our model was able to predict the row correctly.  Since Parks & Rec was our majority class, I'm not surprised to see that our model had greater sensitivity than specificity, given the sensitivity/specificity tradeoff.\n",
    "\n",
    "Our precision rate states that out of all the times our model predicted our majority class (Parks & Rec), our model was correct 92% of the time.\n",
    "\n",
    "Our true negative rate states that out of all the times our model predicted our minority class (The Good Place), our model was correct 95% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to be able to evaluate the coeffiecients to get a sense as to what terms give the model its greatest predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = {\n",
    "    \"column\" : lr_fit.best_estimator_.steps[0][1].get_feature_names(), \n",
    "    \"coef\"   : lr_fit.best_estimator_.steps[2][1].coef_\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '00pm',\n",
       " '01',\n",
       " '02',\n",
       " '021b522cd30caed14d644041db53a4ec',\n",
       " '0275ad358fe747cae7ad2fdbb8bda1a3',\n",
       " '03x12',\n",
       " '04',\n",
       " '045c1aa1b90f61f6f712df4d0b90d62c',\n",
       " '05',\n",
       " '0583',\n",
       " '06',\n",
       " '08',\n",
       " '09',\n",
       " '0edipamaas',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '1000x500',\n",
       " '1003',\n",
       " '100lbs',\n",
       " '100x',\n",
       " '101',\n",
       " '104',\n",
       " '108',\n",
       " '1080p',\n",
       " '109',\n",
       " '10hr',\n",
       " '10jatk',\n",
       " '10th',\n",
       " '11',\n",
       " '110',\n",
       " '11am',\n",
       " '11th',\n",
       " '12',\n",
       " '1201869241',\n",
       " '121',\n",
       " '122',\n",
       " '12358w',\n",
       " '123889331',\n",
       " '125',\n",
       " '128',\n",
       " '13',\n",
       " '136',\n",
       " '1373996494',\n",
       " '13dtre1okb0',\n",
       " '13mins',\n",
       " '14',\n",
       " '146',\n",
       " '14thyear',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '151',\n",
       " '1518',\n",
       " '1520',\n",
       " '154',\n",
       " '159',\n",
       " '16',\n",
       " '1650',\n",
       " '17',\n",
       " '18',\n",
       " '1800s',\n",
       " '181188768',\n",
       " '1865',\n",
       " '19',\n",
       " '1926',\n",
       " '1944',\n",
       " '1945',\n",
       " '1972',\n",
       " '1996',\n",
       " '1997',\n",
       " '1ksupport',\n",
       " '1nqtif',\n",
       " '1o2oredoblueo',\n",
       " '1st',\n",
       " '1u1',\n",
       " '1x10',\n",
       " '20',\n",
       " '200',\n",
       " '2002',\n",
       " '2009',\n",
       " '201',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '20160919173629',\n",
       " '2017',\n",
       " '2018',\n",
       " '202',\n",
       " '2043',\n",
       " '208327',\n",
       " '20and',\n",
       " '20good',\n",
       " '20place',\n",
       " '20recreation',\n",
       " '20s',\n",
       " '20th',\n",
       " '20tht',\n",
       " '20tremendous',\n",
       " '21',\n",
       " '210',\n",
       " '215',\n",
       " '21h54',\n",
       " '21st',\n",
       " '22',\n",
       " '226',\n",
       " '23',\n",
       " '23019677',\n",
       " '234634',\n",
       " '24',\n",
       " '241031',\n",
       " '246',\n",
       " '24th',\n",
       " '25',\n",
       " '255',\n",
       " '26',\n",
       " '26124724',\n",
       " '27',\n",
       " '27smq3',\n",
       " '2814werewolf',\n",
       " '2885',\n",
       " '28ds7i',\n",
       " '28rbfv',\n",
       " '28th',\n",
       " '29',\n",
       " '29th',\n",
       " '2ailmg2l8rtaa',\n",
       " '2c159',\n",
       " '2caps',\n",
       " '2f',\n",
       " '2k16',\n",
       " '2lv7revtinjnwd5fy2rumq',\n",
       " '2nd',\n",
       " '2pst',\n",
       " '2x23',\n",
       " '30',\n",
       " '300',\n",
       " '301',\n",
       " '305',\n",
       " '30829964_320188731842082_2210264181398044672_n',\n",
       " '30pm',\n",
       " '30th',\n",
       " '31',\n",
       " '310',\n",
       " '311',\n",
       " '311711',\n",
       " '32',\n",
       " '322',\n",
       " '3232',\n",
       " '32968',\n",
       " '33316',\n",
       " '33627',\n",
       " '33878',\n",
       " '34205',\n",
       " '34460',\n",
       " '35',\n",
       " '36n05l',\n",
       " '36th',\n",
       " '37',\n",
       " '3750',\n",
       " '3857',\n",
       " '392',\n",
       " '3afkk3',\n",
       " '3am',\n",
       " '3avyad',\n",
       " '3bob',\n",
       " '3dmovies',\n",
       " '3fnbi',\n",
       " '3kool5you',\n",
       " '3rd',\n",
       " '3x06',\n",
       " '3x0fk5f9e7b11',\n",
       " '3x11',\n",
       " '3y2flfkz51431',\n",
       " '40',\n",
       " '40221',\n",
       " '405',\n",
       " '40s',\n",
       " '41',\n",
       " '420',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46mrkb2rhwv01',\n",
       " '48',\n",
       " '480p',\n",
       " '485',\n",
       " '4gdw0qs03qs',\n",
       " '4gsylp',\n",
       " '4mwj81',\n",
       " '4np4wsb',\n",
       " '4th',\n",
       " '4x01',\n",
       " '4x2',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '50th',\n",
       " '50xx853tjhx21',\n",
       " '52',\n",
       " '520',\n",
       " '521',\n",
       " '53',\n",
       " '55',\n",
       " '551',\n",
       " '570n3d_dud3',\n",
       " '575658',\n",
       " '57hggo',\n",
       " '58',\n",
       " '5b9d7208',\n",
       " '5e',\n",
       " '5k',\n",
       " '5p0juq',\n",
       " '5pm',\n",
       " '5th',\n",
       " '60',\n",
       " '60134',\n",
       " '612',\n",
       " '61310168',\n",
       " '6141747',\n",
       " '63',\n",
       " '668',\n",
       " '68',\n",
       " '6th',\n",
       " '720',\n",
       " '720p',\n",
       " '73',\n",
       " '75',\n",
       " '750',\n",
       " '78',\n",
       " '7ncbq3nti4x21',\n",
       " '7pm',\n",
       " '7th',\n",
       " '7x03',\n",
       " '7zci_obml_g',\n",
       " '80',\n",
       " '800',\n",
       " '800ish',\n",
       " '801',\n",
       " '802',\n",
       " '80s',\n",
       " '826la',\n",
       " '82kets',\n",
       " '83',\n",
       " '85',\n",
       " '862240152583901184',\n",
       " '89',\n",
       " '8mpbd',\n",
       " '8pm',\n",
       " '8th',\n",
       " '90',\n",
       " '90l8rk',\n",
       " '90s',\n",
       " '92',\n",
       " '948',\n",
       " '98',\n",
       " '98252',\n",
       " '99',\n",
       " '997',\n",
       " '9c',\n",
       " '9pm',\n",
       " '9th',\n",
       " '___',\n",
       " '___________________________________________________',\n",
       " '_emordnilap',\n",
       " '_in_',\n",
       " '_inc',\n",
       " '_itdoesntmatter_',\n",
       " '_nbbgrbxrhs',\n",
       " '_potatardis_',\n",
       " '_should_',\n",
       " '_stole_',\n",
       " '_very_stable_genius_',\n",
       " '_wykprojectalpha_',\n",
       " '_your_face',\n",
       " 'a4b91fd97cfb18d057fd4ae4fec7c609',\n",
       " 'a_ronn',\n",
       " 'aaaaaaaaaoo',\n",
       " 'aaaaaaand',\n",
       " 'aaaand',\n",
       " 'aag11',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abbi',\n",
       " 'abc',\n",
       " 'abe',\n",
       " 'abeds_bananastand',\n",
       " 'abemoilan',\n",
       " 'abigail',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abjanets',\n",
       " 'able',\n",
       " 'aboutabruise',\n",
       " 'abrahamic',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolve',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'abstain',\n",
       " 'absurdist',\n",
       " 'absurdity',\n",
       " 'abuse',\n",
       " 'abyss',\n",
       " 'ac2bhappy',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'acamu5',\n",
       " 'accent',\n",
       " 'accented',\n",
       " 'accents',\n",
       " 'accept',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accidental_antilogy',\n",
       " 'accidentally',\n",
       " 'accidents',\n",
       " 'accolades',\n",
       " 'accommodating',\n",
       " 'accompanied',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accountant',\n",
       " 'accounting',\n",
       " 'accrue',\n",
       " 'accrued',\n",
       " 'accumulate',\n",
       " 'accused',\n",
       " 'ache',\n",
       " 'aches',\n",
       " 'achieve',\n",
       " 'achievements',\n",
       " 'achieving',\n",
       " 'acid',\n",
       " 'acidcat',\n",
       " 'acknowledge',\n",
       " 'acknowledgement',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acmorgan',\n",
       " 'acquaintance',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'actively',\n",
       " 'activist',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actuality',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'adapting',\n",
       " 'adate',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addict',\n",
       " 'addiction',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'addon',\n",
       " 'addons',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adele',\n",
       " 'adept',\n",
       " 'adjacent',\n",
       " 'administration',\n",
       " 'admirable',\n",
       " 'admiralofawesomeness',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admiringly',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admittance',\n",
       " 'admitted',\n",
       " 'admittedly',\n",
       " 'admitting',\n",
       " 'ado',\n",
       " 'adolescence',\n",
       " 'adopted',\n",
       " 'adorable',\n",
       " 'adorableexplosions',\n",
       " 'adorably',\n",
       " 'adore',\n",
       " 'adores',\n",
       " 'adrimfayn',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advancement',\n",
       " 'advantage',\n",
       " 'adventists',\n",
       " 'adversity',\n",
       " 'advert',\n",
       " 'advice',\n",
       " 'advised',\n",
       " 'advocate',\n",
       " 'aew',\n",
       " 'af',\n",
       " 'afaik',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affects',\n",
       " 'affiliated',\n",
       " 'affiliates',\n",
       " 'afford',\n",
       " 'affords',\n",
       " 'afhy6wa',\n",
       " 'aficionados',\n",
       " 'afraid',\n",
       " 'african',\n",
       " 'afterlife',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'agnes',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agrees',\n",
       " 'agt',\n",
       " 'ah',\n",
       " 'ahainen',\n",
       " 'ahead',\n",
       " 'ahope4836',\n",
       " 'ai',\n",
       " 'aided',\n",
       " 'aidenpng',\n",
       " 'aims',\n",
       " 'air',\n",
       " 'airdate',\n",
       " 'aired',\n",
       " 'airing',\n",
       " 'airport',\n",
       " 'airs',\n",
       " 'aka',\n",
       " 'akanefive',\n",
       " 'al',\n",
       " 'alafolie29',\n",
       " 'alamedagrace',\n",
       " 'alarmed',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'albusseverus14',\n",
       " 'alcohol',\n",
       " 'aldeki',\n",
       " 'aldoushuxleyjr',\n",
       " 'alecjperkins213',\n",
       " 'alert',\n",
       " 'alex',\n",
       " 'alexa',\n",
       " 'alexgreen239',\n",
       " 'alexisgreat420',\n",
       " 'alexpenn',\n",
       " 'alias',\n",
       " 'alie',\n",
       " 'alien',\n",
       " 'alienation',\n",
       " 'aligned',\n",
       " 'aligning',\n",
       " 'alignment',\n",
       " 'alivanis',\n",
       " 'alive',\n",
       " 'alived',\n",
       " 'allegedly',\n",
       " 'allegiance',\n",
       " 'allegorical',\n",
       " 'allegory',\n",
       " 'alliance',\n",
       " 'allies',\n",
       " 'allot',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allude',\n",
       " 'almighty',\n",
       " 'almonds',\n",
       " 'alongside',\n",
       " 'alot',\n",
       " 'alps',\n",
       " 'alright',\n",
       " 'altar',\n",
       " 'altered',\n",
       " 'altering',\n",
       " 'alternate',\n",
       " 'alternated',\n",
       " 'alternating',\n",
       " 'alternative',\n",
       " 'alternatively',\n",
       " 'alternatives',\n",
       " 'altruism',\n",
       " 'altruistic',\n",
       " 'alwayspro',\n",
       " 'ama',\n",
       " 'amateur',\n",
       " 'amazed',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazingmexican9',\n",
       " 'amazon',\n",
       " 'ambassador',\n",
       " 'ambassadors',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambitious',\n",
       " 'ambrose',\n",
       " 'amend',\n",
       " 'amenities',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'americorps',\n",
       " 'amgmercedesbaby',\n",
       " 'amicable',\n",
       " 'amino',\n",
       " 'aminoapps',\n",
       " 'amnesty',\n",
       " 'amounts',\n",
       " 'amp',\n",
       " 'amplified',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'anagonye',\n",
       " 'anal',\n",
       " 'analyse',\n",
       " 'analysing',\n",
       " 'analysis',\n",
       " 'ancestors',\n",
       " 'anchor',\n",
       " 'anchoring',\n",
       " 'ancient',\n",
       " 'andbruno',\n",
       " 'anderson',\n",
       " 'andrei',\n",
       " 'andrew9360',\n",
       " 'andy',\n",
       " 'andys',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'angelic',\n",
       " 'angelique',\n",
       " 'angelnike',\n",
       " 'angeloanthony',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'angle',\n",
       " 'angry',\n",
       " 'anguskirk',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'animation',\n",
       " 'anime',\n",
       " 'aniston',\n",
       " 'ann',\n",
       " 'annalise13ra',\n",
       " 'annan',\n",
       " 'annapurna',\n",
       " 'anne',\n",
       " 'anneperkins',\n",
       " 'anniversary',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announcements',\n",
       " 'announces',\n",
       " 'annoy',\n",
       " 'annoyance',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annoyingly',\n",
       " 'annoys',\n",
       " 'anomaly',\n",
       " 'anonymous21347',\n",
       " 'anotherandomer',\n",
       " 'ansari',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'ant',\n",
       " 'anti',\n",
       " 'anticipate',\n",
       " 'antics',\n",
       " 'antiej',\n",
       " 'antorqs',\n",
       " 'anxieties',\n",
       " 'anxiety',\n",
       " 'anybody',\n",
       " 'anycase',\n",
       " 'anymore',\n",
       " 'anyones',\n",
       " 'anytime',\n",
       " 'anyways',\n",
       " 'anzari',\n",
       " 'ap',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apathetic',\n",
       " 'apathy',\n",
       " 'apathylikes',\n",
       " 'aphroditeandthexbox',\n",
       " 'apocalypse_meow2',\n",
       " 'apocalyptic',\n",
       " 'apologetic',\n",
       " 'apologies',\n",
       " 'apologize',\n",
       " 'app',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appealed',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appearances',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'appetite',\n",
       " 'appetizers',\n",
       " 'apple',\n",
       " 'apples',\n",
       " 'applied',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'appointed',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciation',\n",
       " 'appreciative',\n",
       " 'apprehensive',\n",
       " 'apprentice',\n",
       " 'approach',\n",
       " 'approached',\n",
       " 'approaches',\n",
       " 'appropriate',\n",
       " 'approval',\n",
       " 'approve',\n",
       " 'approved',\n",
       " 'approx',\n",
       " 'approximation',\n",
       " 'apri',\n",
       " 'april',\n",
       " 'aprils',\n",
       " 'aproach',\n",
       " 'apt',\n",
       " 'aquatic',\n",
       " 'aquawoman68',\n",
       " 'aquickburner',\n",
       " 'ar',\n",
       " 'ar417',\n",
       " 'arabic',\n",
       " 'arbitrarily',\n",
       " 'arc',\n",
       " 'arcade',\n",
       " 'arcanine',\n",
       " 'arch',\n",
       " 'archangel',\n",
       " 'archery',\n",
       " 'archetype',\n",
       " 'architect',\n",
       " 'architects',\n",
       " 'architectural',\n",
       " 'architecture',\n",
       " 'archive',\n",
       " 'archives',\n",
       " 'arcy',\n",
       " 'ardyraindropsrd',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'aren',\n",
       " 'arent',\n",
       " 'argenteam',\n",
       " 'arguable',\n",
       " 'arguably',\n",
       " 'argue',\n",
       " 'argued',\n",
       " 'argues',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'aria',\n",
       " 'ariana',\n",
       " 'arinlome',\n",
       " 'arisen',\n",
       " 'aristotle',\n",
       " 'arizona',\n",
       " 'arjungames73',\n",
       " 'arm',\n",
       " 'armies',\n",
       " 'armisen',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'arnett',\n",
       " 'arranged',\n",
       " 'arrangement',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'arrivals',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arrow',\n",
       " 'arrows',\n",
       " 'art',\n",
       " 'artemis_dubois',\n",
       " 'artichoke19',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'articlethumbs',\n",
       " 'artificial',\n",
       " 'artist',\n",
       " 'arts',\n",
       " 'artyen',\n",
       " 'asapaspossible',\n",
       " 'asbhopal1',\n",
       " 'ascultone21',\n",
       " 'ashketchum1845',\n",
       " 'ashole',\n",
       " 'asian',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'ask_adam_scott_anything',\n",
       " 'ask_adam_scott_me_anything',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'askronswanson',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'aspberger',\n",
       " 'aspect',\n",
       " 'aspects',\n",
       " 'aspirations',\n",
       " 'ass',\n",
       " 'assassinated',\n",
       " 'assert',\n",
       " 'asses',\n",
       " 'assholes',\n",
       " 'assign',\n",
       " 'assigned',\n",
       " 'assigning',\n",
       " 'assignment',\n",
       " 'assigns',\n",
       " 'assing',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'associated',\n",
       " 'association',\n",
       " 'assume',\n",
       " 'assumed',\n",
       " 'assuming',\n",
       " 'assumption',\n",
       " 'assuredly',\n",
       " 'astechgold',\n",
       " 'astonish',\n",
       " 'astonished',\n",
       " 'astrocanyounaut',\n",
       " 'ate',\n",
       " 'athenasnike',\n",
       " 'atilegacy',\n",
       " 'atlanta',\n",
       " 'atlantas',\n",
       " 'atleast',\n",
       " 'atmosphere',\n",
       " 'atmospheric',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attacks',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempts',\n",
       " 'attended',\n",
       " 'attendee',\n",
       " 'attendees',\n",
       " 'attending',\n",
       " 'attention',\n",
       " 'attican101',\n",
       " 'attire',\n",
       " 'attitude',\n",
       " 'attitudes',\n",
       " 'attract',\n",
       " 'attracted',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'attractiveness',\n",
       " 'attributed',\n",
       " 'aubrey',\n",
       " 'audiblecoupon',\n",
       " 'audibly',\n",
       " 'audience',\n",
       " 'audio',\n",
       " 'august',\n",
       " 'aura',\n",
       " 'aurelio23',\n",
       " 'austin',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'authentic',\n",
       " 'authentically',\n",
       " 'authenticity',\n",
       " 'author',\n",
       " 'authorities',\n",
       " 'authority',\n",
       " 'authuser',\n",
       " 'autism',\n",
       " 'autobiographies',\n",
       " 'autocorrect',\n",
       " 'autographed',\n",
       " 'autographs',\n",
       " 'automatic',\n",
       " 'automatically',\n",
       " 'autres',\n",
       " 'autumn',\n",
       " 'auxeralis',\n",
       " 'av',\n",
       " 'avail',\n",
       " 'available',\n",
       " 'availeable',\n",
       " 'avant',\n",
       " 'avclub',\n",
       " 'avengers',\n",
       " 'average',\n",
       " 'aveydey',\n",
       " 'avocado',\n",
       " 'avoid',\n",
       " 'avoiding',\n",
       " 'aw',\n",
       " 'awards',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awesomeness',\n",
       " 'awesometoenails',\n",
       " 'awful',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awry',\n",
       " 'aww',\n",
       " 'axe',\n",
       " 'azgrimes',\n",
       " 'aziz',\n",
       " 'aziz_ansaris_modern_romance_ama',\n",
       " 'b99',\n",
       " 'baad',\n",
       " 'baader',\n",
       " 'babe',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'babybuttoneyes',\n",
       " 'bachelor',\n",
       " 'backed',\n",
       " 'backer',\n",
       " 'backfires',\n",
       " 'background',\n",
       " 'backlog',\n",
       " 'backs',\n",
       " 'backstory',\n",
       " 'backtracking',\n",
       " 'backup',\n",
       " 'backwards',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'bad_riter',\n",
       " 'badass',\n",
       " 'badde00',\n",
       " 'badly',\n",
       " 'badness',\n",
       " 'bag',\n",
       " 'bags',\n",
       " 'baie',\n",
       " 'bailadelcorazon',\n",
       " 'bailout',\n",
       " 'baiting',\n",
       " 'baker',\n",
       " 'baking',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'balances',\n",
       " 'bald',\n",
       " 'ball',\n",
       " 'ballon',\n",
       " 'balloon',\n",
       " 'balloons',\n",
       " 'ballroom',\n",
       " 'balls',\n",
       " 'baltar',\n",
       " 'bambadjan',\n",
       " 'band',\n",
       " 'bands',\n",
       " 'bang',\n",
       " 'banger',\n",
       " 'bangs8',\n",
       " 'banjo',\n",
       " 'bank',\n",
       " 'bankrupt',\n",
       " 'bannanadog666',\n",
       " 'banned',\n",
       " 'bannedindc',\n",
       " 'banner',\n",
       " 'banquet',\n",
       " 'bans',\n",
       " 'bar',\n",
       " 'barbecue',\n",
       " 'barber',\n",
       " 'barcelona',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'bargain',\n",
       " 'barking',\n",
       " 'barkley',\n",
       " 'barkos',\n",
       " 'barmaid',\n",
       " 'barn',\n",
       " 'barney',\n",
       " 'barrage',\n",
       " 'barrel',\n",
       " 'barry',\n",
       " 'barshady18',\n",
       " 'bartender',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'based_life',\n",
       " 'baseless',\n",
       " 'baseman',\n",
       " 'basement',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basics',\n",
       " 'basing',\n",
       " 'basis',\n",
       " 'bass',\n",
       " 'bastardized',\n",
       " 'bastards',\n",
       " 'bastion_de_paraplui',\n",
       " 'bat',\n",
       " 'bathroom',\n",
       " 'bathrooms',\n",
       " 'batman',\n",
       " 'battle',\n",
       " 'battles',\n",
       " 'bbf2',\n",
       " 'bbq',\n",
       " 'bbylucy',\n",
       " 'bdreynolds',\n",
       " 'beach',\n",
       " 'beaker',\n",
       " 'bean',\n",
       " 'beanbag',\n",
       " 'beans',\n",
       " ...]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fit.best_estimator_.steps[0][1].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.83621655,  7.07615851, -0.22590275, ...,  6.27333222,\n",
       "         6.27333222,  6.27333222]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fit.best_estimator_.steps[2][1].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = lr_fit.best_estimator_.steps[2][1].coef_ # getting coefficients\n",
    "cols = lr_fit.best_estimator_.steps[0][1].get_feature_names() # getting column names\n",
    "cols = pd.Series(cols) # turn into series\n",
    "coefs = coefs[0]\n",
    "feature_coefs = pd.DataFrame(coefs, index=cols) # put in data frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coefs.columns = ['coefficient']\n",
    "feature_coefs['odds'] = np.exp(feature_coefs['coefficient'])\n",
    "top_pr_coefs = feature_coefs.sort_values('coefficient',ascending=False).head(15) # top 15 features for Parks & Rec\n",
    "top_gp_coefs = feature_coefs.sort_values('coefficient',ascending=False).tail(15) # top 15 features for The Good Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>-153.537635</td>\n",
       "      <td>2.086664e-67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michael</th>\n",
       "      <td>-138.611601</td>\n",
       "      <td>6.335000e-61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eleanor</th>\n",
       "      <td>-116.033632</td>\n",
       "      <td>4.047937e-51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>janet</th>\n",
       "      <td>-106.214002</td>\n",
       "      <td>7.444661e-47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>-103.724942</td>\n",
       "      <td>8.970773e-46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chidi</th>\n",
       "      <td>-94.554072</td>\n",
       "      <td>8.623596e-42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jason</th>\n",
       "      <td>-75.325480</td>\n",
       "      <td>1.934460e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>-71.672073</td>\n",
       "      <td>7.468171e-32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tahani</th>\n",
       "      <td>-65.772156</td>\n",
       "      <td>2.725935e-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tgp</th>\n",
       "      <td>-50.499172</td>\n",
       "      <td>1.170815e-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spoilers</th>\n",
       "      <td>-48.901906</td>\n",
       "      <td>5.783250e-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twist</th>\n",
       "      <td>-44.393370</td>\n",
       "      <td>5.250546e-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theory</th>\n",
       "      <td>-42.849422</td>\n",
       "      <td>2.458852e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shows</th>\n",
       "      <td>-37.848401</td>\n",
       "      <td>3.652989e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kristen</th>\n",
       "      <td>-37.094183</td>\n",
       "      <td>7.766066e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          coefficient          odds\n",
       "place     -153.537635  2.086664e-67\n",
       "michael   -138.611601  6.335000e-61\n",
       "eleanor   -116.033632  4.047937e-51\n",
       "janet     -106.214002  7.444661e-47\n",
       "good      -103.724942  8.970773e-46\n",
       "chidi      -94.554072  8.623596e-42\n",
       "jason      -75.325480  1.934460e-33\n",
       "bad        -71.672073  7.468171e-32\n",
       "tahani     -65.772156  2.725935e-29\n",
       "tgp        -50.499172  1.170815e-22\n",
       "spoilers   -48.901906  5.783250e-22\n",
       "twist      -44.393370  5.250546e-20\n",
       "theory     -42.849422  2.458852e-19\n",
       "shows      -37.848401  3.652989e-17\n",
       "kristen    -37.094183  7.766066e-17"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_gp_coefs.sort_values(by = 'coefficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since The Good Place is the 0 in our binomial, strong coeffiecients will be the most negative coefficients.  This was sorted for better readability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>leslie</th>\n",
       "      <td>120.701440</td>\n",
       "      <td>2.630081e+52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ron</th>\n",
       "      <td>94.380753</td>\n",
       "      <td>9.750803e+40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>andy</th>\n",
       "      <td>79.163195</td>\n",
       "      <td>2.399599e+34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parks</th>\n",
       "      <td>74.829885</td>\n",
       "      <td>3.149244e+32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ben</th>\n",
       "      <td>67.144436</td>\n",
       "      <td>1.446965e+29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tom</th>\n",
       "      <td>59.661110</td>\n",
       "      <td>8.137500e+25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec</th>\n",
       "      <td>59.203671</td>\n",
       "      <td>5.150243e+25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>april</th>\n",
       "      <td>56.106797</td>\n",
       "      <td>2.327408e+24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ann</th>\n",
       "      <td>45.684276</td>\n",
       "      <td>6.925148e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jerry</th>\n",
       "      <td>45.139939</td>\n",
       "      <td>4.018154e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pandr</th>\n",
       "      <td>44.598607</td>\n",
       "      <td>2.338453e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chris</th>\n",
       "      <td>43.498881</td>\n",
       "      <td>7.786175e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pawnee</th>\n",
       "      <td>38.960395</td>\n",
       "      <td>8.323092e+16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swanson</th>\n",
       "      <td>34.237140</td>\n",
       "      <td>7.396070e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mark</th>\n",
       "      <td>34.200973</td>\n",
       "      <td>7.133355e+14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         coefficient          odds\n",
       "leslie    120.701440  2.630081e+52\n",
       "ron        94.380753  9.750803e+40\n",
       "andy       79.163195  2.399599e+34\n",
       "parks      74.829885  3.149244e+32\n",
       "ben        67.144436  1.446965e+29\n",
       "tom        59.661110  8.137500e+25\n",
       "rec        59.203671  5.150243e+25\n",
       "april      56.106797  2.327408e+24\n",
       "ann        45.684276  6.925148e+19\n",
       "jerry      45.139939  4.018154e+19\n",
       "pandr      44.598607  2.338453e+19\n",
       "chris      43.498881  7.786175e+18\n",
       "pawnee     38.960395  8.323092e+16\n",
       "swanson    34.237140  7.396070e+14\n",
       "mark       34.200973  7.133355e+14"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pr_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the impactful coefficients unsurprisingly lined up with a character's name.  For example, if the main character of Parks & Rec, Leslie, was in a row, that row was over 2.630081 sexdecillion more likely to belong to Parks & Rec.  Our strongest coefficient for a Good Place character, Michael, states that a row containing his name was over 6.335000 novemdecillion times more likely to be classified as belonging to the Good Place (which was pretty much the same odds as getting sent to the bad place in the show).\n",
    "\n",
    "Considering the model seemed to have mostly classified the characters correctly, I want to look further into my misclassifications, so I can start coming up with my show ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1371,  100, 1637, 1117, 1186, 1607,  933, 1517,  514, 1438, 1243,\n",
       "            1624, 1930, 1453,  923, 2075, 1156, 1210,  822, 1248,  349, 1698,\n",
       "            1056, 1428,  214,  934, 2083, 1699, 1014, 1560,  418, 1600,  679,\n",
       "             742],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.loc[y_test != lr_preds].index #shows me misclassified rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = X_test[[1371,  100, 1637, 1117, 1186, 1607,  933, 1517,  514, 1438, 1243,\n",
    "            1624, 1930, 1453,  923, 2075,   70,  135, 1156,  572, 1210,  822,\n",
    "            1248,  349, 1698, 1056, 1428,  214,  934, 2083, 1699, 1014, 1560,\n",
    "             418, 1600,  679,  742]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclass_df = pd.DataFrame(misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all_text'], dtype='object')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclass_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>Season 3? Just finished binge watching this sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>If given the chance, would you rather be a mem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>Rewatch with my wife Just started a rewatch wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>Where can I find season three episodes 1-7? I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>Hey fat dinks, does anyone know where to watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>Season 2 is airing on Netflix in Australia, Ir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>Girlfriend likes her new nickname My girlfrien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>Do we know when season 2 premieres? I just sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>The cast in other shows? What are some good sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>Any ideas when season 2 will be out on Netflix...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>What line do you constantly repeat in your eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>Anyone else find Denise hilarious? She's a gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>Episode discussion season 1 episode 9 Someone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>Adam Scott on season 2 Why no Scott? He was a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>Can I start with the 2nd season without missin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>Title card font Does anyone know the font that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Appreciation for u/NightTrainDan Just some lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>need a gif: ben and leslie at grizzle in their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>This is my favorite show to binge the entire s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>What are your favorite quotes to say irl? Fals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>Will season 3 come to Netflix? If not, where c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>Is the real Li'l Seabastian still alive? I wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>I made a Twitter account that tweets out regul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>Gif Request: Does anyone have a gif of the sce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>I haven't laughed this hard in a good while Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>Hey Im the New Mod for /r/PandR, the Subreddit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>I think I just lost 5 years of my life... ...w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>I'M GOING TO TYPE EVERY WORD I KNOW! RECTANGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>Season 4 online Sorry if you guys get these th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>Parks and Rec reference I don't know if anybod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>I was Right! (for the most part i think?) www....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>Soooo... What did you think of the new episode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>Watched the whole show on a United flight... [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>Watching Season 4 Episode 10 for about the 5th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>Watching in the UK? I watched (and loved) seas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>Favorite quotes? \"I like people...places...and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>As our final season approaches... With the fin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               all_text\n",
       "1371  Season 3? Just finished binge watching this sh...\n",
       "100   If given the chance, would you rather be a mem...\n",
       "1637  Rewatch with my wife Just started a rewatch wi...\n",
       "1117  Where can I find season three episodes 1-7? I ...\n",
       "1186  Hey fat dinks, does anyone know where to watch...\n",
       "1607  Season 2 is airing on Netflix in Australia, Ir...\n",
       "933   Girlfriend likes her new nickname My girlfrien...\n",
       "1517  Do we know when season 2 premieres? I just sta...\n",
       "514   The cast in other shows? What are some good sh...\n",
       "1438  Any ideas when season 2 will be out on Netflix...\n",
       "1243  What line do you constantly repeat in your eve...\n",
       "1624  Anyone else find Denise hilarious? She's a gre...\n",
       "1930  Episode discussion season 1 episode 9 Someone ...\n",
       "1453  Adam Scott on season 2 Why no Scott? He was a ...\n",
       "923   Can I start with the 2nd season without missin...\n",
       "2075  Title card font Does anyone know the font that...\n",
       "70    Appreciation for u/NightTrainDan Just some lov...\n",
       "135   need a gif: ben and leslie at grizzle in their...\n",
       "1156  This is my favorite show to binge the entire s...\n",
       "572   What are your favorite quotes to say irl? Fals...\n",
       "1210  Will season 3 come to Netflix? If not, where c...\n",
       "822   Is the real Li'l Seabastian still alive? I wan...\n",
       "1248  I made a Twitter account that tweets out regul...\n",
       "349   Gif Request: Does anyone have a gif of the sce...\n",
       "1698  I haven't laughed this hard in a good while Th...\n",
       "1056  Hey Im the New Mod for /r/PandR, the Subreddit...\n",
       "1428  I think I just lost 5 years of my life... ...w...\n",
       "214   I'M GOING TO TYPE EVERY WORD I KNOW! RECTANGLE...\n",
       "934   Season 4 online Sorry if you guys get these th...\n",
       "2083  Parks and Rec reference I don't know if anybod...\n",
       "1699  I was Right! (for the most part i think?) www....\n",
       "1014  Soooo... What did you think of the new episode...\n",
       "1560  Watched the whole show on a United flight... [...\n",
       "418   Watching Season 4 Episode 10 for about the 5th...\n",
       "1600  Watching in the UK? I watched (and loved) seas...\n",
       "679   Favorite quotes? \"I like people...places...and...\n",
       "742   As our final season approaches... With the fin..."
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclass_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1371    Season 3? Just finished binge watching this sh...\n",
       "100     If given the chance, would you rather be a mem...\n",
       "1637    Rewatch with my wife Just started a rewatch wi...\n",
       "1117    Where can I find season three episodes 1-7? I ...\n",
       "1186    Hey fat dinks, does anyone know where to watch...\n",
       "1607    Season 2 is airing on Netflix in Australia, Ir...\n",
       "933     Girlfriend likes her new nickname My girlfrien...\n",
       "1517    Do we know when season 2 premieres? I just sta...\n",
       "514     The cast in other shows? What are some good sh...\n",
       "1438    Any ideas when season 2 will be out on Netflix...\n",
       "1243    What line do you constantly repeat in your eve...\n",
       "1624    Anyone else find Denise hilarious? She's a gre...\n",
       "1930    Episode discussion season 1 episode 9 Someone ...\n",
       "1453    Adam Scott on season 2 Why no Scott? He was a ...\n",
       "923     Can I start with the 2nd season without missin...\n",
       "2075    Title card font Does anyone know the font that...\n",
       "70      Appreciation for u/NightTrainDan Just some lov...\n",
       "135     need a gif: ben and leslie at grizzle in their...\n",
       "1156    This is my favorite show to binge the entire s...\n",
       "572     What are your favorite quotes to say irl? Fals...\n",
       "1210    Will season 3 come to Netflix? If not, where c...\n",
       "822     Is the real Li'l Seabastian still alive? I wan...\n",
       "1248    I made a Twitter account that tweets out regul...\n",
       "349     Gif Request: Does anyone have a gif of the sce...\n",
       "1698    I haven't laughed this hard in a good while Th...\n",
       "1056    Hey Im the New Mod for /r/PandR, the Subreddit...\n",
       "1428    I think I just lost 5 years of my life... ...w...\n",
       "214     I'M GOING TO TYPE EVERY WORD I KNOW! RECTANGLE...\n",
       "934     Season 4 online Sorry if you guys get these th...\n",
       "2083    Parks and Rec reference I don't know if anybod...\n",
       "1699    I was Right! (for the most part i think?) www....\n",
       "1014    Soooo... What did you think of the new episode...\n",
       "1560    Watched the whole show on a United flight... [...\n",
       "418     Watching Season 4 Episode 10 for about the 5th...\n",
       "1600    Watching in the UK? I watched (and loved) seas...\n",
       "679     Favorite quotes? \"I like people...places...and...\n",
       "742     As our final season approaches... With the fin...\n",
       "Name: all_text, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcvec = CountVectorizer(ngram_range= (1,2), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_count = fcvec.fit_transform(misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['05',\n",
       " '05 21',\n",
       " '10',\n",
       " '10 5th',\n",
       " '112',\n",
       " '112 episodes',\n",
       " '1st',\n",
       " '1st honest',\n",
       " '2014',\n",
       " '2014 05',\n",
       " '2015',\n",
       " '2015 http',\n",
       " '21',\n",
       " '21 parks',\n",
       " '2nd',\n",
       " '2nd loss',\n",
       " '2nd season',\n",
       " '32',\n",
       " '32 weeks',\n",
       " '5th',\n",
       " '5th time',\n",
       " '71hjl4',\n",
       " '71hjl4 yet_another_theory',\n",
       " '80113701',\n",
       " '80113701 ll',\n",
       " 'able',\n",
       " 'able watch',\n",
       " 'absolutely',\n",
       " 'absolutely love',\n",
       " 'account',\n",
       " 'account favorite',\n",
       " 'account jinmeister',\n",
       " 'account posts',\n",
       " 'account tweets',\n",
       " 'actors',\n",
       " 'actors outside',\n",
       " 'actress',\n",
       " 'actress steals',\n",
       " 'actually',\n",
       " 'actually thinking',\n",
       " 'adam',\n",
       " 'adam scott',\n",
       " 'adventure',\n",
       " 'adventure sure',\n",
       " 'afford',\n",
       " 'afford pay',\n",
       " 'air',\n",
       " 'air averages',\n",
       " 'air know',\n",
       " 'airing',\n",
       " 'airing netflix',\n",
       " 'airs',\n",
       " 'airs watch',\n",
       " 'alive',\n",
       " 'alive support',\n",
       " 'alive want',\n",
       " 'alternative',\n",
       " 'alternative badanimaldrawing',\n",
       " 'america',\n",
       " 'america megaphone',\n",
       " 'amp',\n",
       " 'amp crew',\n",
       " 'amp pawntakesrook',\n",
       " 'amp really',\n",
       " 'andy',\n",
       " 'andy realizes',\n",
       " 'angst',\n",
       " 'angst reliving',\n",
       " 'anti',\n",
       " 'anti spoiler',\n",
       " 'antsy',\n",
       " 'antsy best',\n",
       " 'anybody',\n",
       " 'anybody sites',\n",
       " 'anybody spotted',\n",
       " 'apartment',\n",
       " 'apartment save',\n",
       " 'apartments',\n",
       " 'apartments time',\n",
       " 'apologize',\n",
       " 'apologize having',\n",
       " 'app',\n",
       " 'app right',\n",
       " 'applicable',\n",
       " 'applicable countries',\n",
       " 'appreciation',\n",
       " 'appreciation nighttraindan',\n",
       " 'approaches',\n",
       " 'approaches final',\n",
       " 'approaching',\n",
       " 'approaching thought',\n",
       " 'appropriate',\n",
       " 'appropriate subreddit',\n",
       " 'area',\n",
       " 'area bit',\n",
       " 'aside',\n",
       " 'aside doubt',\n",
       " 'audixmusic',\n",
       " 'australia',\n",
       " 'australia ireland',\n",
       " 'available',\n",
       " 'available netflix',\n",
       " 'averages',\n",
       " 'averages eps',\n",
       " 'awful',\n",
       " 'awful week',\n",
       " 'bad',\n",
       " 'bad deleted',\n",
       " 'badanimaldrawing',\n",
       " 'ben',\n",
       " 'ben leslie',\n",
       " 'best',\n",
       " 'best cure',\n",
       " 'binge',\n",
       " 'binge entire',\n",
       " 'binge watching',\n",
       " 'binging',\n",
       " 'binging amp',\n",
       " 'birdpark',\n",
       " 'bit',\n",
       " 'bit picky',\n",
       " 'bot',\n",
       " 'bot account',\n",
       " 'bot running',\n",
       " 'break',\n",
       " 'break monotony',\n",
       " 'breaks',\n",
       " 'breaks sort',\n",
       " 'bring',\n",
       " 'bring smile',\n",
       " 'brumplestiltskins',\n",
       " 'burt',\n",
       " 'burt macklin',\n",
       " 'butthole',\n",
       " 'butthole xironbjorn',\n",
       " 'buy',\n",
       " 'buy sunshine',\n",
       " 'came',\n",
       " 'came mind',\n",
       " 'card',\n",
       " 'card font',\n",
       " 'card love',\n",
       " 'cast',\n",
       " 'cast amp',\n",
       " 'cast involved',\n",
       " 'cast shows',\n",
       " 'ccv21',\n",
       " 'chance',\n",
       " 'chance community',\n",
       " 'chance member',\n",
       " 'chanel',\n",
       " 'chanel ve',\n",
       " 'character',\n",
       " 'character development',\n",
       " 'character end',\n",
       " 'characters',\n",
       " 'characters amp',\n",
       " 'check',\n",
       " 'check goodies',\n",
       " 'check netflix',\n",
       " 'classicalmusictroll',\n",
       " 'clever',\n",
       " 'clever somename6',\n",
       " 'clockworkanomaly',\n",
       " 'close',\n",
       " 'close sinkhole',\n",
       " 'club',\n",
       " 'club treat',\n",
       " 'com',\n",
       " 'com 2014',\n",
       " 'com gallery',\n",
       " 'com tgpcaps',\n",
       " 'com thegoodplace',\n",
       " 'com title',\n",
       " 'come',\n",
       " 'come netflix',\n",
       " 'comin',\n",
       " 'comin shrampies',\n",
       " 'coming',\n",
       " 'coming wait',\n",
       " 'comment',\n",
       " 'comment use',\n",
       " 'comments',\n",
       " 'comments 71hjl4',\n",
       " 'community',\n",
       " 'community alive',\n",
       " 'constantly',\n",
       " 'constantly repeat',\n",
       " 'constantly want',\n",
       " 'context',\n",
       " 'context sure',\n",
       " 'count',\n",
       " 'count 112',\n",
       " 'countries',\n",
       " 'countries check',\n",
       " 'countries https',\n",
       " 'country',\n",
       " 'country don',\n",
       " 'course',\n",
       " 'course specifics',\n",
       " 'crew',\n",
       " 'crew thing',\n",
       " 'cure',\n",
       " 'cure angst',\n",
       " 'davect01',\n",
       " 'day',\n",
       " 'day available',\n",
       " 'day year',\n",
       " 'days',\n",
       " 'days work',\n",
       " 'decide',\n",
       " 'decide actually',\n",
       " 'delete',\n",
       " 'delete deleted',\n",
       " 'deleted',\n",
       " 'deleted deleted',\n",
       " 'demand',\n",
       " 'demand buy',\n",
       " 'denise',\n",
       " 'denise hilarious',\n",
       " 'development',\n",
       " 'development link',\n",
       " 'developmental',\n",
       " 'did',\n",
       " 'did guys',\n",
       " 'did think',\n",
       " 'dinks',\n",
       " 'dinks does',\n",
       " 'discovered',\n",
       " 'discovered don',\n",
       " 'discussion',\n",
       " 'discussion season',\n",
       " 'discussion starting',\n",
       " 'discussion thread',\n",
       " 'discussions',\n",
       " 'discussions personally',\n",
       " 'disguises',\n",
       " 'disguises text',\n",
       " 'does',\n",
       " 'does anybody',\n",
       " 'does gif',\n",
       " 'does happy',\n",
       " 'does know',\n",
       " 'don',\n",
       " 'don air',\n",
       " 'don know',\n",
       " 'don need',\n",
       " 'doubt',\n",
       " 'doubt comin',\n",
       " 'dressed',\n",
       " 'dressed talk',\n",
       " 'drinking',\n",
       " 'drinking wine',\n",
       " 'drumwizard101',\n",
       " 'drunk',\n",
       " 'drunk apologize',\n",
       " 'dundermifflin',\n",
       " 'dundermifflin goes',\n",
       " 'elmos',\n",
       " 'elmos andy',\n",
       " 'en',\n",
       " 'en wikipedia',\n",
       " 'end',\n",
       " 'end season',\n",
       " 'ending',\n",
       " 'ending season',\n",
       " 'england',\n",
       " 'england using',\n",
       " 'england ve',\n",
       " 'enjoy',\n",
       " 'enjoy birdpark',\n",
       " 'entire',\n",
       " 'entire episode',\n",
       " 'entire family',\n",
       " 'entire season',\n",
       " 'episode',\n",
       " 'episode 10',\n",
       " 'episode afford',\n",
       " 'episode ccv21',\n",
       " 'episode discussion',\n",
       " 'episode just',\n",
       " 'episode laughed',\n",
       " 'episode like',\n",
       " 'episode surely',\n",
       " 'episode wasn',\n",
       " 'episode watch',\n",
       " 'episodes',\n",
       " 'episodes 1st',\n",
       " 'episodes 32',\n",
       " 'episodes plan',\n",
       " 'episodes started',\n",
       " 'episodes week',\n",
       " 'eps',\n",
       " 'eps week',\n",
       " 'especially',\n",
       " 'especially appropriate',\n",
       " 'everyday',\n",
       " 'everyday life',\n",
       " 'ew',\n",
       " 'ew com',\n",
       " 'expect',\n",
       " 'expect http',\n",
       " 'extra',\n",
       " 'extra scenes',\n",
       " 'face',\n",
       " 'face lift',\n",
       " 'face makes',\n",
       " 'face started',\n",
       " 'fake',\n",
       " 'fake location',\n",
       " 'false',\n",
       " 'false greekgeek4',\n",
       " 'false pawneescrantonmerger',\n",
       " 'family',\n",
       " 'family room',\n",
       " 'family watches',\n",
       " 'fat',\n",
       " 'fat dinks',\n",
       " 'favorite',\n",
       " 'favorite binge',\n",
       " 'favorite gifs',\n",
       " 'favorite https',\n",
       " 'favorite moments',\n",
       " 'favorite quotes',\n",
       " 'features',\n",
       " 'features fun',\n",
       " 'features thinking',\n",
       " 'feel',\n",
       " 'feel like',\n",
       " 'feel warm',\n",
       " 'final',\n",
       " 'final michael',\n",
       " 'final season',\n",
       " 'finer',\n",
       " 'finer things',\n",
       " 'finished',\n",
       " 'finished binge',\n",
       " 'finished idea',\n",
       " 'flight',\n",
       " 'flight deleted',\n",
       " 'font',\n",
       " 'font does',\n",
       " 'font used',\n",
       " 'free',\n",
       " 'free hulu',\n",
       " 'friend',\n",
       " 'friend uses',\n",
       " 'fun',\n",
       " 'fun things',\n",
       " 'funny',\n",
       " 'funny loved',\n",
       " 'funny ones',\n",
       " 'funny scenes',\n",
       " 'fuzzy',\n",
       " 'fuzzy classicalmusictroll',\n",
       " 'gallery',\n",
       " 'gallery t3eulxv',\n",
       " 'gets',\n",
       " 'gets good',\n",
       " 'gif',\n",
       " 'gif ben',\n",
       " 'gif request',\n",
       " 'gif scene',\n",
       " 'gifs',\n",
       " 'gifs check',\n",
       " 'gifs constantly',\n",
       " 'gifs funny',\n",
       " 'girl',\n",
       " 'girl love',\n",
       " 'girlfriend',\n",
       " 'girlfriend likes',\n",
       " 'girlfriend wanted',\n",
       " 'given',\n",
       " 'given chance',\n",
       " 'given past',\n",
       " 'goddamnit',\n",
       " 'goddamnit love',\n",
       " 'goes',\n",
       " 'goes theflow3000',\n",
       " 'going',\n",
       " 'going antsy',\n",
       " 'going okay',\n",
       " 'going post',\n",
       " 'going type',\n",
       " 'gonwin',\n",
       " 'good',\n",
       " 'good 2nd',\n",
       " 'good latest',\n",
       " 'good place',\n",
       " 'good shows',\n",
       " 'goodies',\n",
       " 'goodies count',\n",
       " 'got',\n",
       " 'got bot',\n",
       " 'got close',\n",
       " 'got little',\n",
       " 'great',\n",
       " 'great actress',\n",
       " 'great chance',\n",
       " 'great character',\n",
       " 'great guys',\n",
       " 'great thanks',\n",
       " 'greekgeek4',\n",
       " 'grizzle',\n",
       " 'grizzle disguises',\n",
       " 'guys',\n",
       " 'guys know',\n",
       " 'guys st',\n",
       " 'guys think',\n",
       " 'guys threads',\n",
       " 'happens',\n",
       " 'happens watching',\n",
       " 'happy',\n",
       " 'happy link',\n",
       " 'hard',\n",
       " 'hard good',\n",
       " 'haven',\n",
       " 'haven laughed',\n",
       " 'having',\n",
       " 'having awful',\n",
       " 'having luck',\n",
       " 'having season',\n",
       " 'hear',\n",
       " 'hear really',\n",
       " 'help',\n",
       " 'help suggestions',\n",
       " 'hey',\n",
       " 'hey fat',\n",
       " 'hey im',\n",
       " 'heyo',\n",
       " 'heyo inspired',\n",
       " 'hibbert',\n",
       " 'hibbert absolutely',\n",
       " 'hilarious',\n",
       " 'hilarious great',\n",
       " 'honest',\n",
       " 'honest impressed',\n",
       " 'household',\n",
       " 'household hubby',\n",
       " 'http',\n",
       " 'http en',\n",
       " 'http insidetv',\n",
       " 'http twitter',\n",
       " 'https',\n",
       " 'https imgur',\n",
       " 'https www',\n",
       " 'hubby',\n",
       " 'hubby suggested',\n",
       " 'hulu',\n",
       " 'hulu ll',\n",
       " 'hulu requires',\n",
       " 'hulu unfortunately',\n",
       " 'humor',\n",
       " 'humor stitches',\n",
       " 'husband',\n",
       " 'husband months',\n",
       " 'idea',\n",
       " 'idea think',\n",
       " 'ideas',\n",
       " 'ideas season',\n",
       " 'im',\n",
       " 'im going',\n",
       " 'im new',\n",
       " 'imgur',\n",
       " 'imgur com',\n",
       " 'imo',\n",
       " 'imo threemileallan',\n",
       " 'impressed',\n",
       " 'impressed hear',\n",
       " 'insidetv',\n",
       " 'insidetv ew',\n",
       " 'inspired',\n",
       " 'inspired twitter',\n",
       " 'instantly',\n",
       " 'instantly said',\n",
       " 'involved',\n",
       " 'involved love',\n",
       " 'ireland',\n",
       " 'ireland new',\n",
       " 'irl',\n",
       " 'irl false',\n",
       " 'isn',\n",
       " 'isn option',\n",
       " 'janet',\n",
       " 'janet snakehole',\n",
       " 'january',\n",
       " 'january 2015',\n",
       " 'jason',\n",
       " 'jason friend',\n",
       " 'jinmeister',\n",
       " 'jtayy12',\n",
       " 'just',\n",
       " 'just clever',\n",
       " 'just finished',\n",
       " 'just funny',\n",
       " 'just lost',\n",
       " 'just love',\n",
       " 'just skip',\n",
       " 'just started',\n",
       " 'just time',\n",
       " 'know',\n",
       " 'know anybody',\n",
       " 'know font',\n",
       " 'know going',\n",
       " 'know list',\n",
       " 'know ll',\n",
       " 'know rectangle',\n",
       " 'know season',\n",
       " 'know watch',\n",
       " 'know won',\n",
       " 'knowing',\n",
       " 'knowing origin',\n",
       " 'knstone',\n",
       " 'latest',\n",
       " 'latest episode',\n",
       " 'laughed',\n",
       " 'laughed entire',\n",
       " 'laughed hard',\n",
       " 'layout',\n",
       " 'layout features',\n",
       " 'leslie',\n",
       " 'leslie grizzle',\n",
       " 'li',\n",
       " 'li seabastian',\n",
       " 'life',\n",
       " 'life replaced',\n",
       " 'life tahani',\n",
       " 'lift',\n",
       " 'lift anti',\n",
       " 'like',\n",
       " 'like going',\n",
       " 'like hulu',\n",
       " 'like member',\n",
       " 'like office',\n",
       " 'like people',\n",
       " 'liked',\n",
       " 'liked knowing',\n",
       " 'likes',\n",
       " 'likes new',\n",
       " 'line',\n",
       " 'line constantly',\n",
       " 'link',\n",
       " 'link favorite',\n",
       " 'link http',\n",
       " 'linking',\n",
       " 'linking past',\n",
       " 'list',\n",
       " 'list applicable',\n",
       " 'little',\n",
       " 'little face',\n",
       " 'little good',\n",
       " 'live',\n",
       " 'live does',\n",
       " 'lived',\n",
       " 'lived states',\n",
       " 'lizlemondonaghy',\n",
       " 'll',\n",
       " 'll make',\n",
       " 'll melanie_rblatt',\n",
       " 'll really',\n",
       " 'll talk',\n",
       " 'll tell',\n",
       " 'll timeline',\n",
       " 'll watch',\n",
       " 'location',\n",
       " 'location isn',\n",
       " 'lol',\n",
       " 'lol times',\n",
       " 'long',\n",
       " 'long prettyusual',\n",
       " 'long time',\n",
       " 'loss',\n",
       " 'loss just',\n",
       " 'lost',\n",
       " 'lost years',\n",
       " 'lot',\n",
       " 'lot did',\n",
       " 'lot place',\n",
       " 'lot time',\n",
       " 'love',\n",
       " 'love actors',\n",
       " 'love lizlemondonaghy',\n",
       " 'love makes',\n",
       " 'love nighttraindan',\n",
       " 'love post',\n",
       " 'love reaction',\n",
       " 'loved',\n",
       " 'loved humor',\n",
       " 'loved season',\n",
       " 'lovely',\n",
       " 'lovely people',\n",
       " 'loving',\n",
       " 'loving davect01',\n",
       " 'luck',\n",
       " 'luck lovely',\n",
       " 'luck lunatic_minge',\n",
       " 'lunatic_minge',\n",
       " 'macklin',\n",
       " 'macklin janet',\n",
       " 'make',\n",
       " 'make personas',\n",
       " 'make rough',\n",
       " 'make similar',\n",
       " 'make sure',\n",
       " 'makes',\n",
       " 'makes feel',\n",
       " 'maybe',\n",
       " 'maybe chanel',\n",
       " 'meet',\n",
       " 'meet bad',\n",
       " 'megaphone',\n",
       " 'megaphone monday',\n",
       " 'melanie_rblatt',\n",
       " 'member',\n",
       " 'member deleted',\n",
       " 'member finer',\n",
       " 'members',\n",
       " 'members cast',\n",
       " 'michael',\n",
       " 'michael schur',\n",
       " 'mid',\n",
       " 'mid season_replacement',\n",
       " 'mind',\n",
       " 'mind sweet',\n",
       " 'missed',\n",
       " 'missed airs',\n",
       " 'missing',\n",
       " 'missing watched',\n",
       " 'mod',\n",
       " 'mod pandr',\n",
       " 'moment',\n",
       " 'moment pre',\n",
       " 'moments',\n",
       " 'moments character',\n",
       " 'moments shows',\n",
       " 'monday',\n",
       " 'monday butthole',\n",
       " 'monotony',\n",
       " 'monotony thanks',\n",
       " 'months',\n",
       " 'months shop',\n",
       " 'movies',\n",
       " 'movies members',\n",
       " 'mumblerapisgarbage',\n",
       " 'nbc',\n",
       " 'nbc website',\n",
       " 'need',\n",
       " 'need gif',\n",
       " 'need signed',\n",
       " 'netflix',\n",
       " 'netflix australia',\n",
       " 'netflix com',\n",
       " 'netflix country',\n",
       " 'netflix entire',\n",
       " 'netflix false',\n",
       " 'netflix season',\n",
       " 'netflix seasons',\n",
       " 'netflix xfinity',\n",
       " 'new',\n",
       " 'new adventure',\n",
       " 'new discussion',\n",
       " 'new episode',\n",
       " 'new girl',\n",
       " 'new mod',\n",
       " 'new nickname',\n",
       " 'new point',\n",
       " 'new zealand',\n",
       " 'nickname',\n",
       " 'nickname girlfriend',\n",
       " 'nickname reason',\n",
       " 'night',\n",
       " 'night ll',\n",
       " 'nighttraindan',\n",
       " 'nighttraindan gifs',\n",
       " 'nighttraindan just',\n",
       " 'nighttraindan knstone',\n",
       " 'nope',\n",
       " 'nope season',\n",
       " 'occasionally',\n",
       " 'occasionally ll',\n",
       " 'occono',\n",
       " 'office',\n",
       " 'office new',\n",
       " 'okay',\n",
       " 'okay sad',\n",
       " 'okay weekly',\n",
       " 'ones',\n",
       " 'ones context',\n",
       " 'online',\n",
       " 'online sorry',\n",
       " 'online watch',\n",
       " 'option',\n",
       " 'option help',\n",
       " 'optional',\n",
       " 'optional husband',\n",
       " 'org',\n",
       " 'org wiki',\n",
       " 'origin',\n",
       " 'origin thought',\n",
       " 'outside',\n",
       " 'outside characters',\n",
       " 'pandr',\n",
       " 'pandr subreddit',\n",
       " 'parks',\n",
       " 'parks rec',\n",
       " 'parks recreation',\n",
       " 'parties',\n",
       " 'parties guys',\n",
       " 'past',\n",
       " 'past recaps',\n",
       " 'past talking',\n",
       " 'past years',\n",
       " 'pawneescrantonmerger',\n",
       " 'pawntakesrook',\n",
       " 'pay',\n",
       " 'pay episode',\n",
       " 'people',\n",
       " 'people alternative',\n",
       " 'people places',\n",
       " 'people want',\n",
       " 'personally',\n",
       " 'personally thank',\n",
       " 'personally willing',\n",
       " 'personas',\n",
       " 'personas dressed',\n",
       " 'phone',\n",
       " 'phone room',\n",
       " 'picky',\n",
       " 'picky layout',\n",
       " 'place',\n",
       " 'place online',\n",
       " 'place timelines',\n",
       " 'places',\n",
       " 'places area',\n",
       " 'places things',\n",
       " 'plan',\n",
       " 'plan watching',\n",
       " 'pnr',\n",
       " 'pnr household',\n",
       " 'point',\n",
       " 'point got',\n",
       " 'post',\n",
       " 'post breaks',\n",
       " 'post drunk',\n",
       " 'post dundermifflin',\n",
       " 'posting',\n",
       " 'posting thread',\n",
       " 'posts',\n",
       " 'posts screencaps',\n",
       " 'practically',\n",
       " 'practically threw',\n",
       " 'pre',\n",
       " 'pre successful',\n",
       " 'premiere',\n",
       " 'premiere air',\n",
       " 'premiere final',\n",
       " 'premiere sources',\n",
       " 'premieres',\n",
       " 'premieres just',\n",
       " 'premium',\n",
       " 'premium account',\n",
       " 'prettyusual',\n",
       " 'prime',\n",
       " 'prime luck',\n",
       " 'probably',\n",
       " 'probably countries',\n",
       " 'probably expect',\n",
       " 'probably january',\n",
       " 'promotion',\n",
       " 'promotion rule',\n",
       " 'quotes',\n",
       " 'quotes like',\n",
       " 'quotes say',\n",
       " 'reaction',\n",
       " 'reaction scene',\n",
       " 'real',\n",
       " 'real li',\n",
       " 'realizes',\n",
       " 'realizes roy',\n",
       " 'really',\n",
       " 'really funny',\n",
       " 'really gets',\n",
       " 'really new',\n",
       " 'really wish',\n",
       " 'reason',\n",
       " 'reason thought',\n",
       " 'rec',\n",
       " 'rec reference',\n",
       " 'recaps',\n",
       " 'recaps reviews',\n",
       " 'recreation',\n",
       " 'recreation season',\n",
       " 'rectangle',\n",
       " 'rectangle america',\n",
       " 'reddit',\n",
       " 'reddit com',\n",
       " 'refer',\n",
       " 'refer new',\n",
       " 'reference',\n",
       " 'reference don',\n",
       " 'reflecting',\n",
       " 'reflecting given',\n",
       " 'regular',\n",
       " 'regular screencaps',\n",
       " 'relevant',\n",
       " 'relevant episodes',\n",
       " 'reliving',\n",
       " 'reliving past',\n",
       " 'repeat',\n",
       " 'repeat everyday',\n",
       " 'replaced',\n",
       " 'replaced swear',\n",
       " 'request',\n",
       " 'request does',\n",
       " 'requires',\n",
       " 'requires premium',\n",
       " 'resorted',\n",
       " 'resorted taking',\n",
       " 'reviews',\n",
       " 'reviews extra',\n",
       " 'rewatch',\n",
       " 'rewatch discussion',\n",
       " 'rewatch wife',\n",
       " 'right',\n",
       " 'right nope',\n",
       " 'right think',\n",
       " 'room',\n",
       " 'room audixmusic',\n",
       " 'room mumblerapisgarbage',\n",
       " 'rough',\n",
       " 'rough schedule',\n",
       " 'roy',\n",
       " 'roy hibbert',\n",
       " 'rule',\n",
       " 'rule does',\n",
       " 'running',\n",
       " 'running night',\n",
       " 's1e5',\n",
       " 's1e5 practically',\n",
       " 's5e10',\n",
       " 's5e10 parties',\n",
       " 'sad',\n",
       " 'sad moments',\n",
       " 'safe',\n",
       " 'safe jason',\n",
       " 'said',\n",
       " 'said liked',\n",
       " 'save',\n",
       " 'save ton',\n",
       " 'say',\n",
       " 'say irl',\n",
       " 'say probably',\n",
       " 'scene',\n",
       " 'scene brumplestiltskins',\n",
       " 'scene having',\n",
       " 'scene imo',\n",
       " 'scene s5e10',\n",
       " 'scenes',\n",
       " 'scenes long',\n",
       " 'scenes special',\n",
       " 'schedule',\n",
       " 'schedule week',\n",
       " 'schur',\n",
       " 'schur know',\n",
       " 'scott',\n",
       " 'scott great',\n",
       " 'scott season',\n",
       " 'screencaps',\n",
       " 'screencaps people',\n",
       " 'screencaps spongebob',\n",
       " 'seabastian',\n",
       " 'seabastian alive',\n",
       " 'season',\n",
       " 'season airing',\n",
       " 'season approaches',\n",
       " 'season approaching',\n",
       " 'season come',\n",
       " 'season coming',\n",
       " 'season day',\n",
       " 'season deleted',\n",
       " 'season ending',\n",
       " 'season episode',\n",
       " 'season episodes',\n",
       " 'season final',\n",
       " 'season free',\n",
       " 'season just',\n",
       " 'season lived',\n",
       " 'season missing',\n",
       " 'season netflix',\n",
       " 'season online',\n",
       " 'season premiere',\n",
       " 'season premieres',\n",
       " 'season scott',\n",
       " 'season spankme9991',\n",
       " 'season want',\n",
       " 'season watch',\n",
       " 'season week',\n",
       " 'season_replacement',\n",
       " 'season_replacement season',\n",
       " 'seasons',\n",
       " 'seasons like',\n",
       " 'second',\n",
       " 'second loving',\n",
       " 'seen',\n",
       " 'seen gif',\n",
       " 'self',\n",
       " 'self im',\n",
       " 'self promotion',\n",
       " 'shifting',\n",
       " 'shifting s1e5',\n",
       " 'shop',\n",
       " 'shop apartment',\n",
       " 'short',\n",
       " 'short break',\n",
       " 'shows',\n",
       " 'shows good',\n",
       " 'shows like',\n",
       " 'shows movies',\n",
       " 'shrampies',\n",
       " 'shrampies youarenotbook',\n",
       " 'signed',\n",
       " 'signed occono',\n",
       " 'similar',\n",
       " 'similar bot',\n",
       " 'sinkhole',\n",
       " 'sinkhole face',\n",
       " 'sites',\n",
       " 'sites maybe',\n",
       " 'skip',\n",
       " 'skip season',\n",
       " 'smile',\n",
       " 'smile face',\n",
       " 'snakehole',\n",
       " 'snakehole used',\n",
       " 'solidarity',\n",
       " 'solidarity reflecting',\n",
       " 'somename6',\n",
       " 'soooo',\n",
       " 'soooo did',\n",
       " 'sorry',\n",
       " 'sorry guys',\n",
       " 'sort',\n",
       " 'sort self',\n",
       " 'sources',\n",
       " 'sources say',\n",
       " 'spankme9991',\n",
       " 'special',\n",
       " 'special features',\n",
       " 'specifics',\n",
       " 'specifics decide',\n",
       " 'spoiler',\n",
       " 'spoiler make',\n",
       " 'spoiler support',\n",
       " 'spoiler type',\n",
       " 'spoilers',\n",
       " 'spoilers use',\n",
       " 'spongebob',\n",
       " 'spongebob make',\n",
       " 'spotted',\n",
       " 'spotted safe',\n",
       " 'st',\n",
       " 'st elmos',\n",
       " 'start',\n",
       " 'start 2nd',\n",
       " 'started',\n",
       " 'started rewatch',\n",
       " 'started shifting',\n",
       " 'started watching',\n",
       " 'started week',\n",
       " 'starting',\n",
       " 'starting season',\n",
       " 'starts',\n",
       " 'starts season',\n",
       " 'states',\n",
       " 'states england',\n",
       " 'steals',\n",
       " 'steals scene',\n",
       " 'stickied',\n",
       " 'stickied ll',\n",
       " 'stitches',\n",
       " 'stitches entire',\n",
       " 'subreddit',\n",
       " 'subreddit got',\n",
       " 'subreddit solidarity',\n",
       " 'successful',\n",
       " 'successful heyo',\n",
       " 'suggested',\n",
       " 'suggested make',\n",
       " 'suggestions',\n",
       " 'suggestions great',\n",
       " 'sunshine',\n",
       " 'support',\n",
       " 'support happens',\n",
       " 'support spoilers',\n",
       " 'sure',\n",
       " 'sure delete',\n",
       " 'sure lol',\n",
       " 'sure post',\n",
       " 'sure use',\n",
       " 'surely',\n",
       " ...]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>05</th>\n",
       "      <th>05 21</th>\n",
       "      <th>10</th>\n",
       "      <th>10 5th</th>\n",
       "      <th>112</th>\n",
       "      <th>112 episodes</th>\n",
       "      <th>1st</th>\n",
       "      <th>1st honest</th>\n",
       "      <th>2014</th>\n",
       "      <th>2014 05</th>\n",
       "      <th>...</th>\n",
       "      <th>years</th>\n",
       "      <th>years life</th>\n",
       "      <th>years posting</th>\n",
       "      <th>yet_another_theory</th>\n",
       "      <th>yet_another_theory gonwin</th>\n",
       "      <th>yo</th>\n",
       "      <th>yo self</th>\n",
       "      <th>youarenotbook</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zealand uk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   05  05 21  10  10 5th  112  112 episodes  1st  1st honest  2014  2014 05  \\\n",
       "0   0      0   0       0    0             0    0           0     0        0   \n",
       "1   0      0   0       0    0             0    0           0     0        0   \n",
       "2   0      0   0       0    0             0    0           0     0        0   \n",
       "3   0      0   0       0    0             0    0           0     0        0   \n",
       "4   0      0   0       0    0             0    0           0     0        0   \n",
       "\n",
       "   ...  years  years life  years posting  yet_another_theory  \\\n",
       "0  ...      0           0              0                   0   \n",
       "1  ...      0           0              0                   0   \n",
       "2  ...      0           0              0                   0   \n",
       "3  ...      0           0              0                   0   \n",
       "4  ...      0           0              0                   0   \n",
       "\n",
       "   yet_another_theory gonwin  yo  yo self  youarenotbook  zealand  zealand uk  \n",
       "0                          0   0        0              0        0           0  \n",
       "1                          0   1        1              0        0           0  \n",
       "2                          0   0        0              0        0           0  \n",
       "3                          0   0        0              0        0           0  \n",
       "4                          0   0        0              0        0           0  \n",
       "\n",
       "[5 rows x 1217 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_df = pd.DataFrame(fin_count.toarray(),\n",
    "                      columns = fcvec.get_feature_names())\n",
    "\n",
    "wrong_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_words = wrong_df.sum().sort_values(ascending = False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_mis = pd.DataFrame(wrong_words.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_mis.columns = ['Word Usage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>netflix</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deleted</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watch</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ll</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watching</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favorite</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guys</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>com</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>does</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episodes</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gif</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>season episode</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entire</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word Usage\n",
       "season                  30\n",
       "episode                 10\n",
       "know                    10\n",
       "just                     9\n",
       "new                      9\n",
       "netflix                  8\n",
       "think                    8\n",
       "deleted                  8\n",
       "week                     7\n",
       "watch                    7\n",
       "ll                       7\n",
       "time                     7\n",
       "watching                 6\n",
       "favorite                 6\n",
       "love                     6\n",
       "guys                     6\n",
       "great                    5\n",
       "com                      5\n",
       "like                     5\n",
       "does                     5\n",
       "episodes                 5\n",
       "gif                      4\n",
       "want                     4\n",
       "season episode           4\n",
       "entire                   4"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_mis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Season, episode, and know were the least predictive terms in the model, with season and episode appearing in their plural forms as well.  This is no surprise, given that fans of both shows would likely refer to a specific season or episode in a post without specifically referencing the show itself.  The reference is assumed because it is already being posted on the specific Subreddit, so there's no need to talk about Season 1 of Parks & Rec, when you can just say Season 1 and everyone already knows what show you're referring to.  Although, there is no need to talk about the #brendanaquits seasons of Parks & Rec anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['season', 'episode', 'know', 'just', 'new', 'netflix', 'think',\n",
       "       'deleted', 'week', 'watch', 'll', 'time', 'watching', 'favorite',\n",
       "       'love', 'guys', 'great', 'com', 'like', 'does', 'episodes', 'gif',\n",
       "       'want', 'season episode', 'entire', 'don', 'make', 'going', 'account',\n",
       "       'final', 'sure', 'good', 'really', 'scene', 'started', 'http', 'having',\n",
       "       'amp', 'watched', 'rewatch', 'post', 'funny', 'premiere', 'guys think',\n",
       "       'probably', 'shows', 'people', 'discussion', 'use', 'hulu', 'past',\n",
       "       'things', 'twitter', 'nighttraindan', 'title', 'got', 'cast', 'thought',\n",
       "       'spoiler', 'gifs', 'lot', 'face', 'final season', 'alive', 'online',\n",
       "       'work', 'parks', 'did', 'card', 'thinking', 'feel', 'im', 'thread',\n",
       "       'luck', 'little', 'bot', 'loved', 'does know', 'binge', 'don know',\n",
       "       'title card', 'link', 'given', 'girlfriend', 'makes', 'twitter account',\n",
       "       'nickname', 'chance', 'features', 'new nickname', 'years', 'support',\n",
       "       'scenes', 'constantly', 'family', 'hey', 'subreddit', 'places', 'day',\n",
       "       'place'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_words.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Support Vector Machine model scored between 90-95% on accuracy, so we were able to achieve our modeling goal from a statistical standpoint, however writing comedies is hard.  This model can't necessarily guarantee a successful pilot.  A lot of the overlap happens on words that still aren't overly descriptive towards themes, but let's give this a shot.\n",
    "\n",
    "From my first test\n",
    "* Interestingly the model seems to misfire on scouts, so I think my show would have to do with campers or talent scouts.  Song is another word the model misfires on.  The words hilarious, actress, charity, and podcast also appear.  Yes, I see it now- the lead is a hilarious struggling actress named Charity, who gets casted on a singing show by Don, the talent scout for the show, who fell in love with our leading actress at first sight, and offered her a spot on his show without hearing her sing, so he can be around her.  Will Don's \"charity offer\" to cast a struggling actress, also named Charity, professionally and romanticly pay off?  If she can't sing, Don's career is over.  If Charity discovers Don's motivations, will she get disgusted and leave the show altogether?  Last thing, Don is British (UK and England are also stopwords).\n",
    "\n",
    "From my second test\n",
    "* The main character is still an Englishman named Don, who has a Twitter account and podcast called \"Guys Think.\"  He is a leading voice for the trying too hard to be overly masculine (basically a British Joe Rogan).  His family tells him it's time for him to settle down and find a girlfriend, and as he tries, he struggles with his own masculinity when finding out that attractive women don't take kindly to his \"manliness.\"  He interviews a sensitive guy named Scott, a seemingly plain guy with a beautiful, intelligent, and famous wife (think Melissa Mayer, who can probably also be played by Kristen Bell).  While he goes onto the attack during the interview, Don finds that Scott may be the type of guy he needs to keep around instead of his \"macho\" friends.  Scott is wary, but is too good of a guy to not help (basically Chidi from the Good Place).\n",
    "\n",
    "These do seem like fun ideas to develop.  If we're truly judging our goal in a non-metric sense, this did generate an idea that my comedic ineptitude wouldn't have discovered on its own.  Now I just need to develop the characters and pitch it to Mike Schur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I were to implement this further and look for even more ideas, I would probably run a decision tree model and possibly try a bootstrapping and bagging method (which I wasn't able to due to time constraints).  Additionally, I might look deeper into which rows misclassified and adjust my stopwords to improve accuracy.  I could even look to scrape more subreddits and attempt to classify new Subreddits in addition to The Good Place and Parks & Rec.  If I do that, I would likely have to rely more on boosting, bagging, and bootstrapping since I would be running classification on more than one target variable.\n",
    "\n",
    "All of this would likely result in a more accurate model, but once again, I don't want to be too accurate because then I wouldn't be left with many words to generate ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Parks & Rec Subreddit](https://www.reddit.com/r/PandR/)\n",
    "- [The Good Place Subreddit](https://www.reddit.com/r/TheGoodPlace/)\n",
    "- [Data Dictionary for Original Data Categories](https://pushshift.io/api-parameters/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
