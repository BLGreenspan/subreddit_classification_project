{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Data Science to Write Comedy\n",
    "Note- If code is rerun, findings may be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By: Brandon Greenspan: [LinkedIn](https://https://www.linkedin.com/in/brandonlgreenspan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents:\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "- [Executive Summary](#Executive-Summary)\n",
    "- [Data Collection](#Data-Collection)\n",
    "- [Cleaning & EDA](##Cleaning-&-EDA)\n",
    "- [Model Preparation](#Model-Preparation)\n",
    "    - [Baseline Model](#Model-0:-Baseline)\n",
    "    - [Train Test Split](#Train-Test-Split)\n",
    "- [Modeling](#Modeling)\n",
    "    - [Logistic Regression](#Model-1:-Logistic-Regression)\n",
    "    - [Multinomial Naive Bayes](#Model-2:-Multinomial-Naive-Bayes)\n",
    "    - [Gaussian Naive Bayes](#Model-3:-Gaussian-Naive-Bayes)\n",
    "    - [KNN](#Model-4:-KNN)\n",
    "    - [Support Vector Machine Model](Model-5:-SVM-Model)\n",
    "- [Model Selection](#Model-Selection)\n",
    "- [Model Evaluation](#Model-Evaluation)\n",
    "- [Conclusion](#Conclusion)\n",
    "    - [Recommendations](#Recommendations)\n",
    "    - [Future Steps](#Future-Steps)\n",
    "- [References](#References)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I am a comedy writer (True Story).  I've run out of my own original ideas, so in order to get a good idea to pitch to Mike Schur, I want to find common word choice/themes that fans of both The Good Place and Parks & Recreation are using.  In order to accomplish this, I will need to build a classification model.  While the success metric I'm aiming for in the model selection is accuracy, ultimately, my goal is to find the words that the model misclassifies in order to best identify the overlap of both shows that might indicate what elements a successful Mike Schur show would contain.  Ideally, I want the __accuracy rate of a selected model to be between 90% & 95%__, which means that my model should be a better predictor than the baseline model, but not too accurate because I actually need enough misclassification to come up with show ideas.  If the model theoretically did not misclassify any variables, then I would have an blank page, which is not an improvement upon how many comedic ideas I can conjure.\n",
    "\n",
    "Before I get to meet Michael Schur, I have to present my process for showrunners at NBC, which is not a technical audience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I comedy writer, I'm only as good as my last script.  Unfortunately, I haven't written any good scripts, and I have accepted the fact that if I can't succeed with comedic talent, I can succeed with data.  Michael Schur had a lot of success with sitcoms like The Office, Parks & Rec, Brooklyn 99, The Good Place, and many more.  I have the ambitious goal to pitch a show idea to Michael Schur, while generating my show idea from data.\n",
    "\n",
    "In order to generate a show that Michael Schur would want to develop, I will scrape the Subreddits for Parks & Rec and The Good Place to get an idea of what truly resonated with fans.  Data cleaning is fairly minimal since we are only working with Booleans and Strings, so we won't need to impute any string data.  To get a better understanding of our data, we will see how long our posts are as well as the most common words used for each Subreddit.  Out of these common words, we will create a custom stoplist for modeling.\n",
    "\n",
    "After calculating our baseline model, we will run a Train, Test, Split for the purposes of training and testing any future models, and evaluating the accuracy scores of each model.  The classification models we build are Logistic Regression, Multinomial Naive Bayes, Gaussian Naive Bayes, KNN, & Support Vector Machine.  Since we will be using a gridsearch method for all of these models in order to finetune our hyperparameters, this does take some time to run.\n",
    "\n",
    "Once we select a model, we will investigate our confusion matrix and assess the model's accuracy, sensitivity, specificity, precision, and true negative rates to get a sense as to the strength of the model beyond the initial accuracy score that led to its selection.  Then, we'll evaluate the strongest coefficients that the model is built on to see which words/phrases are the strongest predictors of a certain show.  Lastly, we'll dig into the words and phrases that most often lead to misclassification.\n",
    "\n",
    "After looking at our misclassified words, we'll finally try to generate a sitcom idea.  Will the idea be enough to impress Mike Schur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as stopwords\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.base import TransformerMixin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import regex as re\n",
    "\n",
    "ignore: Warning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collection will be done through the Pushshift API in order to scrape the subreddits for Parks & Rec and The Good Place.  The function will help automate the process in order for me to create two separate datafames- one for each show and then concatenate them into one database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mahdi helped us build this code through his intro lesson\n",
    "def query_pushshift(subreddit, kind = 'submission', day_window = 365, n = 14):\n",
    "    SUBFIELDS = ['title', 'selftext', 'subreddit', 'created_utc',\n",
    "                 'author', 'num_comments', 'score', 'is_self', 'is_original_content',\n",
    "                'over_18','is_crosspostable']\n",
    "    \n",
    "    # establish base url and stem\n",
    "    BASE_URL = f\"https://api.pushshift.io/reddit/search/{kind}\" # also known as the \"API endpoint\" \n",
    "    stem = f\"{BASE_URL}?subreddit={subreddit}&size=500\" # always pulling max of 500\n",
    "    \n",
    "    # instantiate empty list for temp storage\n",
    "    posts = []\n",
    "    \n",
    "    # implement for loop with `time.sleep(2)`\n",
    "    for i in range(1, n + 1):\n",
    "        URL = \"{}&after={}d\".format(stem, day_window * i)\n",
    "        print(\"Querying from: \" + URL)\n",
    "        response = requests.get(URL)\n",
    "        assert response.status_code == 200\n",
    "        mine = response.json()['data']\n",
    "        df = pd.DataFrame.from_dict(mine)\n",
    "        posts.append(df)\n",
    "        time.sleep(2) # required time as listed by the robots page\n",
    "    \n",
    "    # pd.concat storage list\n",
    "    full = pd.concat(posts, sort=False)\n",
    "    \n",
    "    # if submission\n",
    "    if kind == \"submission\":\n",
    "        # select desired columns\n",
    "        full = full[SUBFIELDS]\n",
    "        # drop duplicates\n",
    "        full.drop_duplicates(inplace = True)\n",
    "        # select `is_self` == True\n",
    "        full = full.loc[full['is_self'] == True]\n",
    "\n",
    "    # create `timestamp` column\n",
    "    full['timestamp'] = full[\"created_utc\"].map(dt.date.fromtimestamp)\n",
    "    \n",
    "    print(\"Query Complete!\")    \n",
    "    return full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=TheGoodPlace&size=500&after=365d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=TheGoodPlace&size=500&after=730d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=TheGoodPlace&size=500&after=1095d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=TheGoodPlace&size=500&after=1460d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=TheGoodPlace&size=500&after=1825d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=TheGoodPlace&size=500&after=2190d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=TheGoodPlace&size=500&after=2555d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=TheGoodPlace&size=500&after=2920d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=TheGoodPlace&size=500&after=3285d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=TheGoodPlace&size=500&after=3650d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=TheGoodPlace&size=500&after=4015d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=TheGoodPlace&size=500&after=4380d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=TheGoodPlace&size=500&after=4745d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=TheGoodPlace&size=500&after=5110d\n",
      "Query Complete!\n"
     ]
    }
   ],
   "source": [
    "good_place_df = query_pushshift('TheGoodPlace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1019, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_place_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=PandR&size=500&after=365d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=PandR&size=500&after=730d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=PandR&size=500&after=1095d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=PandR&size=500&after=1460d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=PandR&size=500&after=1825d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=PandR&size=500&after=2190d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=PandR&size=500&after=2555d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=PandR&size=500&after=2920d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=PandR&size=500&after=3285d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=PandR&size=500&after=3650d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=PandR&size=500&after=4015d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=PandR&size=500&after=4380d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=PandR&size=500&after=4745d\n",
      "Querying from: https://api.pushshift.io/reddit/search/submission?subreddit=PandR&size=500&after=5110d\n",
      "Query Complete!\n"
     ]
    }
   ],
   "source": [
    "parks_and_rec_df = query_pushshift('PandR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1081, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parks_and_rec_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying different values for day_window, n, and size, I wanted to ensure that I had enough data (over 500 rows for each show) and that my classes were relatively balanced.  There weren't nearly as many submissions as I expected for each page, so I decided to scrape 14 years worth of data (the age of Reddit).  This is also interesting to note because the shows did not overlap in timeframe, and considering humor changes with time, it will be interesting to see what words/phrases misclassify due to changes in taste over time.  My classes look balanced (1081 v. 1019, with Parks & Rec being my majority class), so I'm ready to proceed to cleaning.  I have also exported the data into csvs in case my second test run would like to be duplicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([parks_and_rec_df, good_place_df], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenated both subreddits into one database.  Checking my head and tail to ensure it was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>over_18</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Small detain I noticed (not sure if anyone did...</td>\n",
       "      <td>On like my 3rd watch through. Will Arnett must...</td>\n",
       "      <td>PandR</td>\n",
       "      <td>1556457656</td>\n",
       "      <td>michaelskarn007</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-04-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many Lowe’s could Rob Lowe rob, if Rob Low...</td>\n",
       "      <td></td>\n",
       "      <td>PandR</td>\n",
       "      <td>1556466443</td>\n",
       "      <td>spiritofgonzo1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-04-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andy is peter quill</td>\n",
       "      <td>Wouldn’t it be funny if at the end of end game...</td>\n",
       "      <td>PandR</td>\n",
       "      <td>1556481813</td>\n",
       "      <td>Pawandynee</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-04-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Small detain I noticed (not sure if anyone did...   \n",
       "1  How many Lowe’s could Rob Lowe rob, if Rob Low...   \n",
       "2                                Andy is peter quill   \n",
       "\n",
       "                                            selftext subreddit  created_utc  \\\n",
       "0  On like my 3rd watch through. Will Arnett must...     PandR   1556457656   \n",
       "1                                                        PandR   1556466443   \n",
       "2  Wouldn’t it be funny if at the end of end game...     PandR   1556481813   \n",
       "\n",
       "            author  num_comments  score  is_self is_original_content  over_18  \\\n",
       "0  michaelskarn007             3      0     True               False    False   \n",
       "1   spiritofgonzo1             0     27     True               False    False   \n",
       "2       Pawandynee             2      0     True               False    False   \n",
       "\n",
       "  is_crosspostable   timestamp  \n",
       "0             True  2019-04-28  \n",
       "1             True  2019-04-28  \n",
       "2             True  2019-04-28  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_original_content</th>\n",
       "      <th>over_18</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>[SPOILER] Janet Theory</td>\n",
       "      <td>Please do not read this if you haven't watched...</td>\n",
       "      <td>TheGoodPlace</td>\n",
       "      <td>1491858194</td>\n",
       "      <td>BrianFoxShow</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>Is Janet Allegorical? [spoiler]</td>\n",
       "      <td>Since my Janet is god theory didn't resonate -...</td>\n",
       "      <td>TheGoodPlace</td>\n",
       "      <td>1491921533</td>\n",
       "      <td>kdubstep</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>Having just finished the (amazing) first seaso...</td>\n",
       "      <td>That pizza is DEFINITELY gluten-free.</td>\n",
       "      <td>TheGoodPlace</td>\n",
       "      <td>1492246895</td>\n",
       "      <td>weblowinherseys</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-04-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "2097                             [SPOILER] Janet Theory   \n",
       "2098                    Is Janet Allegorical? [spoiler]   \n",
       "2099  Having just finished the (amazing) first seaso...   \n",
       "\n",
       "                                               selftext     subreddit  \\\n",
       "2097  Please do not read this if you haven't watched...  TheGoodPlace   \n",
       "2098  Since my Janet is god theory didn't resonate -...  TheGoodPlace   \n",
       "2099              That pizza is DEFINITELY gluten-free.  TheGoodPlace   \n",
       "\n",
       "      created_utc           author  num_comments  score  is_self  \\\n",
       "2097   1491858194     BrianFoxShow            10     42     True   \n",
       "2098   1491921533         kdubstep             5      3     True   \n",
       "2099   1492246895  weblowinherseys             4     40     True   \n",
       "\n",
       "     is_original_content  over_18 is_crosspostable   timestamp  \n",
       "2097                 NaN    False              NaN  2017-04-10  \n",
       "2098                 NaN    False              NaN  2017-04-11  \n",
       "2099                 NaN    False              NaN  2017-04-15  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checked head and tail to ensure the data properly went to one dataframe, which it did.  Now checking for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                     0\n",
       "selftext                  0\n",
       "subreddit                 0\n",
       "created_utc               0\n",
       "author                    0\n",
       "num_comments              0\n",
       "score                     0\n",
       "is_self                   0\n",
       "is_original_content    1490\n",
       "over_18                   0\n",
       "is_crosspostable       1155\n",
       "timestamp                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                  object\n",
       "selftext               object\n",
       "subreddit              object\n",
       "created_utc             int64\n",
       "author                 object\n",
       "num_comments            int64\n",
       "score                   int64\n",
       "is_self                  bool\n",
       "is_original_content    object\n",
       "over_18                  bool\n",
       "is_crosspostable       object\n",
       "timestamp              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of nulls in original content, and crosspostable.  These are Boolean categories that can be later be turned into binomials, so I'm going to dig a little deeper into the values into that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    610\n",
       "Name: is_original_content, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_original_content'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to keep original content column, since there are no True values.  If every row would hold the same value, then it is unimportant to help differentiate for our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['is_original_content'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     896\n",
       "False     49\n",
       "Name: is_crosspostable, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_crosspostable'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to impute my null values as False.  Which will later be turned into a binomial.  I can assume a null value is equivalent to \"not true,\" so I find this imputation to be justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('False', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['over_18'] = df['over_18'].astype(str).replace({\n",
    "    'False' : '0',\n",
    "    'True' : '1'\n",
    "}).astype(int) \n",
    "'''type is Boolean, while I could have used mapping, I decided to convert booleans to strings, \n",
    "assign my values, then change the type to integer'''\n",
    "df['is_self'] = df['is_self'].astype(str).replace({\n",
    "    'False' : '0',\n",
    "    'True' : '1'\n",
    "}).astype(int)\n",
    "df['is_crosspostable'] = df['is_crosspostable'].astype(str).replace({\n",
    "    'False' : '0',\n",
    "    'True' : '1'\n",
    "}).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All null values have be handled and boolean categories have been converted to binomial values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_self</th>\n",
       "      <th>over_18</th>\n",
       "      <th>is_crosspostable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.100000e+03</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>2100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.475290e+09</td>\n",
       "      <td>10.367143</td>\n",
       "      <td>25.894762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.426667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.880685e+07</td>\n",
       "      <td>34.101018</td>\n",
       "      <td>77.697823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.084233</td>\n",
       "      <td>0.494711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.295516e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.434767e+09</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.495146e+09</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.525667e+09</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.561222e+09</td>\n",
       "      <td>887.000000</td>\n",
       "      <td>1310.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        created_utc  num_comments        score  is_self      over_18  \\\n",
       "count  2.100000e+03   2100.000000  2100.000000   2100.0  2100.000000   \n",
       "mean   1.475290e+09     10.367143    25.894762      1.0     0.007143   \n",
       "std    6.880685e+07     34.101018    77.697823      0.0     0.084233   \n",
       "min    1.295516e+09      0.000000     0.000000      1.0     0.000000   \n",
       "25%    1.434767e+09      2.000000     2.000000      1.0     0.000000   \n",
       "50%    1.495146e+09      5.000000     8.000000      1.0     0.000000   \n",
       "75%    1.525667e+09     11.000000    20.000000      1.0     0.000000   \n",
       "max    1.561222e+09    887.000000  1310.000000      1.0     1.000000   \n",
       "\n",
       "       is_crosspostable  \n",
       "count       2100.000000  \n",
       "mean           0.426667  \n",
       "std            0.494711  \n",
       "min            0.000000  \n",
       "25%            0.000000  \n",
       "50%            0.000000  \n",
       "75%            1.000000  \n",
       "max            1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All values of is_self are 1, so I will drop that.  over_18 is a very imbalanced class, with only .7% having a true value, so I will drop that column too, especially considering that the shows themselves are not 18 and over shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['is_self', 'over_18'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PandR           1081\n",
       "TheGoodPlace    1019\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PandR (Parks & Rec) is my majority class and therefore my future baseline model.  Going to turn this into a binomial with PandR set to 1 and TheGoodPlace set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>is_crosspostable</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>PandR_subr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Small detain I noticed (not sure if anyone did...</td>\n",
       "      <td>On like my 3rd watch through. Will Arnett must...</td>\n",
       "      <td>1556457656</td>\n",
       "      <td>michaelskarn007</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many Lowe’s could Rob Lowe rob, if Rob Low...</td>\n",
       "      <td></td>\n",
       "      <td>1556466443</td>\n",
       "      <td>spiritofgonzo1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andy is peter quill</td>\n",
       "      <td>Wouldn’t it be funny if at the end of end game...</td>\n",
       "      <td>1556481813</td>\n",
       "      <td>Pawandynee</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Champion song?</td>\n",
       "      <td>I know I'm not imagining it, but I can't find ...</td>\n",
       "      <td>1556500237</td>\n",
       "      <td>awesometoenails</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sorry if this has been done here before but I ...</td>\n",
       "      <td>Um Leslie? I typed your symptoms into the thin...</td>\n",
       "      <td>1556650836</td>\n",
       "      <td>Cheerio419</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-04-30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Small detain I noticed (not sure if anyone did...   \n",
       "1  How many Lowe’s could Rob Lowe rob, if Rob Low...   \n",
       "2                                Andy is peter quill   \n",
       "3                                     Champion song?   \n",
       "4  Sorry if this has been done here before but I ...   \n",
       "\n",
       "                                            selftext  created_utc  \\\n",
       "0  On like my 3rd watch through. Will Arnett must...   1556457656   \n",
       "1                                                      1556466443   \n",
       "2  Wouldn’t it be funny if at the end of end game...   1556481813   \n",
       "3  I know I'm not imagining it, but I can't find ...   1556500237   \n",
       "4  Um Leslie? I typed your symptoms into the thin...   1556650836   \n",
       "\n",
       "            author  num_comments  score  is_crosspostable   timestamp  \\\n",
       "0  michaelskarn007             3      0                 1  2019-04-28   \n",
       "1   spiritofgonzo1             0     27                 1  2019-04-28   \n",
       "2       Pawandynee             2      0                 1  2019-04-28   \n",
       "3  awesometoenails             5      1                 1  2019-04-28   \n",
       "4       Cheerio419            18     14                 1  2019-04-30   \n",
       "\n",
       "   PandR_subr  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PandR_subr'] = df['subreddit'].astype(str).replace({\n",
    "    'TheGoodPlace' : '0',\n",
    "    'PandR' : '1'\n",
    "}).astype(int)\n",
    "df.drop(columns = ['subreddit']).head()\n",
    "# my binomial column is replacing the original subreddit column since that has now been turned into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['all_text'] = df['title'] + \" \" + df['selftext'] + \" \" + df['author']\n",
    "df.drop(columns = ['title', 'selftext', 'author'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merged all text into one column since I will be using CountVectorizer in EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_strings = df['all_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will run all of the text through CountVectorizer to create a matrix to get a better sense of word(s) count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(lowercase = True, # turn everything to lower case\n",
    "                       ngram_range = (1,2)) # for EDA, I will look at 1 word and 2 word phrases\n",
    "\n",
    "# data run through cvec must be transformed\n",
    "X_text = cvec.fit_transform(list_of_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 made</th>\n",
       "      <th>00 min</th>\n",
       "      <th>000</th>\n",
       "      <th>000 000</th>\n",
       "      <th>000 605</th>\n",
       "      <th>000 candles</th>\n",
       "      <th>000 days</th>\n",
       "      <th>000 for</th>\n",
       "      <th>000 how</th>\n",
       "      <th>...</th>\n",
       "      <th>zynerd</th>\n",
       "      <th>zzesty</th>\n",
       "      <th>宀宀분당건마</th>\n",
       "      <th>宀宀분당건마 양재오피</th>\n",
       "      <th>분당키스방</th>\n",
       "      <th>분당키스방 제이제이45다컴</th>\n",
       "      <th>양재오피</th>\n",
       "      <th>양재오피 분당키스방</th>\n",
       "      <th>제이제이45다컴</th>\n",
       "      <th>제이제이45다컴 removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 97265 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  00 made  00 min  000  000 000  000 605  000 candles  000 days  000 for  \\\n",
       "0   0        0       0    0        0        0            0         0        0   \n",
       "1   0        0       0    0        0        0            0         0        0   \n",
       "2   0        0       0    0        0        0            0         0        0   \n",
       "3   0        0       0    0        0        0            0         0        0   \n",
       "4   0        0       0    0        0        0            0         0        0   \n",
       "\n",
       "   000 how  ...  zynerd  zzesty  宀宀분당건마  宀宀분당건마 양재오피  분당키스방  분당키스방 제이제이45다컴  \\\n",
       "0        0  ...       0       0       0            0      0               0   \n",
       "1        0  ...       0       0       0            0      0               0   \n",
       "2        0  ...       0       0       0            0      0               0   \n",
       "3        0  ...       0       0       0            0      0               0   \n",
       "4        0  ...       0       0       0            0      0               0   \n",
       "\n",
       "   양재오피  양재오피 분당키스방  제이제이45다컴  제이제이45다컴 removed  \n",
       "0     0           0         0                 0  \n",
       "1     0           0         0                 0  \n",
       "2     0           0         0                 0  \n",
       "3     0           0         0                 0  \n",
       "4     0           0         0                 0  \n",
       "\n",
       "[5 rows x 97265 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_df = pd.DataFrame(X_text.toarray(),\n",
    "                      columns = cvec.get_feature_names())\n",
    "\n",
    "X_text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes the text I've run through CountVectorizer and turned it into a matrix, representing the word/2-word count by document.  I notice that the last few columns are not in English.  I'm going to remove the columns that aren't in English.  This is not to say that the comments aren't valid, but interpretability of any model would be hard to explain if I don't speak the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_df.drop(columns = ['宀宀분당건마', '宀宀분당건마 양재오피', '분당키스방', '분당키스방 제이제이45다컴', '양재오피', '양재오피 분당키스방', '제이제이45다컴', '제이제이45다컴 removed'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While my next steps will seem like a duplication of efforts, I first want to further visualize what words appear frequently in both Parks and Rec & The Good Place.  I then would like to find the overlap of top words used for both shows in order to create a custom stoplist for modeling purposes.  First, let's see how long the post length is for our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths_of_posts = [len(each) for each in df['all_text']]\n",
    "plt.hist(lengths_of_posts, bins = 50)\n",
    "plt.title('Histogram of Post Lengths')\n",
    "plt.xlabel('Post Length')\n",
    "plt.ylabel('Number of Posts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our posts are under 1000 words.  Since a post can't have fewer than 0 words, there is an obvious right skew.  We likely see our peak close to 0 because of posts that would contain of mostly images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_df['PandR_subr'] = df['PandR_subr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,let's create separate dataframes for Parks & Rec and The Good Place to evaluate the most common words in each show's Subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>2890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>1073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pr_word_count\n",
       "the           2890\n",
       "and           1972\n",
       "to            1633\n",
       "of            1217\n",
       "it            1073"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_words_df = pd.DataFrame(X_text_df.loc[X_text_df['PandR_subr'] != 0].sum().sort_values(ascending = False))\n",
    "\n",
    "pr_words_df.drop(index = ['PandR_subr'], inplace = True)\n",
    "\n",
    "pr_words_df.columns = ['pr_word_count']\n",
    "\n",
    "pr_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pr_words_df.head(20).plot(kind = 'barh', legend = None)\n",
    "plt.title('Most Common Words for Parks & Rec')\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Words');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common words mostly consist of words that would be filtered out by sklearn's list of English stop words.  The noticeable exception is leslie, the main character of the show, who appears in the top 20 most frequent words for the Parks & Rec subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gp_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>5774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>3036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>2441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>2064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>1865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gp_word_count\n",
       "the            5774\n",
       "to             3036\n",
       "and            2441\n",
       "of             2064\n",
       "that           1865"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#repeating same process for The Good Place\n",
    "gp_words_df = pd.DataFrame(X_text_df.loc[X_text_df['PandR_subr'] == 0].sum().sort_values(ascending = False))\n",
    "\n",
    "gp_words_df.columns = ['gp_word_count']\n",
    "\n",
    "gp_words_df.drop(index = ['PandR_subr'], inplace = True)\n",
    "\n",
    "gp_words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gp_words_df.head(20).plot(kind = 'barh', legend = None)\n",
    "plt.title('Most Common Words for The Good Place')\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Words');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Parks & Rec, The most common words for The Good Place's Subreddit would also qualify as \"stop words.\"  Also similarly, Michael (one of the main characters for The Good Place) is also listed as a common word, along with the show's name.  My next steps is to evaluate the word count overall, so let's join the dataframes and create another visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr_word_count</th>\n",
       "      <th>gp_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>2890</td>\n",
       "      <td>5774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1972</td>\n",
       "      <td>2441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1633</td>\n",
       "      <td>3036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1217</td>\n",
       "      <td>2064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>1073</td>\n",
       "      <td>1596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human to</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human timescale</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human thought</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human thing</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it said</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97257 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pr_word_count  gp_word_count\n",
       "the                       2890           5774\n",
       "and                       1972           2441\n",
       "to                        1633           3036\n",
       "of                        1217           2064\n",
       "it                        1073           1596\n",
       "...                        ...            ...\n",
       "human to                     0              1\n",
       "human timescale              0              1\n",
       "human thought                0              1\n",
       "human thing                  0              1\n",
       "it said                      0              1\n",
       "\n",
       "[97257 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_df = pr_words_df.join(gp_words_df)\n",
    "\n",
    "word_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pr_word_count</th>\n",
       "      <th>gp_word_count</th>\n",
       "      <th>total_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>2890</td>\n",
       "      <td>5774</td>\n",
       "      <td>8664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>1972</td>\n",
       "      <td>2441</td>\n",
       "      <td>4413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1633</td>\n",
       "      <td>3036</td>\n",
       "      <td>4669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1217</td>\n",
       "      <td>2064</td>\n",
       "      <td>3281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>1073</td>\n",
       "      <td>1596</td>\n",
       "      <td>2669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human to</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human timescale</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human thought</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human thing</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it said</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97257 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pr_word_count  gp_word_count  total_count\n",
       "the                       2890           5774         8664\n",
       "and                       1972           2441         4413\n",
       "to                        1633           3036         4669\n",
       "of                        1217           2064         3281\n",
       "it                        1073           1596         2669\n",
       "...                        ...            ...          ...\n",
       "human to                     0              1            1\n",
       "human timescale              0              1            1\n",
       "human thought                0              1            1\n",
       "human thing                  0              1            1\n",
       "it said                      0              1            1\n",
       "\n",
       "[97257 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_df['total_count'] = word_count_df['pr_word_count'] + word_count_df['gp_word_count']\n",
    "# adds a new column, adding up the word counts for each show\n",
    "word_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_count_df.sort_values(by = ['total_count'], ascending = False).head(20).plot(kind = 'bar')\n",
    "plt.title('Most Common Words Overall')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Word Frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the overall word count for our entire dataframe.  While these are unsuprisingly stop words, it's interesting to note that all of these top 20 terms appear with greater frequency in The Good Place.  When looking back over the common words by show, I do notice that the scale is different for each graph, which is more conveniently displayed via this graph above.  Since we've already found the common words, I want to be proactive and set up a stop words list for any future modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this',\n",
       "       'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'leslie',\n",
       "       'he', 'so', 'be', 'my', 'have', 'like', 'ron', 'they', 'parks', 'or',\n",
       "       'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as',\n",
       "       'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not',\n",
       "       'would', 'one', 'deleted', 'has', 'parks and', 'know', 'andy',\n",
       "       'the show', 'anyone', 'his', 'up', 'out', 'an', 'we', 'com', 'time',\n",
       "       'ben', 'think', 'april', 'where', 'rec', 'how', 'get', 'watch',\n",
       "       'really', 'and rec', 've', 'do', 'http', 'does', 'watching', 'amp',\n",
       "       'who', 'tom', 'any', 'him', 'first', 'some', 'love', 'character',\n",
       "       'find', 'chris', 'don', 'series', 'did', 'by', 'for the', 'pawnee',\n",
       "       'been', 'ann', 'after', 'office', 'now'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_words = X_text_df.loc[X_text_df['PandR_subr'] != 0].sum().sort_values(ascending = False).head(101).index\n",
    "# Generates a top 100 list for most common Parks & Rec words\n",
    "pr_words = pr_words.drop('PandR_subr') \n",
    "# Since it was adding up the count for the dummy column, I had to drop it, so the subreddit name doesn't factor into the model\n",
    "\n",
    "pr_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Office is a top term!  This is most likely referring to the other hit show, The Office, which Parks & Rec often gets compared to, which is likely happening frequently on Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['the', 'to', 'and', 'of', 'that', 'is', 'in', 'it', 'place', 'good',\n",
       "       'this', 'they', 'for', 'but', 'be', 'was', 'good place', 'on',\n",
       "       'michael', 'in the', 'he', 'with', 'bad', 'so', 'eleanor', 'have',\n",
       "       'she', 'the good', 'season', 'we', 'as', 'if', 'are', 'you', 'not',\n",
       "       'what', 'just', 'all', 'show', 'of the', 'her', 'about', 'like',\n",
       "       'there', 'or', 'would', 'bad place', 'chidi', 'people', 'can', 'at',\n",
       "       'how', 'think', 'from', 'out', 'the bad', 'my', 'one', 'his', 'to the',\n",
       "       'janet', 'know', 'when', 'them', 'will', 'episode', 'up', 'because',\n",
       "       'do', 'has', 'an', 'by', 'the show', 'get', 'me', 'their', 'who',\n",
       "       'jason', 'to be', 'tahani', 'other', 'really', 'time', 'why', 'only',\n",
       "       'first', 'more', 'been', 'could', 'even', 'being', 'no', 'were',\n",
       "       'spoilers', 'some', 'see', 'also', 'don', 'into', 'then'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_words = X_text_df.loc[X_text_df['PandR_subr'] == 0].sum().sort_values(ascending = False).head(100).index\n",
    "# Replicate the process for The Good Place\n",
    "gp_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'and',\n",
       " 'to',\n",
       " 'of',\n",
       " 'it',\n",
       " 'in',\n",
       " 'is',\n",
       " 'that',\n",
       " 'for',\n",
       " 'this',\n",
       " 'but',\n",
       " 'was',\n",
       " 'on',\n",
       " 'just',\n",
       " 'season',\n",
       " 'you',\n",
       " 'show',\n",
       " 'with',\n",
       " 'he',\n",
       " 'so',\n",
       " 'be',\n",
       " 'my',\n",
       " 'have',\n",
       " 'like',\n",
       " 'they',\n",
       " 'or',\n",
       " 'can',\n",
       " 'what',\n",
       " 'me',\n",
       " 'episode',\n",
       " 'of the',\n",
       " 'at',\n",
       " 'are',\n",
       " 'she',\n",
       " 'as',\n",
       " 'all',\n",
       " 'in the',\n",
       " 'about',\n",
       " 'there',\n",
       " 'from',\n",
       " 'if',\n",
       " 'her',\n",
       " 'when',\n",
       " 'not',\n",
       " 'would',\n",
       " 'one',\n",
       " 'has',\n",
       " 'know',\n",
       " 'the show',\n",
       " 'his',\n",
       " 'up',\n",
       " 'out',\n",
       " 'an',\n",
       " 'we',\n",
       " 'time',\n",
       " 'think',\n",
       " 'how',\n",
       " 'get',\n",
       " 'really',\n",
       " 'do',\n",
       " 'who',\n",
       " 'first',\n",
       " 'some',\n",
       " 'don',\n",
       " 'by',\n",
       " 'been']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = list(gp_words) + list(pr_words)\n",
    "# creates a master list of the top 100 terms for both shows\n",
    "stops = pd.DataFrame(all_words)\n",
    "\n",
    "custom_stopwords = stops.loc[stops.duplicated() == True]\n",
    "# finds the words that appear in the top 100 for both shows\n",
    "stopwords = list(custom_stopwords[0])\n",
    "# turns those words into a stop word list to utilize in model testing\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all of our models, we are trying to use X (text from subreddit submissions) in order to  to predict y (whether a submission belongs in Parks & Rec or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['all_text'] #predictive variable\n",
    "y = df['PandR_subr'] #target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0: Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5147619047619048"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = df['PandR_subr'].mean()\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model would predict the majority class (Parks & Rec) every single time.  Since 51% of our y values are listed as Parks & Rec, our model would only be accurate 51% of the time.  While I want some misclassification, misclassifying almost 50% of the data is too much for me to go through in order to generate ideas for a show.  It's almost the same effect as having a model that has 100% accuracy in that I'll just continue to have writers block.  We need to build a better classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state = 63,\n",
    "                                                    stratify = y # keeps the ratio the same for our train and test\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([ #instantiate transformers and model\n",
    "    ('vectorizer', CountVectorizer()), \n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "# set hyperparameters\n",
    "\n",
    "pipe_params = {'vectorizer__ngram_range'  : [(1,1),(1,2), (1,3)],\n",
    "              'vectorizer__stop_words': [None, 'english', stopwords], #including cusotm stopwords in addition to english and none\n",
    "              'tfidf__use_idf'  : [True, False], #toggle between use of TFIDF\n",
    "              'lr__solver': ['liblinear', 'lbfgs'],\n",
    "              'lr__penalty': ['l1', 'l2', 'none'] # running this with the lasso and ridge techniques by applying a penalty, also running with no penalty\n",
    "              }\n",
    "\n",
    "gs = GridSearchCV(pipe,\n",
    "                 param_grid = pipe_params,\n",
    "                 cv = 5, # cross validation of 5\n",
    "                 scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I want to try to get the best result for our Logistic Regression model, I set up a gridsearch to run 540 possible models in order to return the best fit.  For hyperparameters, we looked at 1-grams, 2-grams, and 3-grams.  We tried with my custom stopwords, english stopwords, and no stopwords.  We tried it both with and and without the use of tfidf, 2 different solvers, and and also tried applying a lasso penalty, ridge penalty, or no penalty.  Each model went through a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "lr_fit = gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fit.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9295238095238095"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9214285714285714"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(lr_fit.best_estimator_, X, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__penalty': 'none',\n",
       " 'lr__solver': 'lbfgs',\n",
       " 'tfidf__use_idf': True,\n",
       " 'vectorizer__ngram_range': (1, 1),\n",
       " 'vectorizer__stop_words': 'english'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fit.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best Logistic Regression model involved no penalty, the \"lbfgs\" solver, use of TFIDF, an ngram range of (1,1), and sklearn's English stop words.  This model was overfit since it had a perfect training score, but a .9295238095238095 testing score.  Our cross-validation score shows that our model would perform relatively similar against unseen data as it did against our test data, but it is fairly close.  This is a significant improvement from our baseline model of .5212, but we can still try to achieve better accuracy.\n",
    "\n",
    "I would like to try a Naive Bayes model on this data to see if we can improve accuracy.  Since I want to try models with and without TFIDF, I will have to build a Multinomial Naive Bayes model for my tests without TFIDF and then build a Gaussian Naive Bayes model for running TFIDF.  First, let's try our Multinomial Naive Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([ # instantiate\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# set hyperparameters\n",
    "\n",
    "pipe_params = {'vectorizer__ngram_range'  : [(1,1),(1,2), (1,3)],\n",
    "              'vectorizer__stop_words': [None, 'english', stopwords],\n",
    "              'mnb__alpha': [.12, .125, .13] #tested out other alpha values and got it narrowed to this\n",
    "              }\n",
    "\n",
    "gsnb = GridSearchCV(pipe,\n",
    "                 param_grid = pipe_params,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy',\n",
    "                 verbose = 2 # let's get progress updates in real time this time\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best result for the Multinomial Naive Bayes model, I set up a gridsearch to run 135 possible models in order to return the best fit.  For hyperparameters, we looked at 1-grams, 2-grams, and 3-grams.  We tried with my custom stopwords, english stopwords, and no stopwords.  Lastly, we tried adjusting our alpha valaues and reran this many times before settling on the values we chose.  Each model went through a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.12, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.125, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.9s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  mnb__alpha=0.13, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 135 out of 135 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "mnb_fit = gsnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mnb__alpha': 0.12,\n",
       " 'vectorizer__ngram_range': (1, 2),\n",
       " 'vectorizer__stop_words': 'english'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_fit.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_fit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9247619047619049"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mnb_fit.best_estimator_, X, y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best fit involed an alpha of .12, ngram range of (1,2), and the sklearn English stop words.  This model was overfit because it had a perfect training score and a 0.93 testing score.  The 0.93 testing score is still significantly more accutate as our baseline model.  Our cross-validation score shows that our model may not perform as well against unseen data as it did against our test data since the testing score was higher than the cross-validation score.  Next, I want to see how TFIDF would perform with a Naive Bayes model, so I need to try a Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin): #from Mahdi\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dense transformer is required in order for the Gaussian Naive Bayes model to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([ #instantiate transformers and model\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('_', DenseTransformer()),\n",
    "    ('gnb', GaussianNB())\n",
    "])\n",
    "\n",
    "# set hyperparameters\n",
    "\n",
    "pipe_params = {'vectorizer__ngram_range'  : [(1,1),(1,2), (1,3)],\n",
    "              'vectorizer__stop_words': [None, 'english', stopwords],\n",
    "              'tfidf__use_idf'  : [True, False]\n",
    "              }\n",
    "\n",
    "gsgnb = GridSearchCV(pipe,\n",
    "                 param_grid = pipe_params,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy',\n",
    "                 verbose = 2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I want to try to get the best result for our Gaussian Naive Bayes model, I set up a gridsearch to run 90 possible models in order to return the best fit.  For hyperparameters, we looked at 1-grams, 2-grams, and 3-grams.  We tried with my custom stopwords, english stopwords, and no stopwords.  We tried it both with and and without the use of tfidf.  Each model went through a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   5.5s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   4.1s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.9s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.8s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.8s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.8s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   3.0s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.8s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   3.1s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   3.3s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   4.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.4s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.2s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.9s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=  15.5s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   9.2s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=  10.5s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   8.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   8.6s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   5.2s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   5.3s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   5.4s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   6.7s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   5.9s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   7.0s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   6.2s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.8s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   6.3s\n",
      "[CV] tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.6s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.3s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.3s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.8s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   3.1s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   3.0s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.9s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   3.0s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.8s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.7s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   3.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   4.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=  12.0s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=  11.5s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   9.9s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=  10.6s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   9.7s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   6.9s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   5.4s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   5.0s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   5.1s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   4.7s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   6.2s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   6.2s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.8s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.9s\n",
      "[CV] tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   5.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:  5.8min finished\n"
     ]
    }
   ],
   "source": [
    "gnb_fit = gsgnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidf__use_idf': False,\n",
       " 'vectorizer__ngram_range': (1, 3),\n",
       " 'vectorizer__stop_words': 'english'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_fit.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8857142857142857"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_fit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8719047619047618"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(gnb_fit.best_estimator_, X, y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best fit didn't include the use of TFIDF, which is surprising, since the Gaussian model is usually better fit for float values generated by TFIDF.  This model also found the ngram range of (1,3) and sklearn's English stopwords most effective.\n",
    "\n",
    "Since the best model here didn't involve the use of TFIDF, it's no surprise that the Gaussian Naive Bayes model was less accurate than our Multinomial Naive Bayes.\n",
    "\n",
    "This model was overfit because it had a perfect training score with a lesser testing score of 0.89.  This testing score is still higher than our baseline model.  Our cross-validation score shows that our model may not perform as well against unseen data as it did against our test data since the testing score was higher than the cross-validation score.  While the Multinomial Naive Bayes model is certainly our more effective Naive Bayes model, I feel as is all classification problems should attempt to fit a KNN model, so I will set that up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('knn', KNeighborsClassifier())])\n",
    "\n",
    "# set hyperparameters\n",
    "\n",
    "pipe_params = {'vectorizer__ngram_range'  : [(1,1),(1,2), (1,3)],\n",
    "              'vectorizer__stop_words': [None, 'english', stopwords],\n",
    "              'tfidf__use_idf'  : [True, False],\n",
    "              'knn__n_neighbors': [3,5,9],\n",
    "              'knn__weights' : ['uniform', 'distance'], # distance gives more predictive power to closer neighbors\n",
    "               'knn__p': [1,2,'p'] # allows us to test Minkowski, Euclidean, and Manhattan measurements\n",
    "               \n",
    "              }\n",
    "\n",
    "gsknn = GridSearchCV(pipe,\n",
    "                 param_grid = pipe_params,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy',\n",
    "                 verbose = 2   \n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I want to try to get the best result for our KNN model, I set up a gridsearch to run 2120 possible models (this took a little time) in order to return the best fit.  For hyperparameters, we looked at 1-grams, 2-grams, and 3-grams.  We tried with my custom stopwords, english stopwords, and no stopwords.  We tried it both with and and without the use of tfidf.  We also tried running it by looking at the closest 3,5, and 9 neighbors, tried it by weighting and not-weighting neighbor distance, all with the Euclidean, Minkowski, and Mahattan metrics.\n",
    "\n",
    "Each model went through a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=3, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=5, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=1, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=2, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.3s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=uniform, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.1s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brandongreenspan/opt/anaconda3/envs/dsi/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "  FitFailedWarning)\n",
      "[Parallel(n_jobs=1)]: Done 1620 out of 1620 | elapsed: 15.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  knn__n_neighbors=9, knn__p=p, knn__weights=distance, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n"
     ]
    }
   ],
   "source": [
    "knn_fit = gsknn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knn__n_neighbors': 5,\n",
       " 'knn__p': 2,\n",
       " 'knn__weights': 'distance',\n",
       " 'tfidf__use_idf': True,\n",
       " 'vectorizer__ngram_range': (1, 3),\n",
       " 'vectorizer__stop_words': 'english'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_fit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_fit.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9066666666666666"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_fit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8709523809523809"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(knn_fit.best_estimator_, X, y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best fit involed predictions off the 5 nearest neighbors while using the Euclidean metric, assigning weight to the neighbors based on distance.  This included use of TFIDF with the ngram range of (1,3) and sklearn's English stop words.\n",
    "\n",
    "This model was overfit because it had a perfect training score, which was higher than our 0.91 testing score.  This is still more accurate than our baseline model.  Our cross-validation score shows that our model may not perform as well against unseen data as it did against our test data since the testing score was higher than the cross-validation score.  Lastly, I want to attempt a Support Vector Machine model in order to see if we can better classify in a multidimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([ #instantiate\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svc', SVC())])\n",
    "\n",
    "# set hyperparameters\n",
    "\n",
    "pipe_params = {'vectorizer__ngram_range'  : [(1,1),(1,2), (1,3)],\n",
    "              'vectorizer__stop_words': [None, 'english', stopwords],\n",
    "              'tfidf__use_idf'  : [True, False],\n",
    "               'svc__kernel': ['linear', 'poly', 'rbf'],\n",
    "               'svc__random_state': [63]\n",
    "               \n",
    "              }\n",
    "\n",
    "gssvc = GridSearchCV(pipe,\n",
    "                 param_grid = pipe_params,\n",
    "                 cv = 5,\n",
    "                 scoring = 'accuracy',\n",
    "                 verbose = 2   \n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I want to try to get the best result for our SVM model, I set up a gridsearch to run 90 possible models in order to return the best fit.  For hyperparameters, we looked at 1-grams, 2-grams, and 3-grams.  We tried with my custom stopwords, english stopwords, and no stopwords.  We tried it both with and and without the use of tfidf.  Lastly we tried all 3 different kernels that this method allows.\n",
    "\n",
    "Each model went through a 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.0s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.0s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.0s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.6s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.6s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.1s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.0s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.6s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.6s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   2.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.4s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.8s\n",
      "[CV] svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=linear, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.0s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.6s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   2.0s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   5.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   4.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   4.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.6s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   2.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.7s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.6s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   4.0s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.2s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.0s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.9s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.0s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.5s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.4s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.1s\n",
      "[CV] svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=poly, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   1.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   3.5s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.5s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=True, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=None, total=   0.9s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=english, total=   0.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 1), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   0.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=None, total=   2.1s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.6s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=english, total=   1.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   2.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.3s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 2), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.7s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.2s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.4s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   3.5s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=None, total=   4.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.5s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.5s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.5s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   2.0s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=english, total=   1.5s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n",
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.8s\n",
      "[CV] svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svc__kernel=rbf, svc__random_state=63, tfidf__use_idf=False, vectorizer__ngram_range=(1, 3), vectorizer__stop_words=['the', 'and', 'to', 'of', 'it', 'in', 'is', 'that', 'for', 'this', 'but', 'was', 'on', 'just', 'season', 'you', 'show', 'with', 'he', 'so', 'be', 'my', 'have', 'like', 'they', 'or', 'can', 'what', 'me', 'episode', 'of the', 'at', 'are', 'she', 'as', 'all', 'in the', 'about', 'there', 'from', 'if', 'her', 'when', 'not', 'would', 'one', 'has', 'know', 'the show', 'his', 'up', 'out', 'an', 'we', 'time', 'think', 'how', 'get', 'really', 'do', 'who', 'first', 'some', 'don', 'by', 'been'], total=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 270 out of 270 | elapsed:  7.2min finished\n"
     ]
    }
   ],
   "source": [
    "sv_fit = gssvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993650793650793"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_fit.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9371428571428572"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_fit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(sv_fit.best_estimator_, X, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svc__kernel': 'linear',\n",
       " 'svc__random_state': 63,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vectorizer__ngram_range': (1, 2),\n",
       " 'vectorizer__stop_words': 'english'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv_fit.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best fit involed using the linear kernel with TFIDF, a ngram range of (1,2), and sklearn's English stopwords.\n",
    "\n",
    "This model was overfit because it had a .999 training score, which was higher than our 0.940 testing score.  This is still more accurate than our baseline model.  Our cross-validation score shows that our model may not perform as well against unseen data as it did against our test data since the testing score was higher than the cross-validation score. \n",
    "\n",
    "Let's compare these models to see which one we should look to further evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy score for our baseline model is: 0.5148.\n",
      "Training accuracy score for our Logistic Regression model is: 1.0.\n",
      "Testing accuracy score for our Logistic Regression model is: 0.9295.\n",
      "Training accuracy score for our Multinomial Naive Bayes model is: 1.0.\n",
      "Testing accuracy score for our Multinomial Naive Bayes model is: 0.9333.\n",
      "Training accuracy score for our Gaussian Naive Bayes model is: 1.0.\n",
      "Testing accuracy score for our Gaussian Naive Bayes model is: 0.8857.\n",
      "Training accuracy score for our KNN model is: 1.0.\n",
      "Testing accuracy score for our KNN  model is: 0.9067.\n",
      "Training accuracy score for our Support Vector model is: 0.9994.\n",
      "Testing accuracy score for our Support Vector model is: 0.9371.\n"
     ]
    }
   ],
   "source": [
    "print(f'Testing accuracy score for our baseline model is: {round(baseline, 4)}.')\n",
    "\n",
    "\n",
    "print(f'Training accuracy score for our Logistic Regression model is: {round(lr_fit.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our Logistic Regression model is: {round(lr_fit.score(X_test, y_test),4)}.')\n",
    "\n",
    "\n",
    "print(f'Training accuracy score for our Multinomial Naive Bayes model is: {round(mnb_fit.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our Multinomial Naive Bayes model is: {round(mnb_fit.score(X_test, y_test),4)}.')\n",
    "\n",
    "print(f'Training accuracy score for our Gaussian Naive Bayes model is: {round(gnb_fit.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our Gaussian Naive Bayes model is: {round(gnb_fit.score(X_test, y_test),4)}.')\n",
    "\n",
    "print(f'Training accuracy score for our KNN model is: {round(knn_fit.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our KNN  model is: {round(knn_fit.score(X_test, y_test),4)}.')\n",
    "\n",
    "print(f'Training accuracy score for our Support Vector model is: {round(sv_fit.score(X_train, y_train),4)}.')\n",
    "print(f'Testing accuracy score for our Support Vector model is: {round(sv_fit.score(X_test, y_test),4)}.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models were overfit since they all had a better training score than testing score.  For a predictive model, I would select the Support Vector model because it had the highest testing score and cross validation score, which would imply that it would perform the best if exposed to unseen data.  For interpretability purposes, I am selecting the Logistic Regression model.  The Logistic Regression model performed similarly enough to the Multinomial Naive Bayes model, so I can justify this decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I want to create a confusion matrix, so I can find the accuracy, specificity, sensitivity, and precision rates for my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds = lr_fit.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, lr_preds).ravel() # From Danielle Medellin's confusion matrix setup\n",
    "cm = confusion_matrix(y_test, lr_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[234,  21],\n",
       "       [ 16, 254]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The Good Place</th>\n",
       "      <th>Parks &amp; Rec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The Good Place</th>\n",
       "      <td>234</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parks &amp; Rec</th>\n",
       "      <td>16</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                The Good Place  Parks & Rec\n",
       "The Good Place             234           21\n",
       "Parks & Rec                 16          254"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_df = pd.DataFrame(cm)\n",
    "cm_df.columns = ['The Good Place', 'Parks & Rec']\n",
    "\n",
    "cm_df.index = ['The Good Place', \"Parks & Rec\"]\n",
    "#This adds the appropriate row and column names for the confusion matrix\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm_df, annot = True, fmt = 'g', cmap = 'Blues')\n",
    "plt.title('Confusion Matrix', size = 15)\n",
    "plt.ylabel('Predictions', size = 15)\n",
    "plt.xlabel('Actual Class', size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Rate: 0.9295238095238095\n",
      "Sensitivity Rate: 0.9407407407407408\n",
      "Specificity Rate: 0.9176470588235294\n",
      "Precision Rate: 0.9236363636363636\n",
      "True Negative Rate: 0.936\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp + tn)/(tp+fn+fp+tn)\n",
    "print(f'Accuracy Rate: {accuracy}')\n",
    "\n",
    "sensitivity = tp/(tp+fn)\n",
    "print(f'Sensitivity Rate: {sensitivity}')\n",
    "            \n",
    "specificity = tn/(tn+fp)\n",
    "print(f'Specificity Rate: {specificity}')\n",
    "\n",
    "precision = tp/(tp+fp)\n",
    "print(f'Precision Rate: {precision}')\n",
    "\n",
    "true_neg = tn/(tn+fn)\n",
    "print(f'True Negative Rate: {true_neg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy rate states that 93% of all predictions the model made were correct.\n",
    "\n",
    "Our sensitivity rate states that 96% of the time that a row was in the Parks & Rec subreddit, our model was able to predict the row correctly.\n",
    "\n",
    "Our specificity rate states that 90% of the time that a row was not in the Parks & Rec subreddit, our model was able to predict the row correctly.  Since Parks & Rec was our majority class, I'm not surprised to see that our model had greater sensitivity than specificity, given the sensitivity/specificity tradeoff.\n",
    "\n",
    "Our precision rate states that out of all the times our model predicted our majority class (Parks & Rec), our model was correct 91% of the time.\n",
    "\n",
    "Our true negative rate states that out of all the times our model predicted our minority class (The Good Place), our model was correct 96% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to be able to evaluate the coeffiecients to get a sense as to what terms give the model its greatest predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = {\n",
    "    \"column\" : lr_fit.best_estimator_.steps[0][1].get_feature_names(), \n",
    "    \"coef\"   : lr_fit.best_estimator_.steps[2][1].coef_\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '00pm',\n",
       " '01',\n",
       " '02',\n",
       " '021b522cd30caed14d644041db53a4ec',\n",
       " '0275ad358fe747cae7ad2fdbb8bda1a3',\n",
       " '03x12',\n",
       " '04',\n",
       " '045c1aa1b90f61f6f712df4d0b90d62c',\n",
       " '05',\n",
       " '0583',\n",
       " '06',\n",
       " '08',\n",
       " '09',\n",
       " '0edipamaas',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '1000x500',\n",
       " '1003',\n",
       " '100lbs',\n",
       " '100x',\n",
       " '101',\n",
       " '104',\n",
       " '108',\n",
       " '1080p',\n",
       " '109',\n",
       " '10hr',\n",
       " '10jatk',\n",
       " '10th',\n",
       " '11',\n",
       " '110',\n",
       " '11am',\n",
       " '11th',\n",
       " '12',\n",
       " '1201869241',\n",
       " '121',\n",
       " '122',\n",
       " '12358w',\n",
       " '123889331',\n",
       " '125',\n",
       " '128',\n",
       " '13',\n",
       " '136',\n",
       " '1373996494',\n",
       " '13dtre1okb0',\n",
       " '13mins',\n",
       " '14',\n",
       " '146',\n",
       " '14thyear',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '151',\n",
       " '1518',\n",
       " '1520',\n",
       " '154',\n",
       " '159',\n",
       " '16',\n",
       " '1650',\n",
       " '17',\n",
       " '18',\n",
       " '1800s',\n",
       " '181188768',\n",
       " '1865',\n",
       " '19',\n",
       " '1926',\n",
       " '1944',\n",
       " '1945',\n",
       " '1972',\n",
       " '1996',\n",
       " '1997',\n",
       " '1ksupport',\n",
       " '1nqtif',\n",
       " '1o2oredoblueo',\n",
       " '1st',\n",
       " '1u1',\n",
       " '1x10',\n",
       " '20',\n",
       " '200',\n",
       " '2002',\n",
       " '2009',\n",
       " '201',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '20160919173629',\n",
       " '2017',\n",
       " '2018',\n",
       " '202',\n",
       " '2043',\n",
       " '208327',\n",
       " '20and',\n",
       " '20good',\n",
       " '20place',\n",
       " '20recreation',\n",
       " '20s',\n",
       " '20th',\n",
       " '20tht',\n",
       " '20tremendous',\n",
       " '21',\n",
       " '210',\n",
       " '215',\n",
       " '21h54',\n",
       " '21st',\n",
       " '22',\n",
       " '226',\n",
       " '23',\n",
       " '23019677',\n",
       " '234634',\n",
       " '24',\n",
       " '241031',\n",
       " '246',\n",
       " '24th',\n",
       " '25',\n",
       " '255',\n",
       " '26',\n",
       " '26124724',\n",
       " '27',\n",
       " '27smq3',\n",
       " '2814werewolf',\n",
       " '2885',\n",
       " '28ds7i',\n",
       " '28rbfv',\n",
       " '28th',\n",
       " '29',\n",
       " '29th',\n",
       " '2ailmg2l8rtaa',\n",
       " '2c159',\n",
       " '2caps',\n",
       " '2f',\n",
       " '2k16',\n",
       " '2lv7revtinjnwd5fy2rumq',\n",
       " '2nd',\n",
       " '2pst',\n",
       " '2x23',\n",
       " '30',\n",
       " '300',\n",
       " '301',\n",
       " '305',\n",
       " '30829964_320188731842082_2210264181398044672_n',\n",
       " '30pm',\n",
       " '30th',\n",
       " '31',\n",
       " '310',\n",
       " '311',\n",
       " '311711',\n",
       " '32',\n",
       " '322',\n",
       " '3232',\n",
       " '32968',\n",
       " '33316',\n",
       " '33627',\n",
       " '33878',\n",
       " '34205',\n",
       " '34460',\n",
       " '35',\n",
       " '36n05l',\n",
       " '36th',\n",
       " '37',\n",
       " '3750',\n",
       " '3857',\n",
       " '392',\n",
       " '3afkk3',\n",
       " '3am',\n",
       " '3avyad',\n",
       " '3bob',\n",
       " '3dmovies',\n",
       " '3fnbi',\n",
       " '3kool5you',\n",
       " '3rd',\n",
       " '3x06',\n",
       " '3x0fk5f9e7b11',\n",
       " '3x11',\n",
       " '3y2flfkz51431',\n",
       " '40',\n",
       " '40221',\n",
       " '405',\n",
       " '40s',\n",
       " '41',\n",
       " '420',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46mrkb2rhwv01',\n",
       " '48',\n",
       " '480p',\n",
       " '485',\n",
       " '4gdw0qs03qs',\n",
       " '4gsylp',\n",
       " '4mwj81',\n",
       " '4np4wsb',\n",
       " '4th',\n",
       " '4x01',\n",
       " '4x2',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '50th',\n",
       " '50xx853tjhx21',\n",
       " '52',\n",
       " '520',\n",
       " '521',\n",
       " '53',\n",
       " '55',\n",
       " '551',\n",
       " '570n3d_dud3',\n",
       " '575658',\n",
       " '57hggo',\n",
       " '58',\n",
       " '5b9d7208',\n",
       " '5e',\n",
       " '5k',\n",
       " '5p0juq',\n",
       " '5pm',\n",
       " '5th',\n",
       " '60',\n",
       " '60134',\n",
       " '612',\n",
       " '61310168',\n",
       " '6141747',\n",
       " '63',\n",
       " '668',\n",
       " '68',\n",
       " '6th',\n",
       " '720',\n",
       " '720p',\n",
       " '73',\n",
       " '75',\n",
       " '750',\n",
       " '78',\n",
       " '7ncbq3nti4x21',\n",
       " '7pm',\n",
       " '7th',\n",
       " '7x03',\n",
       " '7zci_obml_g',\n",
       " '80',\n",
       " '800',\n",
       " '800ish',\n",
       " '801',\n",
       " '802',\n",
       " '80s',\n",
       " '826la',\n",
       " '82kets',\n",
       " '83',\n",
       " '85',\n",
       " '862240152583901184',\n",
       " '89',\n",
       " '8mpbd',\n",
       " '8pm',\n",
       " '8th',\n",
       " '90',\n",
       " '90l8rk',\n",
       " '90s',\n",
       " '92',\n",
       " '948',\n",
       " '98',\n",
       " '98252',\n",
       " '99',\n",
       " '997',\n",
       " '9c',\n",
       " '9pm',\n",
       " '9th',\n",
       " '___',\n",
       " '___________________________________________________',\n",
       " '_emordnilap',\n",
       " '_in_',\n",
       " '_inc',\n",
       " '_itdoesntmatter_',\n",
       " '_nbbgrbxrhs',\n",
       " '_potatardis_',\n",
       " '_should_',\n",
       " '_stole_',\n",
       " '_very_stable_genius_',\n",
       " '_wykprojectalpha_',\n",
       " '_your_face',\n",
       " 'a4b91fd97cfb18d057fd4ae4fec7c609',\n",
       " 'a_ronn',\n",
       " 'aaaaaaaaaoo',\n",
       " 'aaaaaaand',\n",
       " 'aaaand',\n",
       " 'aag11',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abbi',\n",
       " 'abc',\n",
       " 'abe',\n",
       " 'abeds_bananastand',\n",
       " 'abemoilan',\n",
       " 'abigail',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abjanets',\n",
       " 'able',\n",
       " 'aboutabruise',\n",
       " 'abrahamic',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'absent',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolve',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'abstain',\n",
       " 'absurdist',\n",
       " 'absurdity',\n",
       " 'abuse',\n",
       " 'abyss',\n",
       " 'ac2bhappy',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'acamu5',\n",
       " 'accent',\n",
       " 'accented',\n",
       " 'accents',\n",
       " 'accept',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accidental_antilogy',\n",
       " 'accidentally',\n",
       " 'accidents',\n",
       " 'accolades',\n",
       " 'accommodating',\n",
       " 'accompanied',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accountable',\n",
       " 'accountant',\n",
       " 'accounting',\n",
       " 'accrue',\n",
       " 'accrued',\n",
       " 'accumulate',\n",
       " 'accused',\n",
       " 'ache',\n",
       " 'aches',\n",
       " 'achieve',\n",
       " 'achievements',\n",
       " 'achieving',\n",
       " 'acid',\n",
       " 'acidcat',\n",
       " 'acknowledge',\n",
       " 'acknowledgement',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acmorgan',\n",
       " 'acquaintance',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'actively',\n",
       " 'activist',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actuality',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'adapting',\n",
       " 'adate',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addict',\n",
       " 'addiction',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'addon',\n",
       " 'addons',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adele',\n",
       " 'adept',\n",
       " 'adjacent',\n",
       " 'administration',\n",
       " 'admirable',\n",
       " 'admiralofawesomeness',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admiringly',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admittance',\n",
       " 'admitted',\n",
       " 'admittedly',\n",
       " 'admitting',\n",
       " 'ado',\n",
       " 'adolescence',\n",
       " 'adopted',\n",
       " 'adorable',\n",
       " 'adorableexplosions',\n",
       " 'adorably',\n",
       " 'adore',\n",
       " 'adores',\n",
       " 'adrimfayn',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advancement',\n",
       " 'advantage',\n",
       " 'adventists',\n",
       " 'adversity',\n",
       " 'advert',\n",
       " 'advice',\n",
       " 'advised',\n",
       " 'advocate',\n",
       " 'aew',\n",
       " 'af',\n",
       " 'afaik',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affects',\n",
       " 'affiliated',\n",
       " 'affiliates',\n",
       " 'afford',\n",
       " 'affords',\n",
       " 'afhy6wa',\n",
       " 'aficionados',\n",
       " 'afraid',\n",
       " 'african',\n",
       " 'afterlife',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'agnes',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agrees',\n",
       " 'agt',\n",
       " 'ah',\n",
       " 'ahainen',\n",
       " 'ahead',\n",
       " 'ahope4836',\n",
       " 'ai',\n",
       " 'aided',\n",
       " 'aidenpng',\n",
       " 'aims',\n",
       " 'air',\n",
       " 'airdate',\n",
       " 'aired',\n",
       " 'airing',\n",
       " 'airport',\n",
       " 'airs',\n",
       " 'aka',\n",
       " 'akanefive',\n",
       " 'al',\n",
       " 'alafolie29',\n",
       " 'alamedagrace',\n",
       " 'alarmed',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'albusseverus14',\n",
       " 'alcohol',\n",
       " 'aldeki',\n",
       " 'aldoushuxleyjr',\n",
       " 'alecjperkins213',\n",
       " 'alert',\n",
       " 'alex',\n",
       " 'alexa',\n",
       " 'alexgreen239',\n",
       " 'alexisgreat420',\n",
       " 'alexpenn',\n",
       " 'alias',\n",
       " 'alie',\n",
       " 'alien',\n",
       " 'alienation',\n",
       " 'aligned',\n",
       " 'aligning',\n",
       " 'alignment',\n",
       " 'alivanis',\n",
       " 'alive',\n",
       " 'alived',\n",
       " 'allegedly',\n",
       " 'allegiance',\n",
       " 'allegorical',\n",
       " 'allegory',\n",
       " 'alliance',\n",
       " 'allies',\n",
       " 'allot',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'allude',\n",
       " 'almighty',\n",
       " 'almonds',\n",
       " 'alongside',\n",
       " 'alot',\n",
       " 'alps',\n",
       " 'alright',\n",
       " 'altar',\n",
       " 'altered',\n",
       " 'altering',\n",
       " 'alternate',\n",
       " 'alternated',\n",
       " 'alternating',\n",
       " 'alternative',\n",
       " 'alternatively',\n",
       " 'alternatives',\n",
       " 'altruism',\n",
       " 'altruistic',\n",
       " 'alwayspro',\n",
       " 'ama',\n",
       " 'amateur',\n",
       " 'amazed',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazingmexican9',\n",
       " 'amazon',\n",
       " 'ambassador',\n",
       " 'ambassadors',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambitious',\n",
       " 'ambrose',\n",
       " 'amend',\n",
       " 'amenities',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'americorps',\n",
       " 'amgmercedesbaby',\n",
       " 'amicable',\n",
       " 'amino',\n",
       " 'aminoapps',\n",
       " 'amnesty',\n",
       " 'amounts',\n",
       " 'amp',\n",
       " 'amplified',\n",
       " 'amusing',\n",
       " 'amy',\n",
       " 'anagonye',\n",
       " 'anal',\n",
       " 'analyse',\n",
       " 'analysing',\n",
       " 'analysis',\n",
       " 'ancestors',\n",
       " 'anchor',\n",
       " 'anchoring',\n",
       " 'ancient',\n",
       " 'andbruno',\n",
       " 'anderson',\n",
       " 'andrei',\n",
       " 'andrew9360',\n",
       " 'andy',\n",
       " 'andys',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'angelic',\n",
       " 'angelique',\n",
       " 'angelnike',\n",
       " 'angeloanthony',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'angle',\n",
       " 'angry',\n",
       " 'anguskirk',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'animation',\n",
       " 'anime',\n",
       " 'aniston',\n",
       " 'ann',\n",
       " 'annalise13ra',\n",
       " 'annan',\n",
       " 'annapurna',\n",
       " 'anne',\n",
       " 'anneperkins',\n",
       " 'anniversary',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announcements',\n",
       " 'announces',\n",
       " 'annoy',\n",
       " 'annoyance',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annoyingly',\n",
       " 'annoys',\n",
       " 'anomaly',\n",
       " 'anonymous21347',\n",
       " 'anotherandomer',\n",
       " 'ansari',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answering',\n",
       " 'answers',\n",
       " 'ant',\n",
       " 'anti',\n",
       " 'anticipate',\n",
       " 'antics',\n",
       " 'antiej',\n",
       " 'antorqs',\n",
       " 'anxieties',\n",
       " 'anxiety',\n",
       " 'anybody',\n",
       " 'anycase',\n",
       " 'anymore',\n",
       " 'anyones',\n",
       " 'anytime',\n",
       " 'anyways',\n",
       " 'anzari',\n",
       " 'ap',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apathetic',\n",
       " 'apathy',\n",
       " 'apathylikes',\n",
       " 'aphroditeandthexbox',\n",
       " 'apocalypse_meow2',\n",
       " 'apocalyptic',\n",
       " 'apologetic',\n",
       " 'apologies',\n",
       " 'apologize',\n",
       " 'app',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appealed',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appearances',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'appears',\n",
       " 'appetite',\n",
       " 'appetizers',\n",
       " 'apple',\n",
       " 'apples',\n",
       " 'applied',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'appointed',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciation',\n",
       " 'appreciative',\n",
       " 'apprehensive',\n",
       " 'apprentice',\n",
       " 'approach',\n",
       " 'approached',\n",
       " 'approaches',\n",
       " 'appropriate',\n",
       " 'approval',\n",
       " 'approve',\n",
       " 'approved',\n",
       " 'approx',\n",
       " 'approximation',\n",
       " 'apri',\n",
       " 'april',\n",
       " 'aprils',\n",
       " 'aproach',\n",
       " 'apt',\n",
       " 'aquatic',\n",
       " 'aquawoman68',\n",
       " 'aquickburner',\n",
       " 'ar',\n",
       " 'ar417',\n",
       " 'arabic',\n",
       " 'arbitrarily',\n",
       " 'arc',\n",
       " 'arcade',\n",
       " 'arcanine',\n",
       " 'arch',\n",
       " 'archangel',\n",
       " 'archery',\n",
       " 'archetype',\n",
       " 'architect',\n",
       " 'architects',\n",
       " 'architectural',\n",
       " 'architecture',\n",
       " 'archive',\n",
       " 'archives',\n",
       " 'arcy',\n",
       " 'ardyraindropsrd',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'aren',\n",
       " 'arent',\n",
       " 'argenteam',\n",
       " 'arguable',\n",
       " 'arguably',\n",
       " 'argue',\n",
       " 'argued',\n",
       " 'argues',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'arguments',\n",
       " 'aria',\n",
       " 'ariana',\n",
       " 'arinlome',\n",
       " 'arisen',\n",
       " 'aristotle',\n",
       " 'arizona',\n",
       " 'arjungames73',\n",
       " 'arm',\n",
       " 'armies',\n",
       " 'armisen',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'arnett',\n",
       " 'arranged',\n",
       " 'arrangement',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'arrivals',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arrow',\n",
       " 'arrows',\n",
       " 'art',\n",
       " 'artemis_dubois',\n",
       " 'artichoke19',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'articlethumbs',\n",
       " 'artificial',\n",
       " 'artist',\n",
       " 'arts',\n",
       " 'artyen',\n",
       " 'asapaspossible',\n",
       " 'asbhopal1',\n",
       " 'ascultone21',\n",
       " 'ashketchum1845',\n",
       " 'ashole',\n",
       " 'asian',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'ask_adam_scott_anything',\n",
       " 'ask_adam_scott_me_anything',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'askronswanson',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'aspberger',\n",
       " 'aspect',\n",
       " 'aspects',\n",
       " 'aspirations',\n",
       " 'ass',\n",
       " 'assassinated',\n",
       " 'assert',\n",
       " 'asses',\n",
       " 'assholes',\n",
       " 'assign',\n",
       " 'assigned',\n",
       " 'assigning',\n",
       " 'assignment',\n",
       " 'assigns',\n",
       " 'assing',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'associated',\n",
       " 'association',\n",
       " 'assume',\n",
       " 'assumed',\n",
       " 'assuming',\n",
       " 'assumption',\n",
       " 'assuredly',\n",
       " 'astechgold',\n",
       " 'astonish',\n",
       " 'astonished',\n",
       " 'astrocanyounaut',\n",
       " 'ate',\n",
       " 'athenasnike',\n",
       " 'atilegacy',\n",
       " 'atlanta',\n",
       " 'atlantas',\n",
       " 'atleast',\n",
       " 'atmosphere',\n",
       " 'atmospheric',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attacks',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempts',\n",
       " 'attended',\n",
       " 'attendee',\n",
       " 'attendees',\n",
       " 'attending',\n",
       " 'attention',\n",
       " 'attican101',\n",
       " 'attire',\n",
       " 'attitude',\n",
       " 'attitudes',\n",
       " 'attract',\n",
       " 'attracted',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'attractiveness',\n",
       " 'attributed',\n",
       " 'aubrey',\n",
       " 'audiblecoupon',\n",
       " 'audibly',\n",
       " 'audience',\n",
       " 'audio',\n",
       " 'august',\n",
       " 'aura',\n",
       " 'aurelio23',\n",
       " 'austin',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'authentic',\n",
       " 'authentically',\n",
       " 'authenticity',\n",
       " 'author',\n",
       " 'authorities',\n",
       " 'authority',\n",
       " 'authuser',\n",
       " 'autism',\n",
       " 'autobiographies',\n",
       " 'autocorrect',\n",
       " 'autographed',\n",
       " 'autographs',\n",
       " 'automatic',\n",
       " 'automatically',\n",
       " 'autres',\n",
       " 'autumn',\n",
       " 'auxeralis',\n",
       " 'av',\n",
       " 'avail',\n",
       " 'available',\n",
       " 'availeable',\n",
       " 'avant',\n",
       " 'avclub',\n",
       " 'avengers',\n",
       " 'average',\n",
       " 'aveydey',\n",
       " 'avocado',\n",
       " 'avoid',\n",
       " 'avoiding',\n",
       " 'aw',\n",
       " 'awards',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awesomeness',\n",
       " 'awesometoenails',\n",
       " 'awful',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awry',\n",
       " 'aww',\n",
       " 'axe',\n",
       " 'azgrimes',\n",
       " 'aziz',\n",
       " 'aziz_ansaris_modern_romance_ama',\n",
       " 'b99',\n",
       " 'baad',\n",
       " 'baader',\n",
       " 'babe',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'babybuttoneyes',\n",
       " 'bachelor',\n",
       " 'backed',\n",
       " 'backer',\n",
       " 'backfires',\n",
       " 'background',\n",
       " 'backlog',\n",
       " 'backs',\n",
       " 'backstory',\n",
       " 'backtracking',\n",
       " 'backup',\n",
       " 'backwards',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'bad_riter',\n",
       " 'badass',\n",
       " 'badde00',\n",
       " 'badly',\n",
       " 'badness',\n",
       " 'bag',\n",
       " 'bags',\n",
       " 'baie',\n",
       " 'bailadelcorazon',\n",
       " 'bailout',\n",
       " 'baiting',\n",
       " 'baker',\n",
       " 'baking',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'balances',\n",
       " 'bald',\n",
       " 'ball',\n",
       " 'ballon',\n",
       " 'balloon',\n",
       " 'balloons',\n",
       " 'ballroom',\n",
       " 'balls',\n",
       " 'baltar',\n",
       " 'bambadjan',\n",
       " 'band',\n",
       " 'bands',\n",
       " 'bang',\n",
       " 'banger',\n",
       " 'bangs8',\n",
       " 'banjo',\n",
       " 'bank',\n",
       " 'bankrupt',\n",
       " 'bannanadog666',\n",
       " 'banned',\n",
       " 'bannedindc',\n",
       " 'banner',\n",
       " 'banquet',\n",
       " 'bans',\n",
       " 'bar',\n",
       " 'barbecue',\n",
       " 'barber',\n",
       " 'barcelona',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'bargain',\n",
       " 'barking',\n",
       " 'barkley',\n",
       " 'barkos',\n",
       " 'barmaid',\n",
       " 'barn',\n",
       " 'barney',\n",
       " 'barrage',\n",
       " 'barrel',\n",
       " 'barry',\n",
       " 'barshady18',\n",
       " 'bartender',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'based_life',\n",
       " 'baseless',\n",
       " 'baseman',\n",
       " 'basement',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basics',\n",
       " 'basing',\n",
       " 'basis',\n",
       " 'bass',\n",
       " 'bastardized',\n",
       " 'bastards',\n",
       " 'bastion_de_paraplui',\n",
       " 'bat',\n",
       " 'bathroom',\n",
       " 'bathrooms',\n",
       " 'batman',\n",
       " 'battle',\n",
       " 'battles',\n",
       " 'bbf2',\n",
       " 'bbq',\n",
       " 'bbylucy',\n",
       " 'bdreynolds',\n",
       " 'beach',\n",
       " 'beaker',\n",
       " 'bean',\n",
       " 'beanbag',\n",
       " 'beans',\n",
       " ...]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fit.best_estimator_.steps[0][1].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.50084022,  5.70632967, -0.19912162, ...,  5.24069579,\n",
       "         5.24069579,  5.24069579]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fit.best_estimator_.steps[2][1].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = lr_fit.best_estimator_.steps[2][1].coef_ # getting coefficients\n",
    "cols = lr_fit.best_estimator_.steps[0][1].get_feature_names() # getting column names\n",
    "cols = pd.Series(cols) # turn into series\n",
    "coefs = coefs[0]\n",
    "feature_coefs = pd.DataFrame(coefs, index=cols) # put in data frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_coefs.columns = ['coefficient']\n",
    "feature_coefs['odds'] = np.exp(feature_coefs['coefficient'])\n",
    "top_pr_coefs = feature_coefs.sort_values('coefficient',ascending=False).head(15) # top 15 features for Parks & Rec\n",
    "top_gp_coefs = feature_coefs.sort_values('coefficient',ascending=False).tail(15) # top 15 features for The Good Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>-124.723846</td>\n",
       "      <td>6.809605e-55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michael</th>\n",
       "      <td>-113.220824</td>\n",
       "      <td>6.742502e-50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eleanor</th>\n",
       "      <td>-95.610539</td>\n",
       "      <td>2.998269e-42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>janet</th>\n",
       "      <td>-85.200127</td>\n",
       "      <td>9.955314e-38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>-84.601606</td>\n",
       "      <td>1.811296e-37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chidi</th>\n",
       "      <td>-77.184192</td>\n",
       "      <td>3.015304e-34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jason</th>\n",
       "      <td>-61.684452</td>\n",
       "      <td>1.624737e-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>-59.790213</td>\n",
       "      <td>1.080041e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tahani</th>\n",
       "      <td>-53.290424</td>\n",
       "      <td>7.182289e-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tgp</th>\n",
       "      <td>-40.918100</td>\n",
       "      <td>1.696270e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spoilers</th>\n",
       "      <td>-38.843891</td>\n",
       "      <td>1.349934e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twist</th>\n",
       "      <td>-35.413871</td>\n",
       "      <td>4.168226e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theory</th>\n",
       "      <td>-35.132836</td>\n",
       "      <td>5.520814e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shows</th>\n",
       "      <td>-30.049438</td>\n",
       "      <td>8.906249e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kristen</th>\n",
       "      <td>-29.437132</td>\n",
       "      <td>1.642918e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          coefficient          odds\n",
       "place     -124.723846  6.809605e-55\n",
       "michael   -113.220824  6.742502e-50\n",
       "eleanor    -95.610539  2.998269e-42\n",
       "janet      -85.200127  9.955314e-38\n",
       "good       -84.601606  1.811296e-37\n",
       "chidi      -77.184192  3.015304e-34\n",
       "jason      -61.684452  1.624737e-27\n",
       "bad        -59.790213  1.080041e-26\n",
       "tahani     -53.290424  7.182289e-24\n",
       "tgp        -40.918100  1.696270e-18\n",
       "spoilers   -38.843891  1.349934e-17\n",
       "twist      -35.413871  4.168226e-16\n",
       "theory     -35.132836  5.520814e-16\n",
       "shows      -30.049438  8.906249e-14\n",
       "kristen    -29.437132  1.642918e-13"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_gp_coefs.sort_values(by = 'coefficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since The Good Place is the 0 in our binomial, strong coeffiecients will be the most negative coefficients.  This was sorted for better readability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>leslie</th>\n",
       "      <td>91.271073</td>\n",
       "      <td>4.350343e+39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ron</th>\n",
       "      <td>72.275710</td>\n",
       "      <td>2.448736e+31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>andy</th>\n",
       "      <td>60.518786</td>\n",
       "      <td>1.918558e+26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parks</th>\n",
       "      <td>57.808234</td>\n",
       "      <td>1.275842e+25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ben</th>\n",
       "      <td>51.199388</td>\n",
       "      <td>1.720330e+22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tom</th>\n",
       "      <td>46.249176</td>\n",
       "      <td>1.218321e+20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec</th>\n",
       "      <td>45.402175</td>\n",
       "      <td>5.222930e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>april</th>\n",
       "      <td>42.547440</td>\n",
       "      <td>3.006897e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ann</th>\n",
       "      <td>35.292855</td>\n",
       "      <td>2.125653e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pandr</th>\n",
       "      <td>34.805428</td>\n",
       "      <td>1.305585e+15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jerry</th>\n",
       "      <td>34.197243</td>\n",
       "      <td>7.106797e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chris</th>\n",
       "      <td>33.821881</td>\n",
       "      <td>4.882656e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pawnee</th>\n",
       "      <td>30.062195</td>\n",
       "      <td>1.137222e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scripts</th>\n",
       "      <td>27.473066</td>\n",
       "      <td>8.538885e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mark</th>\n",
       "      <td>27.396334</td>\n",
       "      <td>7.908184e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         coefficient          odds\n",
       "leslie     91.271073  4.350343e+39\n",
       "ron        72.275710  2.448736e+31\n",
       "andy       60.518786  1.918558e+26\n",
       "parks      57.808234  1.275842e+25\n",
       "ben        51.199388  1.720330e+22\n",
       "tom        46.249176  1.218321e+20\n",
       "rec        45.402175  5.222930e+19\n",
       "april      42.547440  3.006897e+18\n",
       "ann        35.292855  2.125653e+15\n",
       "pandr      34.805428  1.305585e+15\n",
       "jerry      34.197243  7.106797e+14\n",
       "chris      33.821881  4.882656e+14\n",
       "pawnee     30.062195  1.137222e+13\n",
       "scripts    27.473066  8.538885e+11\n",
       "mark       27.396334  7.908184e+11"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pr_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the impactful coefficients unsurprisingly lined up with a character's name.  For example, if the main character of Parks & Rec, Leslie, was in a row, that row was over 2.216409 sextillion more likely to belong to Parks & Rec.  Our strongest coefficient for a Good Place character, Michael, states that a row containing his name was over 743.0456 octillion times more likely to be classified as belonging to the Good Place (which was pretty much the same odds as getting sent to the bad place in the show).\n",
    "\n",
    "Considering the model seemed to have mostly classified the characters correctly, I want to look further into my misclassifications, so I can start coming up with my show ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1371,  100, 1637, 1117, 1186, 1607,  933, 1517,  514, 1438, 1243,\n",
       "            1624, 1930, 1453,  923, 2075,   70,  135, 1156,  572, 1210,  822,\n",
       "            1248,  349, 1698, 1056, 1428,  214,  934, 2083, 1699, 1014, 1560,\n",
       "             418, 1600,  679,  742],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.loc[y_test != lr_preds].index #shows me misclassified rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = X_test[[1371,  100, 1637, 1117, 1186, 1607,  933, 1517,  514, 1438, 1243,\n",
    "            1624, 1930, 1453,  923, 2075,   70,  135, 1156,  572, 1210,  822,\n",
    "            1248,  349, 1698, 1056, 1428,  214,  934, 2083, 1699, 1014, 1560,\n",
    "             418, 1600,  679,  742]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclass_df = pd.DataFrame(misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['all_text'], dtype='object')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclass_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>Season 3? Just finished binge watching this sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>If given the chance, would you rather be a mem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>Rewatch with my wife Just started a rewatch wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>Where can I find season three episodes 1-7? I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>Hey fat dinks, does anyone know where to watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>Season 2 is airing on Netflix in Australia, Ir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>Girlfriend likes her new nickname My girlfrien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>Do we know when season 2 premieres? I just sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>The cast in other shows? What are some good sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>Any ideas when season 2 will be out on Netflix...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>What line do you constantly repeat in your eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>Anyone else find Denise hilarious? She's a gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>Episode discussion season 1 episode 9 Someone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>Adam Scott on season 2 Why no Scott? He was a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>Can I start with the 2nd season without missin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>Title card font Does anyone know the font that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Appreciation for u/NightTrainDan Just some lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>need a gif: ben and leslie at grizzle in their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>This is my favorite show to binge the entire s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>What are your favorite quotes to say irl?  paw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>Will season 3 come to Netflix? If not, where c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>Is the real Li'l Seabastian still alive? I wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>I made a Twitter account that tweets out regul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>Gif Request: Does anyone have a gif of the sce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>I haven't laughed this hard in a good while Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>Hey Im the New Mod for /r/PandR, the Subreddit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>I think I just lost 5 years of my life... ...w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>I'M GOING TO TYPE EVERY WORD I KNOW! RECTANGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>Season 4 online Sorry if you guys get these th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>Parks and Rec reference I don't know if anybod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>I was Right! (for the most part i think?) www....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>Soooo... What did you think of the new episode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>Watched the whole show on a United flight... [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>Watching Season 4 Episode 10 for about the 5th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>Watching in the UK? I watched (and loved) seas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>Favorite quotes? \"I like people...places...and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>As our final season approaches... With the fin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               all_text\n",
       "1371  Season 3? Just finished binge watching this sh...\n",
       "100   If given the chance, would you rather be a mem...\n",
       "1637  Rewatch with my wife Just started a rewatch wi...\n",
       "1117  Where can I find season three episodes 1-7? I ...\n",
       "1186  Hey fat dinks, does anyone know where to watch...\n",
       "1607  Season 2 is airing on Netflix in Australia, Ir...\n",
       "933   Girlfriend likes her new nickname My girlfrien...\n",
       "1517  Do we know when season 2 premieres? I just sta...\n",
       "514   The cast in other shows? What are some good sh...\n",
       "1438  Any ideas when season 2 will be out on Netflix...\n",
       "1243  What line do you constantly repeat in your eve...\n",
       "1624  Anyone else find Denise hilarious? She's a gre...\n",
       "1930  Episode discussion season 1 episode 9 Someone ...\n",
       "1453  Adam Scott on season 2 Why no Scott? He was a ...\n",
       "923   Can I start with the 2nd season without missin...\n",
       "2075  Title card font Does anyone know the font that...\n",
       "70    Appreciation for u/NightTrainDan Just some lov...\n",
       "135   need a gif: ben and leslie at grizzle in their...\n",
       "1156  This is my favorite show to binge the entire s...\n",
       "572   What are your favorite quotes to say irl?  paw...\n",
       "1210  Will season 3 come to Netflix? If not, where c...\n",
       "822   Is the real Li'l Seabastian still alive? I wan...\n",
       "1248  I made a Twitter account that tweets out regul...\n",
       "349   Gif Request: Does anyone have a gif of the sce...\n",
       "1698  I haven't laughed this hard in a good while Th...\n",
       "1056  Hey Im the New Mod for /r/PandR, the Subreddit...\n",
       "1428  I think I just lost 5 years of my life... ...w...\n",
       "214   I'M GOING TO TYPE EVERY WORD I KNOW! RECTANGLE...\n",
       "934   Season 4 online Sorry if you guys get these th...\n",
       "2083  Parks and Rec reference I don't know if anybod...\n",
       "1699  I was Right! (for the most part i think?) www....\n",
       "1014  Soooo... What did you think of the new episode...\n",
       "1560  Watched the whole show on a United flight... [...\n",
       "418   Watching Season 4 Episode 10 for about the 5th...\n",
       "1600  Watching in the UK? I watched (and loved) seas...\n",
       "679   Favorite quotes? \"I like people...places...and...\n",
       "742   As our final season approaches... With the fin..."
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclass_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1371    Season 3? Just finished binge watching this sh...\n",
       "100     If given the chance, would you rather be a mem...\n",
       "1637    Rewatch with my wife Just started a rewatch wi...\n",
       "1117    Where can I find season three episodes 1-7? I ...\n",
       "1186    Hey fat dinks, does anyone know where to watch...\n",
       "1607    Season 2 is airing on Netflix in Australia, Ir...\n",
       "933     Girlfriend likes her new nickname My girlfrien...\n",
       "1517    Do we know when season 2 premieres? I just sta...\n",
       "514     The cast in other shows? What are some good sh...\n",
       "1438    Any ideas when season 2 will be out on Netflix...\n",
       "1243    What line do you constantly repeat in your eve...\n",
       "1624    Anyone else find Denise hilarious? She's a gre...\n",
       "1930    Episode discussion season 1 episode 9 Someone ...\n",
       "1453    Adam Scott on season 2 Why no Scott? He was a ...\n",
       "923     Can I start with the 2nd season without missin...\n",
       "2075    Title card font Does anyone know the font that...\n",
       "70      Appreciation for u/NightTrainDan Just some lov...\n",
       "135     need a gif: ben and leslie at grizzle in their...\n",
       "1156    This is my favorite show to binge the entire s...\n",
       "572     What are your favorite quotes to say irl?  paw...\n",
       "1210    Will season 3 come to Netflix? If not, where c...\n",
       "822     Is the real Li'l Seabastian still alive? I wan...\n",
       "1248    I made a Twitter account that tweets out regul...\n",
       "349     Gif Request: Does anyone have a gif of the sce...\n",
       "1698    I haven't laughed this hard in a good while Th...\n",
       "1056    Hey Im the New Mod for /r/PandR, the Subreddit...\n",
       "1428    I think I just lost 5 years of my life... ...w...\n",
       "214     I'M GOING TO TYPE EVERY WORD I KNOW! RECTANGLE...\n",
       "934     Season 4 online Sorry if you guys get these th...\n",
       "2083    Parks and Rec reference I don't know if anybod...\n",
       "1699    I was Right! (for the most part i think?) www....\n",
       "1014    Soooo... What did you think of the new episode...\n",
       "1560    Watched the whole show on a United flight... [...\n",
       "418     Watching Season 4 Episode 10 for about the 5th...\n",
       "1600    Watching in the UK? I watched (and loved) seas...\n",
       "679     Favorite quotes? \"I like people...places...and...\n",
       "742     As our final season approaches... With the fin...\n",
       "Name: all_text, dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcvec = CountVectorizer(ngram_range= (1,2), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_count = fcvec.fit_transform(misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['05',\n",
       " '05 21',\n",
       " '10',\n",
       " '10 5th',\n",
       " '112',\n",
       " '112 episodes',\n",
       " '1st',\n",
       " '1st honest',\n",
       " '2014',\n",
       " '2014 05',\n",
       " '2015',\n",
       " '2015 http',\n",
       " '21',\n",
       " '21 parks',\n",
       " '2nd',\n",
       " '2nd loss',\n",
       " '2nd season',\n",
       " '32',\n",
       " '32 weeks',\n",
       " '5th',\n",
       " '5th time',\n",
       " '71hjl4',\n",
       " '71hjl4 yet_another_theory',\n",
       " '80113701',\n",
       " '80113701 ll',\n",
       " 'able',\n",
       " 'able watch',\n",
       " 'absolutely',\n",
       " 'absolutely love',\n",
       " 'account',\n",
       " 'account favorite',\n",
       " 'account jinmeister',\n",
       " 'account posts',\n",
       " 'account tweets',\n",
       " 'actors',\n",
       " 'actors outside',\n",
       " 'actress',\n",
       " 'actress steals',\n",
       " 'actually',\n",
       " 'actually thinking',\n",
       " 'adam',\n",
       " 'adam scott',\n",
       " 'adventure',\n",
       " 'adventure sure',\n",
       " 'afford',\n",
       " 'afford pay',\n",
       " 'air',\n",
       " 'air averages',\n",
       " 'air know',\n",
       " 'airing',\n",
       " 'airing netflix',\n",
       " 'airs',\n",
       " 'airs watch',\n",
       " 'alive',\n",
       " 'alive support',\n",
       " 'alive want',\n",
       " 'alternative',\n",
       " 'alternative badanimaldrawing',\n",
       " 'america',\n",
       " 'america megaphone',\n",
       " 'amp',\n",
       " 'amp crew',\n",
       " 'amp pawntakesrook',\n",
       " 'amp really',\n",
       " 'andy',\n",
       " 'andy realizes',\n",
       " 'angst',\n",
       " 'angst reliving',\n",
       " 'anti',\n",
       " 'anti spoiler',\n",
       " 'antsy',\n",
       " 'antsy best',\n",
       " 'anybody',\n",
       " 'anybody sites',\n",
       " 'anybody spotted',\n",
       " 'apartment',\n",
       " 'apartment save',\n",
       " 'apartments',\n",
       " 'apartments time',\n",
       " 'apologize',\n",
       " 'apologize having',\n",
       " 'app',\n",
       " 'app right',\n",
       " 'applicable',\n",
       " 'applicable countries',\n",
       " 'appreciation',\n",
       " 'appreciation nighttraindan',\n",
       " 'approaches',\n",
       " 'approaches final',\n",
       " 'approaching',\n",
       " 'approaching thought',\n",
       " 'appropriate',\n",
       " 'appropriate subreddit',\n",
       " 'area',\n",
       " 'area bit',\n",
       " 'aside',\n",
       " 'aside doubt',\n",
       " 'audixmusic',\n",
       " 'australia',\n",
       " 'australia ireland',\n",
       " 'available',\n",
       " 'available netflix',\n",
       " 'averages',\n",
       " 'averages eps',\n",
       " 'awful',\n",
       " 'awful week',\n",
       " 'bad',\n",
       " 'bad deleted',\n",
       " 'badanimaldrawing',\n",
       " 'ben',\n",
       " 'ben leslie',\n",
       " 'best',\n",
       " 'best cure',\n",
       " 'binge',\n",
       " 'binge entire',\n",
       " 'binge watching',\n",
       " 'binging',\n",
       " 'binging amp',\n",
       " 'birdpark',\n",
       " 'bit',\n",
       " 'bit picky',\n",
       " 'bot',\n",
       " 'bot account',\n",
       " 'bot running',\n",
       " 'break',\n",
       " 'break monotony',\n",
       " 'breaks',\n",
       " 'breaks sort',\n",
       " 'bring',\n",
       " 'bring smile',\n",
       " 'brumplestiltskins',\n",
       " 'burt',\n",
       " 'burt macklin',\n",
       " 'butthole',\n",
       " 'butthole xironbjorn',\n",
       " 'buy',\n",
       " 'buy sunshine',\n",
       " 'came',\n",
       " 'came mind',\n",
       " 'card',\n",
       " 'card font',\n",
       " 'card love',\n",
       " 'cast',\n",
       " 'cast amp',\n",
       " 'cast involved',\n",
       " 'cast shows',\n",
       " 'ccv21',\n",
       " 'chance',\n",
       " 'chance community',\n",
       " 'chance member',\n",
       " 'chanel',\n",
       " 'chanel ve',\n",
       " 'character',\n",
       " 'character development',\n",
       " 'character end',\n",
       " 'characters',\n",
       " 'characters amp',\n",
       " 'check',\n",
       " 'check goodies',\n",
       " 'check netflix',\n",
       " 'classicalmusictroll',\n",
       " 'clever',\n",
       " 'clever somename6',\n",
       " 'clockworkanomaly',\n",
       " 'close',\n",
       " 'close sinkhole',\n",
       " 'club',\n",
       " 'club treat',\n",
       " 'com',\n",
       " 'com 2014',\n",
       " 'com gallery',\n",
       " 'com tgpcaps',\n",
       " 'com thegoodplace',\n",
       " 'com title',\n",
       " 'come',\n",
       " 'come netflix',\n",
       " 'comin',\n",
       " 'comin shrampies',\n",
       " 'coming',\n",
       " 'coming wait',\n",
       " 'comment',\n",
       " 'comment use',\n",
       " 'comments',\n",
       " 'comments 71hjl4',\n",
       " 'community',\n",
       " 'community alive',\n",
       " 'constantly',\n",
       " 'constantly repeat',\n",
       " 'constantly want',\n",
       " 'context',\n",
       " 'context sure',\n",
       " 'count',\n",
       " 'count 112',\n",
       " 'countries',\n",
       " 'countries check',\n",
       " 'countries https',\n",
       " 'country',\n",
       " 'country don',\n",
       " 'course',\n",
       " 'course specifics',\n",
       " 'crew',\n",
       " 'crew thing',\n",
       " 'cure',\n",
       " 'cure angst',\n",
       " 'davect01',\n",
       " 'day',\n",
       " 'day available',\n",
       " 'day year',\n",
       " 'days',\n",
       " 'days work',\n",
       " 'decide',\n",
       " 'decide actually',\n",
       " 'delete',\n",
       " 'delete deleted',\n",
       " 'deleted',\n",
       " 'deleted deleted',\n",
       " 'demand',\n",
       " 'demand buy',\n",
       " 'denise',\n",
       " 'denise hilarious',\n",
       " 'development',\n",
       " 'development link',\n",
       " 'developmental',\n",
       " 'did',\n",
       " 'did guys',\n",
       " 'did think',\n",
       " 'dinks',\n",
       " 'dinks does',\n",
       " 'discovered',\n",
       " 'discovered don',\n",
       " 'discussion',\n",
       " 'discussion season',\n",
       " 'discussion starting',\n",
       " 'discussion thread',\n",
       " 'discussions',\n",
       " 'discussions personally',\n",
       " 'disguises',\n",
       " 'disguises text',\n",
       " 'does',\n",
       " 'does anybody',\n",
       " 'does gif',\n",
       " 'does happy',\n",
       " 'does know',\n",
       " 'don',\n",
       " 'don air',\n",
       " 'don know',\n",
       " 'don need',\n",
       " 'doubt',\n",
       " 'doubt comin',\n",
       " 'dressed',\n",
       " 'dressed talk',\n",
       " 'drinking',\n",
       " 'drinking wine',\n",
       " 'drumwizard101',\n",
       " 'drunk',\n",
       " 'drunk apologize',\n",
       " 'dundermifflin',\n",
       " 'dundermifflin goes',\n",
       " 'elmos',\n",
       " 'elmos andy',\n",
       " 'en',\n",
       " 'en wikipedia',\n",
       " 'end',\n",
       " 'end season',\n",
       " 'ending',\n",
       " 'ending season',\n",
       " 'england',\n",
       " 'england using',\n",
       " 'england ve',\n",
       " 'enjoy',\n",
       " 'enjoy birdpark',\n",
       " 'entire',\n",
       " 'entire episode',\n",
       " 'entire family',\n",
       " 'entire season',\n",
       " 'episode',\n",
       " 'episode 10',\n",
       " 'episode afford',\n",
       " 'episode ccv21',\n",
       " 'episode discussion',\n",
       " 'episode just',\n",
       " 'episode laughed',\n",
       " 'episode like',\n",
       " 'episode surely',\n",
       " 'episode wasn',\n",
       " 'episode watch',\n",
       " 'episodes',\n",
       " 'episodes 1st',\n",
       " 'episodes 32',\n",
       " 'episodes plan',\n",
       " 'episodes started',\n",
       " 'episodes week',\n",
       " 'eps',\n",
       " 'eps week',\n",
       " 'especially',\n",
       " 'especially appropriate',\n",
       " 'everyday',\n",
       " 'everyday life',\n",
       " 'ew',\n",
       " 'ew com',\n",
       " 'expect',\n",
       " 'expect http',\n",
       " 'extra',\n",
       " 'extra scenes',\n",
       " 'face',\n",
       " 'face lift',\n",
       " 'face makes',\n",
       " 'face started',\n",
       " 'fake',\n",
       " 'fake location',\n",
       " 'family',\n",
       " 'family room',\n",
       " 'family watches',\n",
       " 'fat',\n",
       " 'fat dinks',\n",
       " 'favorite',\n",
       " 'favorite binge',\n",
       " 'favorite gifs',\n",
       " 'favorite https',\n",
       " 'favorite moments',\n",
       " 'favorite quotes',\n",
       " 'features',\n",
       " 'features fun',\n",
       " 'features thinking',\n",
       " 'feel',\n",
       " 'feel like',\n",
       " 'feel warm',\n",
       " 'final',\n",
       " 'final michael',\n",
       " 'final season',\n",
       " 'finer',\n",
       " 'finer things',\n",
       " 'finished',\n",
       " 'finished binge',\n",
       " 'finished idea',\n",
       " 'flight',\n",
       " 'flight deleted',\n",
       " 'font',\n",
       " 'font does',\n",
       " 'font used',\n",
       " 'free',\n",
       " 'free hulu',\n",
       " 'friend',\n",
       " 'friend uses',\n",
       " 'fun',\n",
       " 'fun things',\n",
       " 'funny',\n",
       " 'funny loved',\n",
       " 'funny ones',\n",
       " 'funny scenes',\n",
       " 'fuzzy',\n",
       " 'fuzzy classicalmusictroll',\n",
       " 'gallery',\n",
       " 'gallery t3eulxv',\n",
       " 'gets',\n",
       " 'gets good',\n",
       " 'gif',\n",
       " 'gif ben',\n",
       " 'gif request',\n",
       " 'gif scene',\n",
       " 'gifs',\n",
       " 'gifs check',\n",
       " 'gifs constantly',\n",
       " 'gifs funny',\n",
       " 'girl',\n",
       " 'girl love',\n",
       " 'girlfriend',\n",
       " 'girlfriend likes',\n",
       " 'girlfriend wanted',\n",
       " 'given',\n",
       " 'given chance',\n",
       " 'given past',\n",
       " 'goddamnit',\n",
       " 'goddamnit love',\n",
       " 'goes',\n",
       " 'goes theflow3000',\n",
       " 'going',\n",
       " 'going antsy',\n",
       " 'going okay',\n",
       " 'going post',\n",
       " 'going type',\n",
       " 'gonwin',\n",
       " 'good',\n",
       " 'good 2nd',\n",
       " 'good latest',\n",
       " 'good place',\n",
       " 'good shows',\n",
       " 'goodies',\n",
       " 'goodies count',\n",
       " 'got',\n",
       " 'got bot',\n",
       " 'got close',\n",
       " 'got little',\n",
       " 'great',\n",
       " 'great actress',\n",
       " 'great chance',\n",
       " 'great character',\n",
       " 'great guys',\n",
       " 'great thanks',\n",
       " 'greekgeek4',\n",
       " 'grizzle',\n",
       " 'grizzle disguises',\n",
       " 'guys',\n",
       " 'guys know',\n",
       " 'guys st',\n",
       " 'guys think',\n",
       " 'guys threads',\n",
       " 'happens',\n",
       " 'happens watching',\n",
       " 'happy',\n",
       " 'happy link',\n",
       " 'hard',\n",
       " 'hard good',\n",
       " 'haven',\n",
       " 'haven laughed',\n",
       " 'having',\n",
       " 'having awful',\n",
       " 'having luck',\n",
       " 'having season',\n",
       " 'hear',\n",
       " 'hear really',\n",
       " 'help',\n",
       " 'help suggestions',\n",
       " 'hey',\n",
       " 'hey fat',\n",
       " 'hey im',\n",
       " 'heyo',\n",
       " 'heyo inspired',\n",
       " 'hibbert',\n",
       " 'hibbert absolutely',\n",
       " 'hilarious',\n",
       " 'hilarious great',\n",
       " 'honest',\n",
       " 'honest impressed',\n",
       " 'household',\n",
       " 'household hubby',\n",
       " 'http',\n",
       " 'http en',\n",
       " 'http insidetv',\n",
       " 'http twitter',\n",
       " 'https',\n",
       " 'https imgur',\n",
       " 'https www',\n",
       " 'hubby',\n",
       " 'hubby suggested',\n",
       " 'hulu',\n",
       " 'hulu ll',\n",
       " 'hulu requires',\n",
       " 'hulu unfortunately',\n",
       " 'humor',\n",
       " 'humor stitches',\n",
       " 'husband',\n",
       " 'husband months',\n",
       " 'idea',\n",
       " 'idea think',\n",
       " 'ideas',\n",
       " 'ideas season',\n",
       " 'im',\n",
       " 'im going',\n",
       " 'im new',\n",
       " 'imgur',\n",
       " 'imgur com',\n",
       " 'imo',\n",
       " 'imo threemileallan',\n",
       " 'impressed',\n",
       " 'impressed hear',\n",
       " 'insidetv',\n",
       " 'insidetv ew',\n",
       " 'inspired',\n",
       " 'inspired twitter',\n",
       " 'instantly',\n",
       " 'instantly said',\n",
       " 'involved',\n",
       " 'involved love',\n",
       " 'ireland',\n",
       " 'ireland new',\n",
       " 'irl',\n",
       " 'irl pawneescrantonmerger',\n",
       " 'isn',\n",
       " 'isn option',\n",
       " 'janet',\n",
       " 'janet snakehole',\n",
       " 'january',\n",
       " 'january 2015',\n",
       " 'jason',\n",
       " 'jason friend',\n",
       " 'jinmeister',\n",
       " 'jtayy12',\n",
       " 'just',\n",
       " 'just clever',\n",
       " 'just finished',\n",
       " 'just funny',\n",
       " 'just lost',\n",
       " 'just love',\n",
       " 'just skip',\n",
       " 'just started',\n",
       " 'just time',\n",
       " 'know',\n",
       " 'know anybody',\n",
       " 'know font',\n",
       " 'know going',\n",
       " 'know list',\n",
       " 'know ll',\n",
       " 'know rectangle',\n",
       " 'know season',\n",
       " 'know watch',\n",
       " 'know won',\n",
       " 'knowing',\n",
       " 'knowing origin',\n",
       " 'knstone',\n",
       " 'latest',\n",
       " 'latest episode',\n",
       " 'laughed',\n",
       " 'laughed entire',\n",
       " 'laughed hard',\n",
       " 'layout',\n",
       " 'layout features',\n",
       " 'leslie',\n",
       " 'leslie grizzle',\n",
       " 'li',\n",
       " 'li seabastian',\n",
       " 'life',\n",
       " 'life replaced',\n",
       " 'life tahani',\n",
       " 'lift',\n",
       " 'lift anti',\n",
       " 'like',\n",
       " 'like going',\n",
       " 'like hulu',\n",
       " 'like member',\n",
       " 'like office',\n",
       " 'like people',\n",
       " 'liked',\n",
       " 'liked knowing',\n",
       " 'likes',\n",
       " 'likes new',\n",
       " 'line',\n",
       " 'line constantly',\n",
       " 'link',\n",
       " 'link favorite',\n",
       " 'link http',\n",
       " 'linking',\n",
       " 'linking past',\n",
       " 'list',\n",
       " 'list applicable',\n",
       " 'little',\n",
       " 'little face',\n",
       " 'little good',\n",
       " 'live',\n",
       " 'live does',\n",
       " 'lived',\n",
       " 'lived states',\n",
       " 'lizlemondonaghy',\n",
       " 'll',\n",
       " 'll make',\n",
       " 'll melanie_rblatt',\n",
       " 'll really',\n",
       " 'll talk',\n",
       " 'll tell',\n",
       " 'll timeline',\n",
       " 'll watch',\n",
       " 'location',\n",
       " 'location isn',\n",
       " 'lol',\n",
       " 'lol times',\n",
       " 'long',\n",
       " 'long prettyusual',\n",
       " 'long time',\n",
       " 'loss',\n",
       " 'loss just',\n",
       " 'lost',\n",
       " 'lost years',\n",
       " 'lot',\n",
       " 'lot did',\n",
       " 'lot place',\n",
       " 'lot time',\n",
       " 'love',\n",
       " 'love actors',\n",
       " 'love lizlemondonaghy',\n",
       " 'love makes',\n",
       " 'love nighttraindan',\n",
       " 'love post',\n",
       " 'love reaction',\n",
       " 'loved',\n",
       " 'loved humor',\n",
       " 'loved season',\n",
       " 'lovely',\n",
       " 'lovely people',\n",
       " 'loving',\n",
       " 'loving davect01',\n",
       " 'luck',\n",
       " 'luck lovely',\n",
       " 'luck lunatic_minge',\n",
       " 'lunatic_minge',\n",
       " 'macklin',\n",
       " 'macklin janet',\n",
       " 'make',\n",
       " 'make personas',\n",
       " 'make rough',\n",
       " 'make similar',\n",
       " 'make sure',\n",
       " 'makes',\n",
       " 'makes feel',\n",
       " 'maybe',\n",
       " 'maybe chanel',\n",
       " 'meet',\n",
       " 'meet bad',\n",
       " 'megaphone',\n",
       " 'megaphone monday',\n",
       " 'melanie_rblatt',\n",
       " 'member',\n",
       " 'member deleted',\n",
       " 'member finer',\n",
       " 'members',\n",
       " 'members cast',\n",
       " 'michael',\n",
       " 'michael schur',\n",
       " 'mid',\n",
       " 'mid season_replacement',\n",
       " 'mind',\n",
       " 'mind sweet',\n",
       " 'missed',\n",
       " 'missed airs',\n",
       " 'missing',\n",
       " 'missing watched',\n",
       " 'mod',\n",
       " 'mod pandr',\n",
       " 'moment',\n",
       " 'moment pre',\n",
       " 'moments',\n",
       " 'moments character',\n",
       " 'moments shows',\n",
       " 'monday',\n",
       " 'monday butthole',\n",
       " 'monotony',\n",
       " 'monotony thanks',\n",
       " 'months',\n",
       " 'months shop',\n",
       " 'movies',\n",
       " 'movies members',\n",
       " 'mumblerapisgarbage',\n",
       " 'nbc',\n",
       " 'nbc website',\n",
       " 'need',\n",
       " 'need gif',\n",
       " 'need signed',\n",
       " 'netflix',\n",
       " 'netflix australia',\n",
       " 'netflix com',\n",
       " 'netflix country',\n",
       " 'netflix entire',\n",
       " 'netflix greekgeek4',\n",
       " 'netflix season',\n",
       " 'netflix seasons',\n",
       " 'netflix xfinity',\n",
       " 'new',\n",
       " 'new adventure',\n",
       " 'new discussion',\n",
       " 'new episode',\n",
       " 'new girl',\n",
       " 'new mod',\n",
       " 'new nickname',\n",
       " 'new point',\n",
       " 'new zealand',\n",
       " 'nickname',\n",
       " 'nickname girlfriend',\n",
       " 'nickname reason',\n",
       " 'night',\n",
       " 'night ll',\n",
       " 'nighttraindan',\n",
       " 'nighttraindan gifs',\n",
       " 'nighttraindan just',\n",
       " 'nighttraindan knstone',\n",
       " 'nope',\n",
       " 'nope season',\n",
       " 'occasionally',\n",
       " 'occasionally ll',\n",
       " 'occono',\n",
       " 'office',\n",
       " 'office new',\n",
       " 'okay',\n",
       " 'okay sad',\n",
       " 'okay weekly',\n",
       " 'ones',\n",
       " 'ones context',\n",
       " 'online',\n",
       " 'online sorry',\n",
       " 'online watch',\n",
       " 'option',\n",
       " 'option help',\n",
       " 'optional',\n",
       " 'optional husband',\n",
       " 'org',\n",
       " 'org wiki',\n",
       " 'origin',\n",
       " 'origin thought',\n",
       " 'outside',\n",
       " 'outside characters',\n",
       " 'pandr',\n",
       " 'pandr subreddit',\n",
       " 'parks',\n",
       " 'parks rec',\n",
       " 'parks recreation',\n",
       " 'parties',\n",
       " 'parties guys',\n",
       " 'past',\n",
       " 'past recaps',\n",
       " 'past talking',\n",
       " 'past years',\n",
       " 'pawneescrantonmerger',\n",
       " 'pawntakesrook',\n",
       " 'pay',\n",
       " 'pay episode',\n",
       " 'people',\n",
       " 'people alternative',\n",
       " 'people places',\n",
       " 'people want',\n",
       " 'personally',\n",
       " 'personally thank',\n",
       " 'personally willing',\n",
       " 'personas',\n",
       " 'personas dressed',\n",
       " 'phone',\n",
       " 'phone room',\n",
       " 'picky',\n",
       " 'picky layout',\n",
       " 'place',\n",
       " 'place online',\n",
       " 'place timelines',\n",
       " 'places',\n",
       " 'places area',\n",
       " 'places things',\n",
       " 'plan',\n",
       " 'plan watching',\n",
       " 'pnr',\n",
       " 'pnr household',\n",
       " 'point',\n",
       " 'point got',\n",
       " 'post',\n",
       " 'post breaks',\n",
       " 'post drunk',\n",
       " 'post dundermifflin',\n",
       " 'posting',\n",
       " 'posting thread',\n",
       " 'posts',\n",
       " 'posts screencaps',\n",
       " 'practically',\n",
       " 'practically threw',\n",
       " 'pre',\n",
       " 'pre successful',\n",
       " 'premiere',\n",
       " 'premiere air',\n",
       " 'premiere final',\n",
       " 'premiere sources',\n",
       " 'premieres',\n",
       " 'premieres just',\n",
       " 'premium',\n",
       " 'premium account',\n",
       " 'prettyusual',\n",
       " 'prime',\n",
       " 'prime luck',\n",
       " 'probably',\n",
       " 'probably countries',\n",
       " 'probably expect',\n",
       " 'probably january',\n",
       " 'promotion',\n",
       " 'promotion rule',\n",
       " 'quotes',\n",
       " 'quotes like',\n",
       " 'quotes say',\n",
       " 'reaction',\n",
       " 'reaction scene',\n",
       " 'real',\n",
       " 'real li',\n",
       " 'realizes',\n",
       " 'realizes roy',\n",
       " 'really',\n",
       " 'really funny',\n",
       " 'really gets',\n",
       " 'really new',\n",
       " 'really wish',\n",
       " 'reason',\n",
       " 'reason thought',\n",
       " 'rec',\n",
       " 'rec reference',\n",
       " 'recaps',\n",
       " 'recaps reviews',\n",
       " 'recreation',\n",
       " 'recreation season',\n",
       " 'rectangle',\n",
       " 'rectangle america',\n",
       " 'reddit',\n",
       " 'reddit com',\n",
       " 'refer',\n",
       " 'refer new',\n",
       " 'reference',\n",
       " 'reference don',\n",
       " 'reflecting',\n",
       " 'reflecting given',\n",
       " 'regular',\n",
       " 'regular screencaps',\n",
       " 'relevant',\n",
       " 'relevant episodes',\n",
       " 'reliving',\n",
       " 'reliving past',\n",
       " 'repeat',\n",
       " 'repeat everyday',\n",
       " 'replaced',\n",
       " 'replaced swear',\n",
       " 'request',\n",
       " 'request does',\n",
       " 'requires',\n",
       " 'requires premium',\n",
       " 'resorted',\n",
       " 'resorted taking',\n",
       " 'reviews',\n",
       " 'reviews extra',\n",
       " 'rewatch',\n",
       " 'rewatch discussion',\n",
       " 'rewatch wife',\n",
       " 'right',\n",
       " 'right nope',\n",
       " 'right think',\n",
       " 'room',\n",
       " 'room audixmusic',\n",
       " 'room mumblerapisgarbage',\n",
       " 'rough',\n",
       " 'rough schedule',\n",
       " 'roy',\n",
       " 'roy hibbert',\n",
       " 'rule',\n",
       " 'rule does',\n",
       " 'running',\n",
       " 'running night',\n",
       " 's1e5',\n",
       " 's1e5 practically',\n",
       " 's5e10',\n",
       " 's5e10 parties',\n",
       " 'sad',\n",
       " 'sad moments',\n",
       " 'safe',\n",
       " 'safe jason',\n",
       " 'said',\n",
       " 'said liked',\n",
       " 'save',\n",
       " 'save ton',\n",
       " 'say',\n",
       " 'say irl',\n",
       " 'say probably',\n",
       " 'scene',\n",
       " 'scene brumplestiltskins',\n",
       " 'scene having',\n",
       " 'scene imo',\n",
       " 'scene s5e10',\n",
       " 'scenes',\n",
       " 'scenes long',\n",
       " 'scenes special',\n",
       " 'schedule',\n",
       " 'schedule week',\n",
       " 'schur',\n",
       " 'schur know',\n",
       " 'scott',\n",
       " 'scott great',\n",
       " 'scott season',\n",
       " 'screencaps',\n",
       " 'screencaps people',\n",
       " 'screencaps spongebob',\n",
       " 'seabastian',\n",
       " 'seabastian alive',\n",
       " 'season',\n",
       " 'season airing',\n",
       " 'season approaches',\n",
       " 'season approaching',\n",
       " 'season come',\n",
       " 'season coming',\n",
       " 'season day',\n",
       " 'season deleted',\n",
       " 'season ending',\n",
       " 'season episode',\n",
       " 'season episodes',\n",
       " 'season final',\n",
       " 'season free',\n",
       " 'season just',\n",
       " 'season lived',\n",
       " 'season missing',\n",
       " 'season netflix',\n",
       " 'season online',\n",
       " 'season premiere',\n",
       " 'season premieres',\n",
       " 'season scott',\n",
       " 'season spankme9991',\n",
       " 'season want',\n",
       " 'season watch',\n",
       " 'season week',\n",
       " 'season_replacement',\n",
       " 'season_replacement season',\n",
       " 'seasons',\n",
       " 'seasons like',\n",
       " 'second',\n",
       " 'second loving',\n",
       " 'seen',\n",
       " 'seen gif',\n",
       " 'self',\n",
       " 'self im',\n",
       " 'self promotion',\n",
       " 'shifting',\n",
       " 'shifting s1e5',\n",
       " 'shop',\n",
       " 'shop apartment',\n",
       " 'short',\n",
       " 'short break',\n",
       " 'shows',\n",
       " 'shows good',\n",
       " 'shows like',\n",
       " 'shows movies',\n",
       " 'shrampies',\n",
       " 'shrampies youarenotbook',\n",
       " 'signed',\n",
       " 'signed occono',\n",
       " 'similar',\n",
       " 'similar bot',\n",
       " 'sinkhole',\n",
       " 'sinkhole face',\n",
       " 'sites',\n",
       " 'sites maybe',\n",
       " 'skip',\n",
       " 'skip season',\n",
       " 'smile',\n",
       " 'smile face',\n",
       " 'snakehole',\n",
       " 'snakehole used',\n",
       " 'solidarity',\n",
       " 'solidarity reflecting',\n",
       " 'somename6',\n",
       " 'soooo',\n",
       " 'soooo did',\n",
       " 'sorry',\n",
       " 'sorry guys',\n",
       " 'sort',\n",
       " 'sort self',\n",
       " 'sources',\n",
       " 'sources say',\n",
       " 'spankme9991',\n",
       " 'special',\n",
       " 'special features',\n",
       " 'specifics',\n",
       " 'specifics decide',\n",
       " 'spoiler',\n",
       " 'spoiler make',\n",
       " 'spoiler support',\n",
       " 'spoiler type',\n",
       " 'spoilers',\n",
       " 'spoilers use',\n",
       " 'spongebob',\n",
       " 'spongebob make',\n",
       " 'spotted',\n",
       " 'spotted safe',\n",
       " 'st',\n",
       " 'st elmos',\n",
       " 'start',\n",
       " 'start 2nd',\n",
       " 'started',\n",
       " 'started rewatch',\n",
       " 'started shifting',\n",
       " 'started watching',\n",
       " 'started week',\n",
       " 'starting',\n",
       " 'starting season',\n",
       " 'starts',\n",
       " 'starts season',\n",
       " 'states',\n",
       " 'states england',\n",
       " 'steals',\n",
       " 'steals scene',\n",
       " 'stickied',\n",
       " 'stickied ll',\n",
       " 'stitches',\n",
       " 'stitches entire',\n",
       " 'subreddit',\n",
       " 'subreddit got',\n",
       " 'subreddit solidarity',\n",
       " 'successful',\n",
       " 'successful heyo',\n",
       " 'suggested',\n",
       " 'suggested make',\n",
       " 'suggestions',\n",
       " 'suggestions great',\n",
       " 'sunshine',\n",
       " 'support',\n",
       " 'support happens',\n",
       " 'support spoilers',\n",
       " 'sure',\n",
       " 'sure delete',\n",
       " 'sure lol',\n",
       " 'sure post',\n",
       " 'sure use',\n",
       " 'surely',\n",
       " 'surely nbc',\n",
       " 'surprised',\n",
       " 'surprised work',\n",
       " ...]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>05</th>\n",
       "      <th>05 21</th>\n",
       "      <th>10</th>\n",
       "      <th>10 5th</th>\n",
       "      <th>112</th>\n",
       "      <th>112 episodes</th>\n",
       "      <th>1st</th>\n",
       "      <th>1st honest</th>\n",
       "      <th>2014</th>\n",
       "      <th>2014 05</th>\n",
       "      <th>...</th>\n",
       "      <th>years</th>\n",
       "      <th>years life</th>\n",
       "      <th>years posting</th>\n",
       "      <th>yet_another_theory</th>\n",
       "      <th>yet_another_theory gonwin</th>\n",
       "      <th>yo</th>\n",
       "      <th>yo self</th>\n",
       "      <th>youarenotbook</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zealand uk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1214 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   05  05 21  10  10 5th  112  112 episodes  1st  1st honest  2014  2014 05  \\\n",
       "0   0      0   0       0    0             0    0           0     0        0   \n",
       "1   0      0   0       0    0             0    0           0     0        0   \n",
       "2   0      0   0       0    0             0    0           0     0        0   \n",
       "3   0      0   0       0    0             0    0           0     0        0   \n",
       "4   0      0   0       0    0             0    0           0     0        0   \n",
       "\n",
       "   ...  years  years life  years posting  yet_another_theory  \\\n",
       "0  ...      0           0              0                   0   \n",
       "1  ...      0           0              0                   0   \n",
       "2  ...      0           0              0                   0   \n",
       "3  ...      0           0              0                   0   \n",
       "4  ...      0           0              0                   0   \n",
       "\n",
       "   yet_another_theory gonwin  yo  yo self  youarenotbook  zealand  zealand uk  \n",
       "0                          0   0        0              0        0           0  \n",
       "1                          0   1        1              0        0           0  \n",
       "2                          0   0        0              0        0           0  \n",
       "3                          0   0        0              0        0           0  \n",
       "4                          0   0        0              0        0           0  \n",
       "\n",
       "[5 rows x 1214 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_df = pd.DataFrame(fin_count.toarray(),\n",
    "                      columns = fcvec.get_feature_names())\n",
    "\n",
    "wrong_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_words = wrong_df.sum().sort_values(ascending = False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_mis = pd.DataFrame(wrong_words.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_mis.columns = ['Word Usage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deleted</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>netflix</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ll</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watch</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guys</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>watching</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favorite</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>does</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episodes</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>com</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entire</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>season episode</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word Usage\n",
       "season                  30\n",
       "episode                 10\n",
       "know                    10\n",
       "just                     9\n",
       "new                      9\n",
       "deleted                  8\n",
       "netflix                  8\n",
       "think                    8\n",
       "time                     7\n",
       "week                     7\n",
       "ll                       7\n",
       "watch                    7\n",
       "guys                     6\n",
       "love                     6\n",
       "watching                 6\n",
       "favorite                 6\n",
       "does                     5\n",
       "episodes                 5\n",
       "great                    5\n",
       "com                      5\n",
       "like                     5\n",
       "entire                   4\n",
       "sure                     4\n",
       "want                     4\n",
       "season episode           4"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_mis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Season, episode, and know were the least predictive terms in the model, with season and episode appearing in their plural forms as well.  This is no surprise, given that fans of both shows would likely refer to a specific season or episode in a post without specifically referencing the show itself.  The reference is assumed because it is already being posted on the specific Subreddit, so there's no need to talk about Season 1 of Parks & Rec, when you can just say Season 1 and everyone already knows what show you're referring to.  Although, there is no need to talk about the #brendanaquits seasons of Parks & Rec anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['season', 'episode', 'know', 'just', 'new', 'deleted', 'netflix',\n",
       "       'think', 'time', 'week', 'll', 'watch', 'guys', 'love', 'watching',\n",
       "       'favorite', 'does', 'episodes', 'great', 'com', 'like', 'entire',\n",
       "       'sure', 'want', 'season episode', 'good', 'account', 'make', 'started',\n",
       "       'scene', 'final', 'really', 'don', 'going', 'gif', 'final season',\n",
       "       'got', 'discussion', 'watched', 'having', 'thought', 'probably', 'past',\n",
       "       'face', 'amp', 'guys think', 'spoiler', 'use', 'post', 'cast', 'http',\n",
       "       'twitter', 'nighttraindan', 'people', 'funny', 'rewatch', 'title',\n",
       "       'premiere', 'shows', 'lot', 'gifs', 'hulu', 'things', 'character',\n",
       "       'features', 'chance', 'thanks', 'im', 'parks', 'favorite quotes',\n",
       "       'card', 'thinking', 'scott', 'https', 'thread', 'family', 'online',\n",
       "       'bot', 'okay', 'binge', 'title card', 'twitter account', 'check',\n",
       "       'season netflix', 'personally', 'did', 'season premiere', 'room',\n",
       "       'right', 'rewatch wife', 'self', 'season just', 'entire episode',\n",
       "       'england', 'gif scene', 'girlfriend', 'given', 'don know', 'does know',\n",
       "       'finished'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_words.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Support Vector Machine model scored between 90-95% on accuracy, so we were able to achieve our modeling goal from a statistical standpoint, however writing comedies is hard.  This model can't necessarily guarantee a successful pilot.  A lot of the overlap happens on words that still aren't overly descriptive towards themes, but let's give this a shot.\n",
    "\n",
    "From my first test\n",
    "* Interestingly the model seems to misfire on scouts, so I think my show would have to do with campers or talent scouts.  Song is another word the model misfires on.  The words hilarious, actress, charity, and podcast also appear.  Yes, I see it now- the lead is a hilarious struggling actress named Charity, who gets casted on a singing show by Don, the talent scout for the show, who fell in love with our leading actress at first sight, and offered her a spot on his show without hearing her sing, so he can be around her.  Will Don's \"charity offer\" to cast a struggling actress, also named Charity, professionally and romanticly pay off?  If she can't sing, Don's career is over.  If Charity discovers Don's motivations, will she get disgusted and leave the show altogether?  Last thing, Don is British (UK and England are also stopwords).\n",
    "\n",
    "From my second test\n",
    "* The main character is still an Englishman named Don, who has a Twitter account and podcast called \"Guys Think.\"  He is a leading voice for the trying too hard to be overly masculine (basically a British Joe Rogan).  His family tells him it's time for him to settle down and find a girlfriend, and as he tries, he struggles with his own masculinity when finding out that attractive women don't take kindly to his \"manliness.\"  He interviews a sensitive guy named Scott, a seemingly plain guy with a beautiful, intelligent, and famous wife (think Melissa Mayer, who can probably also be played by Kristen Bell).  While he goes onto the attack during the interview, Don finds that Scott may be the type of guy he needs to keep around instead of his \"macho\" friends.  Scott is wary, but is too good of a guy to not help (basically Chidi from the Good Place).\n",
    "\n",
    "These do seem like fun ideas to develop.  If we're truly judging our goal in a non-metric sense, this did generate an idea that my comedic ineptitude wouldn't have discovered on its own.  Now I just need to develop the characters and pitch it to Mike Schur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I were to implement this further and look for even more ideas, I would probably run a decision tree model and possibly try a bootstrapping and bagging method (which I wasn't able to due to time constraints).  Additionally, I might look deeper into which rows misclassified and adjust my stopwords to improve accuracy.  I could even look to scrape more subreddits and attempt to classify new Subreddits in addition to The Good Place and Parks & Rec.  If I do that, I would likely have to rely more on boosting, bagging, and bootstrapping since I would be running classification on more than one target variable.\n",
    "\n",
    "All of this would likely result in a more accurate model, but once again, I don't want to be too accurate because then I wouldn't be left with many words to generate ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Parks & Rec Subreddit](https://www.reddit.com/r/PandR/)\n",
    "- [The Good Place Subreddit](https://www.reddit.com/r/TheGoodPlace/)\n",
    "- [Data Dictionary for Original Data Categories](https://pushshift.io/api-parameters/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
